<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Inference Engine (Clocher): engine/src/kernels/ie_kernels_cuda.cu File Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Inference Engine (Clocher)<span id="projectnumber">&#160;0.2</span>
   </div>
   <div id="projectbrief">C11 CPU/GPU inference baseline with strict metrics &amp; harness</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search',true);
  $(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('ie__kernels__cuda_8cu.html',''); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#define-members">Macros</a> &#124;
<a href="#typedef-members">Typedefs</a> &#124;
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle"><div class="title">ie_kernels_cuda.cu File Reference</div></div>
</div><!--header-->
<div class="contents">

<p>CUDA implementations of GEMV/activation/packing kernels + C-ABI launchers.  
<a href="#details">More...</a></p>
<div class="textblock"><code>#include &lt;cuda_runtime.h&gt;</code><br />
<code>#include &lt;math_constants.h&gt;</code><br />
<code>#include &lt;stdint.h&gt;</code><br />
<code>#include &lt;stddef.h&gt;</code><br />
<code>#include &quot;<a class="el" href="ie__kernels__cuda_8h_source.html">ie_kernels_cuda.h</a>&quot;</code><br />
</div><div class="textblock"><div class="dynheader">
Include dependency graph for ie_kernels_cuda.cu:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu__incl.svg" width="527" height="207"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>
</div>
<p><a href="ie__kernels__cuda_8cu_source.html">Go to the source code of this file.</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="define-members" name="define-members"></a>
Macros</h2></td></tr>
<tr class="memitem:afb3284f12e7d5038612ece3185920449" id="r_afb3284f12e7d5038612ece3185920449"><td class="memItemLeft" align="right" valign="top">#define&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#afb3284f12e7d5038612ece3185920449">IE_CUDA_OK</a>&#160;&#160;&#160;0</td></tr>
<tr class="separator:afb3284f12e7d5038612ece3185920449"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad253e9c439228a67be0e55b0f4c81edf" id="r_ad253e9c439228a67be0e55b0f4c81edf"><td class="memItemLeft" align="right" valign="top">#define&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ad253e9c439228a67be0e55b0f4c81edf">IE_CUDA_STREAM_T_DEFINED</a></td></tr>
<tr class="separator:ad253e9c439228a67be0e55b0f4c81edf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af4c083462b1e956e6461aa246d2af9a6" id="r_af4c083462b1e956e6461aa246d2af9a6"><td class="memItemLeft" align="right" valign="top">#define&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#af4c083462b1e956e6461aa246d2af9a6">CUDA_GUARD</a>(call)</td></tr>
<tr class="memdesc:af4c083462b1e956e6461aa246d2af9a6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Guard macro for CUDA runtime calls inside launchers.  <br /></td></tr>
<tr class="separator:af4c083462b1e956e6461aa246d2af9a6"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="typedef-members" name="typedef-members"></a>
Typedefs</h2></td></tr>
<tr class="memitem:aa036ed43d83d669bfc8c15c81a7efe73" id="r_aa036ed43d83d669bfc8c15c81a7efe73"><td class="memItemLeft" align="right" valign="top">typedef cudaStream_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#aa036ed43d83d669bfc8c15c81a7efe73">ie_cuda_stream_t</a></td></tr>
<tr class="separator:aa036ed43d83d669bfc8c15c81a7efe73"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a841d35ecb02f9850f311a5724d21deac" id="r_a841d35ecb02f9850f311a5724d21deac"><td class="memItemLeft" align="right" valign="top">static void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a841d35ecb02f9850f311a5724d21deac">ie_cuda_set_err</a> (const char *msg)</td></tr>
<tr class="memdesc:a841d35ecb02f9850f311a5724d21deac"><td class="mdescLeft">&#160;</td><td class="mdescRight">Store an error message into the thread-local buffer (truncates).  <br /></td></tr>
<tr class="separator:a841d35ecb02f9850f311a5724d21deac"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga6656c0ddd9df382048e7cd162e0c1d64" id="r_ga6656c0ddd9df382048e7cd162e0c1d64"><td class="memItemLeft" align="right" valign="top">const char *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__IE__GPU.html#ga6656c0ddd9df382048e7cd162e0c1d64">ie_cuda_last_error_string</a> (void)</td></tr>
<tr class="memdesc:ga6656c0ddd9df382048e7cd162e0c1d64"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return a thread-local message describing the last CUDA error.     <br /></td></tr>
<tr class="separator:ga6656c0ddd9df382048e7cd162e0c1d64"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a603115e5bb835d21609518d0af053e8d" id="r_a603115e5bb835d21609518d0af053e8d"><td class="memItemLeft" align="right" valign="top">__device__ float&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a603115e5bb835d21609518d0af053e8d">ie_apply_activation</a> (float x, <a class="el" href="group__IE__GPU.html#ga08ba1d3777f64c23e99124d2e460e701">ie_act_kind_t</a> act)</td></tr>
<tr class="memdesc:a603115e5bb835d21609518d0af053e8d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Apply activation in an epilogue.  <br /></td></tr>
<tr class="separator:a603115e5bb835d21609518d0af053e8d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af00402674d56c833c92c62df1cef1e84" id="r_af00402674d56c833c92c62df1cef1e84"><td class="memItemLeft" align="right" valign="top">__global__ void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#af00402674d56c833c92c62df1cef1e84">k_gemv_rowwise_f32</a> (const float *__restrict__ W, const float *__restrict__ x, float *__restrict__ y, int rows, int cols, int ldW, float alpha, float beta)</td></tr>
<tr class="memdesc:af00402674d56c833c92c62df1cef1e84"><td class="mdescLeft">&#160;</td><td class="mdescRight">Kernel: row-wise GEMV (computes <code>y = alpha * W * x + beta * y</code>).  <br /></td></tr>
<tr class="separator:af00402674d56c833c92c62df1cef1e84"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab9150fe6fc7da507974bfdd38b262ce2" id="r_ab9150fe6fc7da507974bfdd38b262ce2"><td class="memItemLeft" align="right" valign="top">__global__ void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ab9150fe6fc7da507974bfdd38b262ce2">k_gemv_bias_act_f32</a> (const float *__restrict__ W, const float *__restrict__ x, const float *__restrict__ bias, float *__restrict__ y, int rows, int cols, int ldW, float alpha, float beta, <a class="el" href="group__IE__GPU.html#ga08ba1d3777f64c23e99124d2e460e701">ie_act_kind_t</a> act)</td></tr>
<tr class="memdesc:ab9150fe6fc7da507974bfdd38b262ce2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Kernel: fused GEMV + bias + activation.  <br /></td></tr>
<tr class="separator:ab9150fe6fc7da507974bfdd38b262ce2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aea063f21d4bdf3ba4108f9677476aad4" id="r_aea063f21d4bdf3ba4108f9677476aad4"><td class="memItemLeft" align="right" valign="top">__global__ void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#aea063f21d4bdf3ba4108f9677476aad4">k_vec_tanh_f32</a> (float *__restrict__ y, const float *__restrict__ x, int n)</td></tr>
<tr class="memdesc:aea063f21d4bdf3ba4108f9677476aad4"><td class="mdescLeft">&#160;</td><td class="mdescRight">Kernel: elementwise <code>tanh</code> on a vector.  <br /></td></tr>
<tr class="separator:aea063f21d4bdf3ba4108f9677476aad4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac440d51163bc9f16e835d00aedcb4b79" id="r_ac440d51163bc9f16e835d00aedcb4b79"><td class="memItemLeft" align="right" valign="top">__global__ void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ac440d51163bc9f16e835d00aedcb4b79">k_pack_w_blockedk_f32</a> (float *__restrict__ Wp, const float *__restrict__ W, int rows, int cols, int ldW, int block_k)</td></tr>
<tr class="memdesc:ac440d51163bc9f16e835d00aedcb4b79"><td class="mdescLeft">&#160;</td><td class="mdescRight">Kernel: pack row-major <code>W</code> into Blocked-K layout (see header docs).  <br /></td></tr>
<tr class="separator:ac440d51163bc9f16e835d00aedcb4b79"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga92e383308a681db6bb757c3add5906f0" id="r_ga92e383308a681db6bb757c3add5906f0"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__IE__GPU.html#ga92e383308a681db6bb757c3add5906f0">ie_cuda_launch_gemv_rowwise_f32</a> (const float *W, const float *x, float *y, int rows, int cols, int ldW, float alpha, float beta, <a class="el" href="group__IE__GPU.html#gaefb3d7d282a514152960b6c3d812355b">ie_cuda_stream_t</a> stream)</td></tr>
<tr class="memdesc:ga92e383308a681db6bb757c3add5906f0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute <code>y = alpha * W * x + beta * y</code> (row-wise GEMV).     <br /></td></tr>
<tr class="separator:ga92e383308a681db6bb757c3add5906f0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaa2441c827e95b20c0413b0372ffc262b" id="r_gaa2441c827e95b20c0413b0372ffc262b"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__IE__GPU.html#gaa2441c827e95b20c0413b0372ffc262b">ie_cuda_launch_gemv_bias_act_f32</a> (const float *W, const float *x, const float *bias, float *y, int rows, int cols, int ldW, float alpha, float beta, <a class="el" href="group__IE__GPU.html#ga08ba1d3777f64c23e99124d2e460e701">ie_act_kind_t</a> act, <a class="el" href="group__IE__GPU.html#gaefb3d7d282a514152960b6c3d812355b">ie_cuda_stream_t</a> stream)</td></tr>
<tr class="memdesc:gaa2441c827e95b20c0413b0372ffc262b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute <code>y = act(alpha * W*x + bias + beta*y)</code> in one pass.     <br /></td></tr>
<tr class="separator:gaa2441c827e95b20c0413b0372ffc262b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga4e8445935545f1b5bbb671826543ffab" id="r_ga4e8445935545f1b5bbb671826543ffab"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__IE__GPU.html#ga4e8445935545f1b5bbb671826543ffab">ie_cuda_launch_vec_tanh_f32</a> (float *y, const float *x, int n, <a class="el" href="group__IE__GPU.html#gaefb3d7d282a514152960b6c3d812355b">ie_cuda_stream_t</a> stream)</td></tr>
<tr class="memdesc:ga4e8445935545f1b5bbb671826543ffab"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute <code>y[i] = tanh(x[i])</code> for <code>i in [0, n)</code>.     <br /></td></tr>
<tr class="separator:ga4e8445935545f1b5bbb671826543ffab"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga72b6dfd858bd993a18d9c5dfb3112389" id="r_ga72b6dfd858bd993a18d9c5dfb3112389"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__IE__GPU.html#ga72b6dfd858bd993a18d9c5dfb3112389">ie_cuda_launch_pack_w_blockedk_f32</a> (float *Wp, const float *W, int rows, int cols, int ldW, int block_k, <a class="el" href="group__IE__GPU.html#gaefb3d7d282a514152960b6c3d812355b">ie_cuda_stream_t</a> stream)</td></tr>
<tr class="memdesc:ga72b6dfd858bd993a18d9c5dfb3112389"><td class="mdescLeft">&#160;</td><td class="mdescRight">Pack a row-major matrix into Blocked-K layout for better memory access.     <br /></td></tr>
<tr class="separator:ga72b6dfd858bd993a18d9c5dfb3112389"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="var-members" name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:a5ab38bcd2467e982c68d46e66acfa9ef" id="r_a5ab38bcd2467e982c68d46e66acfa9ef"><td class="memItemLeft" align="right" valign="top">static __thread char&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a5ab38bcd2467e982c68d46e66acfa9ef">g_ie_cuda_err</a> [256] = {0}</td></tr>
<tr class="memdesc:a5ab38bcd2467e982c68d46e66acfa9ef"><td class="mdescLeft">&#160;</td><td class="mdescRight">Thread-local buffer holding the most recent CUDA error message.  <br /></td></tr>
<tr class="separator:a5ab38bcd2467e982c68d46e66acfa9ef"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>CUDA implementations of GEMV/activation/packing kernels + C-ABI launchers. </p>
<p>This is the <b>only</b> TU that includes CUDA headers. The public header <code><a class="el" href="ie__kernels__cuda_8h.html" title="CUDA GPU kernels and C-ABI launchers for hot-path vector/matrix ops.">ie_kernels_cuda.h</a></code> keeps the rest of the codebase CUDA-agnostic.</p>
<h2><a class="anchor" id="autotoc_md62"></a>
Kernel policy (baseline, safe defaults)</h2>
<ul>
<li><b>GEMV row-wise:</b> one block per output row, 256 threads per block, strided K loop with shared-memory reduction.</li>
<li><b>Fused GEMV+bias+activation:</b> same grid policy with a short epilogue.</li>
<li><b>Vector tanh:</b> grid-stride loop with 256-thread blocks, large grid.</li>
<li><b>Packing (Blocked-K):</b> 2D grid over <code>(cols, rows)</code> with 32x8 threads.</li>
</ul>
<p>These settings favor portability and predictability. They are easy to tune per architecture once you profile on target GPUs. </p>

<p class="definition">Definition in file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
</div><h2 class="groupheader">Macro Definition Documentation</h2>
<a id="af4c083462b1e956e6461aa246d2af9a6" name="af4c083462b1e956e6461aa246d2af9a6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af4c083462b1e956e6461aa246d2af9a6">&#9670;&#160;</a></span>CUDA_GUARD</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">#define CUDA_GUARD</td>
          <td>(</td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>call</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Value:</b><div class="fragment"><div class="line">  <span class="keywordflow">do</span> {                                                                       \</div>
<div class="line">    cudaError_t _st = (call);                                                \</div>
<div class="line">    if (_st != cudaSuccess) {                                                \</div>
<div class="line">      ie_cuda_set_err(cudaGetErrorString(_st));                              \</div>
<div class="line">      <span class="keywordflow">return</span> -(int)_st;                                                      \</div>
<div class="line">    }                                                                        \</div>
<div class="line">  } <span class="keywordflow">while</span> (0)</div>
</div><!-- fragment -->
<p>Guard macro for CUDA runtime calls inside launchers. </p>
<p>On failure: records the CUDA error string and returns the negative error code. </p>

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00068">68</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">   68</span><span class="preprocessor">#define CUDA_GUARD(call)                                                     \</span></div>
<div class="line"><span class="lineno">   69</span><span class="preprocessor">  do {                                                                       \</span></div>
<div class="line"><span class="lineno">   70</span><span class="preprocessor">    cudaError_t _st = (call);                                                \</span></div>
<div class="line"><span class="lineno">   71</span><span class="preprocessor">    if (_st != cudaSuccess) {                                                \</span></div>
<div class="line"><span class="lineno">   72</span><span class="preprocessor">      ie_cuda_set_err(cudaGetErrorString(_st));                              \</span></div>
<div class="line"><span class="lineno">   73</span><span class="preprocessor">      return -(int)_st;                                                      \</span></div>
<div class="line"><span class="lineno">   74</span><span class="preprocessor">    }                                                                        \</span></div>
<div class="line"><span class="lineno">   75</span><span class="preprocessor">  } while (0)</span></div>
</div><!-- fragment -->
<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00270">ie_cuda_launch_gemv_bias_act_f32()</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00246">ie_cuda_launch_gemv_rowwise_f32()</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00317">ie_cuda_launch_pack_w_blockedk_f32()</a>, and <a class="el" href="ie__kernels__cuda_8cu_source.html#l00297">ie_cuda_launch_vec_tanh_f32()</a>.</p>

</div>
</div>
<a id="afb3284f12e7d5038612ece3185920449" name="afb3284f12e7d5038612ece3185920449"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afb3284f12e7d5038612ece3185920449">&#9670;&#160;</a></span>IE_CUDA_OK</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">#define IE_CUDA_OK&#160;&#160;&#160;0</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00024">24</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>

<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00270">ie_cuda_launch_gemv_bias_act_f32()</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00246">ie_cuda_launch_gemv_rowwise_f32()</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00317">ie_cuda_launch_pack_w_blockedk_f32()</a>, and <a class="el" href="ie__kernels__cuda_8cu_source.html#l00297">ie_cuda_launch_vec_tanh_f32()</a>.</p>

</div>
</div>
<a id="ad253e9c439228a67be0e55b0f4c81edf" name="ad253e9c439228a67be0e55b0f4c81edf"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad253e9c439228a67be0e55b0f4c81edf">&#9670;&#160;</a></span>IE_CUDA_STREAM_T_DEFINED</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">#define IE_CUDA_STREAM_T_DEFINED</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00027">27</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>

</div>
</div>
<h2 class="groupheader">Typedef Documentation</h2>
<a id="aa036ed43d83d669bfc8c15c81a7efe73" name="aa036ed43d83d669bfc8c15c81a7efe73"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa036ed43d83d669bfc8c15c81a7efe73">&#9670;&#160;</a></span>ie_cuda_stream_t</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">typedef cudaStream_t <a class="el" href="group__IE__GPU.html#gaefb3d7d282a514152960b6c3d812355b">ie_cuda_stream_t</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00028">28</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>

</div>
</div>
<h2 class="groupheader">Function Documentation</h2>
<a id="a603115e5bb835d21609518d0af053e8d" name="a603115e5bb835d21609518d0af053e8d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a603115e5bb835d21609518d0af053e8d">&#9670;&#160;</a></span>ie_apply_activation()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">__device__ float ie_apply_activation </td>
          <td>(</td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>x</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group__IE__GPU.html#ga08ba1d3777f64c23e99124d2e460e701">ie_act_kind_t</a></td>          <td class="paramname"><span class="paramname"><em>act</em></span>&#160;)</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel inline">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Apply activation in an epilogue. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">x</td><td>Input value. </td></tr>
    <tr><td class="paramname">act</td><td>Activation kind (see <a class="el" href="group__IE__GPU.html#ga08ba1d3777f64c23e99124d2e460e701" title="Activation types supported by fused kernels.">ie_act_kind_t</a>). </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Activated value. </dd></dl>

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00088">88</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">   88</span>                                                                        {</div>
<div class="line"><span class="lineno">   89</span>  <span class="keywordflow">if</span> (act == <a class="code hl_enumvalue" href="group__IE__GPU.html#ggab2c254b332e574ed55c2537075337f85a4efb020b6e30222eac585f53f1d3e47f">IE_ACT_RELU</a>) {</div>
<div class="line"><span class="lineno">   90</span>    <span class="keywordflow">return</span> x &gt; 0.f ? x : 0.f;</div>
<div class="line"><span class="lineno">   91</span>  } <span class="keywordflow">else</span> <span class="keywordflow">if</span> (act == <a class="code hl_enumvalue" href="group__IE__GPU.html#ggab2c254b332e574ed55c2537075337f85a5c3ab96a97ea7aa059b44e7508c28a08">IE_ACT_TANH</a>) {</div>
<div class="line"><span class="lineno">   92</span>    <span class="keywordflow">return</span> tanhf(x);</div>
<div class="line"><span class="lineno">   93</span>  }</div>
<div class="line"><span class="lineno">   94</span>  <span class="keywordflow">return</span> x; <span class="comment">/* IE_ACT_NONE */</span></div>
<div class="line"><span class="lineno">   95</span>}</div>
<div class="ttc" id="agroup__IE__GPU_html_ggab2c254b332e574ed55c2537075337f85a4efb020b6e30222eac585f53f1d3e47f"><div class="ttname"><a href="group__IE__GPU.html#ggab2c254b332e574ed55c2537075337f85a4efb020b6e30222eac585f53f1d3e47f">IE_ACT_RELU</a></div><div class="ttdeci">@ IE_ACT_RELU</div><div class="ttdoc">ReLU: max(0, x).</div><div class="ttdef"><b>Definition</b> <a href="ie__kernels__cuda_8h_source.html#l00057">ie_kernels_cuda.h:57</a></div></div>
<div class="ttc" id="agroup__IE__GPU_html_ggab2c254b332e574ed55c2537075337f85a5c3ab96a97ea7aa059b44e7508c28a08"><div class="ttname"><a href="group__IE__GPU.html#ggab2c254b332e574ed55c2537075337f85a5c3ab96a97ea7aa059b44e7508c28a08">IE_ACT_TANH</a></div><div class="ttdeci">@ IE_ACT_TANH</div><div class="ttdoc">Hyperbolic tangent.</div><div class="ttdef"><b>Definition</b> <a href="ie__kernels__cuda_8h_source.html#l00058">ie_kernels_cuda.h:58</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="ie__kernels__cuda_8h_source.html#l00057">IE_ACT_RELU</a>, and <a class="el" href="ie__kernels__cuda_8h_source.html#l00058">IE_ACT_TANH</a>.</p>

<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00163">k_gemv_bias_act_f32()</a>.</p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu_a603115e5bb835d21609518d0af053e8d_icgraph.svg" width="583" height="56"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>

</div>
</div>
<a id="a841d35ecb02f9850f311a5724d21deac" name="a841d35ecb02f9850f311a5724d21deac"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a841d35ecb02f9850f311a5724d21deac">&#9670;&#160;</a></span>ie_cuda_set_err()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">static void ie_cuda_set_err </td>
          <td>(</td>
          <td class="paramtype">const char *</td>          <td class="paramname"><span class="paramname"><em>msg</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel inline">inline</span><span class="mlabel static">static</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Store an error message into the thread-local buffer (truncates). </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">msg</td><td>NUL-terminated message to copy; if <code>NULL</code>, clears the buffer. </td></tr>
  </table>
  </dd>
</dl>

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00049">49</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">   49</span>                                                    {</div>
<div class="line"><span class="lineno">   50</span>  <span class="keywordflow">if</span> (!msg) { <a class="code hl_variable" href="#a5ab38bcd2467e982c68d46e66acfa9ef">g_ie_cuda_err</a>[0] = <span class="charliteral">&#39;\0&#39;</span>; <span class="keywordflow">return</span>; }</div>
<div class="line"><span class="lineno">   51</span>  <span class="keywordtype">size_t</span> i = 0;</div>
<div class="line"><span class="lineno">   52</span>  <span class="keywordflow">for</span> (; i + 1 &lt; <span class="keyword">sizeof</span>(<a class="code hl_variable" href="#a5ab38bcd2467e982c68d46e66acfa9ef">g_ie_cuda_err</a>) &amp;&amp; msg[i]; ++i) <a class="code hl_variable" href="#a5ab38bcd2467e982c68d46e66acfa9ef">g_ie_cuda_err</a>[i] = msg[i];</div>
<div class="line"><span class="lineno">   53</span>  <a class="code hl_variable" href="#a5ab38bcd2467e982c68d46e66acfa9ef">g_ie_cuda_err</a>[i] = <span class="charliteral">&#39;\0&#39;</span>;</div>
<div class="line"><span class="lineno">   54</span>}</div>
<div class="ttc" id="aie__kernels__cuda_8cu_html_a5ab38bcd2467e982c68d46e66acfa9ef"><div class="ttname"><a href="#a5ab38bcd2467e982c68d46e66acfa9ef">g_ie_cuda_err</a></div><div class="ttdeci">static __thread char g_ie_cuda_err[256]</div><div class="ttdoc">Thread-local buffer holding the most recent CUDA error message.</div><div class="ttdef"><b>Definition</b> <a href="ie__kernels__cuda_8cu_source.html#l00042">ie_kernels_cuda.cu:42</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="ie__kernels__cuda_8cu_source.html#l00042">g_ie_cuda_err</a>.</p>

<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00270">ie_cuda_launch_gemv_bias_act_f32()</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00246">ie_cuda_launch_gemv_rowwise_f32()</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00317">ie_cuda_launch_pack_w_blockedk_f32()</a>, and <a class="el" href="ie__kernels__cuda_8cu_source.html#l00297">ie_cuda_launch_vec_tanh_f32()</a>.</p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu_a841d35ecb02f9850f311a5724d21deac_icgraph.svg" width="348" height="264"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>

</div>
</div>
<a id="ab9150fe6fc7da507974bfdd38b262ce2" name="ab9150fe6fc7da507974bfdd38b262ce2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab9150fe6fc7da507974bfdd38b262ce2">&#9670;&#160;</a></span>k_gemv_bias_act_f32()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">__global__ void k_gemv_bias_act_f32 </td>
          <td>(</td>
          <td class="paramtype">const float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>W</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>x</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>bias</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>y</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>rows</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>cols</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>ldW</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>alpha</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>beta</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group__IE__GPU.html#ga08ba1d3777f64c23e99124d2e460e701">ie_act_kind_t</a></td>          <td class="paramname"><span class="paramname"><em>act</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Kernel: fused GEMV + bias + activation. </p>
<p>Computes <code>y = act(alpha * W*x + bias + beta*y)</code>.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">W</td><td>Row-major matrix, <code>rows x ldW</code> (device). </td></tr>
    <tr><td class="paramname">x</td><td>Input vector of length <code>cols</code> (device). </td></tr>
    <tr><td class="paramname">bias</td><td>Per-row bias of length <code>rows</code> (device, may be NULL). </td></tr>
    <tr><td class="paramname">y</td><td>Output vector of length <code>rows</code> (device). </td></tr>
    <tr><td class="paramname">rows</td><td>Number of rows / elements in <code>y</code>. </td></tr>
    <tr><td class="paramname">cols</td><td>Number of columns / elements in <code>x</code>. </td></tr>
    <tr><td class="paramname">ldW</td><td>Leading dimension (&gt;= cols). </td></tr>
    <tr><td class="paramname">alpha</td><td>Scale for <code>W*x</code>. </td></tr>
    <tr><td class="paramname">beta</td><td>Scale for existing <code>y</code>. </td></tr>
    <tr><td class="paramname">act</td><td>Activation kind. </td></tr>
  </table>
  </dd>
</dl>

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00163">163</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  169</span>                                                       {</div>
<div class="line"><span class="lineno">  170</span>  <span class="keywordtype">int</span> r = blockIdx.x;</div>
<div class="line"><span class="lineno">  171</span>  <span class="keywordflow">if</span> (r &gt;= rows) <span class="keywordflow">return</span>;</div>
<div class="line"><span class="lineno">  172</span> </div>
<div class="line"><span class="lineno">  173</span>  <span class="keywordtype">float</span> acc = 0.f;</div>
<div class="line"><span class="lineno">  174</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> k = threadIdx.x; k &lt; cols; k += blockDim.x) {</div>
<div class="line"><span class="lineno">  175</span>    acc += W[(size_t)r * (<span class="keywordtype">size_t</span>)ldW + (size_t)k] * x[k];</div>
<div class="line"><span class="lineno">  176</span>  }</div>
<div class="line"><span class="lineno">  177</span> </div>
<div class="line"><span class="lineno">  178</span>  __shared__ <span class="keywordtype">float</span> buf[256];</div>
<div class="line"><span class="lineno">  179</span>  buf[threadIdx.x] = acc;</div>
<div class="line"><span class="lineno">  180</span>  __syncthreads();</div>
<div class="line"><span class="lineno">  181</span> </div>
<div class="line"><span class="lineno">  182</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> s = blockDim.x &gt;&gt; 1; s &gt; 0; s &gt;&gt;= 1) {</div>
<div class="line"><span class="lineno">  183</span>    <span class="keywordflow">if</span> (threadIdx.x &lt; s) buf[threadIdx.x] += buf[threadIdx.x + s];</div>
<div class="line"><span class="lineno">  184</span>    __syncthreads();</div>
<div class="line"><span class="lineno">  185</span>  }</div>
<div class="line"><span class="lineno">  186</span> </div>
<div class="line"><span class="lineno">  187</span>  <span class="keywordflow">if</span> (threadIdx.x == 0) {</div>
<div class="line"><span class="lineno">  188</span>    <span class="keywordtype">float</span> b = bias ? bias[r] : 0.f;</div>
<div class="line"><span class="lineno">  189</span>    <span class="keywordtype">float</span> out = alpha * buf[0] + b;</div>
<div class="line"><span class="lineno">  190</span>    <span class="keywordflow">if</span> (beta != 0.f) out += beta * y[r];</div>
<div class="line"><span class="lineno">  191</span>    y[r] = <a class="code hl_function" href="#a603115e5bb835d21609518d0af053e8d">ie_apply_activation</a>(out, act);</div>
<div class="line"><span class="lineno">  192</span>  }</div>
<div class="line"><span class="lineno">  193</span>}</div>
<div class="ttc" id="aie__kernels__cuda_8cu_html_a603115e5bb835d21609518d0af053e8d"><div class="ttname"><a href="#a603115e5bb835d21609518d0af053e8d">ie_apply_activation</a></div><div class="ttdeci">__device__ float ie_apply_activation(float x, ie_act_kind_t act)</div><div class="ttdoc">Apply activation in an epilogue.</div><div class="ttdef"><b>Definition</b> <a href="ie__kernels__cuda_8cu_source.html#l00088">ie_kernels_cuda.cu:88</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="ie__kernels__cuda_8cu_source.html#l00088">ie_apply_activation()</a>.</p>

<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00270">ie_cuda_launch_gemv_bias_act_f32()</a>.</p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu_ab9150fe6fc7da507974bfdd38b262ce2_cgraph.svg" width="370" height="39"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu_ab9150fe6fc7da507974bfdd38b262ce2_icgraph.svg" width="387" height="56"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>

</div>
</div>
<a id="af00402674d56c833c92c62df1cef1e84" name="af00402674d56c833c92c62df1cef1e84"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af00402674d56c833c92c62df1cef1e84">&#9670;&#160;</a></span>k_gemv_rowwise_f32()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">__global__ void k_gemv_rowwise_f32 </td>
          <td>(</td>
          <td class="paramtype">const float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>W</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>x</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>y</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>rows</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>cols</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>ldW</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>alpha</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>beta</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Kernel: row-wise GEMV (computes <code>y = alpha * W * x + beta * y</code>). </p>
<p><b>Mapping:</b> one block per output row; threads perform a strided reduction along the K (column) dimension; partial sums are reduced in shared memory.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">W</td><td>Row-major matrix, <code>rows x ldW</code> (device). </td></tr>
    <tr><td class="paramname">x</td><td>Input vector of length <code>cols</code> (device). </td></tr>
    <tr><td class="paramname">y</td><td>Output vector of length <code>rows</code> (device). </td></tr>
    <tr><td class="paramname">rows</td><td>Number of rows in <code>W</code> and elements in <code>y</code>. </td></tr>
    <tr><td class="paramname">cols</td><td>Number of columns in <code>W</code> and elements in <code>x</code>. </td></tr>
    <tr><td class="paramname">ldW</td><td>Leading dimension (&gt;= cols). </td></tr>
    <tr><td class="paramname">alpha</td><td>Scale for <code>W*x</code>. </td></tr>
    <tr><td class="paramname">beta</td><td>Scale for existing <code>y</code>. </td></tr>
  </table>
  </dd>
</dl>

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00116">116</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  120</span>                                                            {</div>
<div class="line"><span class="lineno">  121</span>  <span class="keywordtype">int</span> r = blockIdx.x;</div>
<div class="line"><span class="lineno">  122</span>  <span class="keywordflow">if</span> (r &gt;= rows) <span class="keywordflow">return</span>;</div>
<div class="line"><span class="lineno">  123</span> </div>
<div class="line"><span class="lineno">  124</span>  <span class="keywordtype">float</span> acc = 0.f;</div>
<div class="line"><span class="lineno">  125</span>  <span class="comment">/* Parallel dot-product across K with a strided loop. */</span></div>
<div class="line"><span class="lineno">  126</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> k = threadIdx.x; k &lt; cols; k += blockDim.x) {</div>
<div class="line"><span class="lineno">  127</span>    acc += W[(size_t)r * (<span class="keywordtype">size_t</span>)ldW + (size_t)k] * x[k];</div>
<div class="line"><span class="lineno">  128</span>  }</div>
<div class="line"><span class="lineno">  129</span> </div>
<div class="line"><span class="lineno">  130</span>  <span class="comment">/* Shared-memory reduction within the block. */</span></div>
<div class="line"><span class="lineno">  131</span>  __shared__ <span class="keywordtype">float</span> buf[256];</div>
<div class="line"><span class="lineno">  132</span>  buf[threadIdx.x] = acc;</div>
<div class="line"><span class="lineno">  133</span>  __syncthreads();</div>
<div class="line"><span class="lineno">  134</span> </div>
<div class="line"><span class="lineno">  135</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> s = blockDim.x &gt;&gt; 1; s &gt; 0; s &gt;&gt;= 1) {</div>
<div class="line"><span class="lineno">  136</span>    <span class="keywordflow">if</span> (threadIdx.x &lt; s) buf[threadIdx.x] += buf[threadIdx.x + s];</div>
<div class="line"><span class="lineno">  137</span>    __syncthreads();</div>
<div class="line"><span class="lineno">  138</span>  }</div>
<div class="line"><span class="lineno">  139</span> </div>
<div class="line"><span class="lineno">  140</span>  <span class="keywordflow">if</span> (threadIdx.x == 0) {</div>
<div class="line"><span class="lineno">  141</span>    <span class="keywordtype">float</span> out = alpha * buf[0];</div>
<div class="line"><span class="lineno">  142</span>    <span class="keywordflow">if</span> (beta != 0.f) out += beta * y[r];</div>
<div class="line"><span class="lineno">  143</span>    y[r] = out;</div>
<div class="line"><span class="lineno">  144</span>  }</div>
<div class="line"><span class="lineno">  145</span>}</div>
</div><!-- fragment -->
<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00246">ie_cuda_launch_gemv_rowwise_f32()</a>.</p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu_af00402674d56c833c92c62df1cef1e84_icgraph.svg" width="387" height="56"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>

</div>
</div>
<a id="ac440d51163bc9f16e835d00aedcb4b79" name="ac440d51163bc9f16e835d00aedcb4b79"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac440d51163bc9f16e835d00aedcb4b79">&#9670;&#160;</a></span>k_pack_w_blockedk_f32()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">__global__ void k_pack_w_blockedk_f32 </td>
          <td>(</td>
          <td class="paramtype">float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>Wp</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>W</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>rows</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>cols</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>ldW</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>block_k</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Kernel: pack row-major <code>W</code> into Blocked-K layout (see header docs). </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">Wp</td><td>Destination (device). </td></tr>
    <tr><td class="paramname">W</td><td>Source row-major (device). </td></tr>
    <tr><td class="paramname">rows</td><td>Rows. </td></tr>
    <tr><td class="paramname">cols</td><td>Cols. </td></tr>
    <tr><td class="paramname">ldW</td><td>Leading dimension of <code>W</code>. </td></tr>
    <tr><td class="paramname">block_k</td><td>Tile size along K. </td></tr>
  </table>
  </dd>
</dl>

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00222">222</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  225</span>                                                   {</div>
<div class="line"><span class="lineno">  226</span>  <span class="keywordtype">int</span> r0 = blockIdx.y * blockDim.y + threadIdx.y;</div>
<div class="line"><span class="lineno">  227</span>  <span class="keywordtype">int</span> k0 = blockIdx.x * blockDim.x + threadIdx.x;</div>
<div class="line"><span class="lineno">  228</span> </div>
<div class="line"><span class="lineno">  229</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> r = r0; r &lt; rows; r += blockDim.y * gridDim.y) {</div>
<div class="line"><span class="lineno">  230</span>    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> k = k0; k &lt; cols; k += blockDim.x * gridDim.x) {</div>
<div class="line"><span class="lineno">  231</span>      <span class="keywordtype">int</span> kb = k / block_k;</div>
<div class="line"><span class="lineno">  232</span>      <span class="keywordtype">int</span> ko = k % block_k;</div>
<div class="line"><span class="lineno">  233</span>      <span class="keywordtype">size_t</span> dst = ((size_t)kb * (size_t)rows + (size_t)r) * (<span class="keywordtype">size_t</span>)block_k + (size_t)ko;</div>
<div class="line"><span class="lineno">  234</span>      Wp[dst] = W[(size_t)r * (<span class="keywordtype">size_t</span>)ldW + (size_t)k];</div>
<div class="line"><span class="lineno">  235</span>    }</div>
<div class="line"><span class="lineno">  236</span>  }</div>
<div class="line"><span class="lineno">  237</span>}</div>
</div><!-- fragment -->
<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00317">ie_cuda_launch_pack_w_blockedk_f32()</a>.</p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu_ac440d51163bc9f16e835d00aedcb4b79_icgraph.svg" width="396" height="56"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>

</div>
</div>
<a id="aea063f21d4bdf3ba4108f9677476aad4" name="aea063f21d4bdf3ba4108f9677476aad4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aea063f21d4bdf3ba4108f9677476aad4">&#9670;&#160;</a></span>k_vec_tanh_f32()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">__global__ void k_vec_tanh_f32 </td>
          <td>(</td>
          <td class="paramtype">float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>y</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>x</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>n</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Kernel: elementwise <code>tanh</code> on a vector. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">y</td><td>Output vector (device). </td></tr>
    <tr><td class="paramname">x</td><td>Input vector (device). </td></tr>
    <tr><td class="paramname">n</td><td>Number of elements. </td></tr>
  </table>
  </dd>
</dl>

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00202">202</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  204</span>                                      {</div>
<div class="line"><span class="lineno">  205</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</div>
<div class="line"><span class="lineno">  206</span>       i &lt; n;</div>
<div class="line"><span class="lineno">  207</span>       i += blockDim.x * gridDim.x) {</div>
<div class="line"><span class="lineno">  208</span>    y[i] = tanhf(x[i]);</div>
<div class="line"><span class="lineno">  209</span>  }</div>
<div class="line"><span class="lineno">  210</span>}</div>
</div><!-- fragment -->
<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00297">ie_cuda_launch_vec_tanh_f32()</a>.</p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu_aea063f21d4bdf3ba4108f9677476aad4_icgraph.svg" width="334" height="56"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>

</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="a5ab38bcd2467e982c68d46e66acfa9ef" name="a5ab38bcd2467e982c68d46e66acfa9ef"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5ab38bcd2467e982c68d46e66acfa9ef">&#9670;&#160;</a></span>g_ie_cuda_err</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">__thread char g_ie_cuda_err[256] = {0}</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel static">static</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Thread-local buffer holding the most recent CUDA error message. </p>
<p>The C-ABI launchers store any CUDA error textual description here so callers can retrieve it via <a class="el" href="group__IE__GPU.html#ga6656c0ddd9df382048e7cd162e0c1d64" title="Return a thread-local message describing the last CUDA error.">ie_cuda_last_error_string()</a>. </p>

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00042">42</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">   42</span>{0};</div>
</div><!-- fragment -->
<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00059">ie_cuda_last_error_string()</a>, and <a class="el" href="ie__kernels__cuda_8cu_source.html#l00049">ie_cuda_set_err()</a>.</p>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="dir_996f45160da62e1a3d7f6046fad68f51.html">engine</a></li><li class="navelem"><a class="el" href="dir_3d9126aa00c041bc0b8f859d1965a0f4.html">src</a></li><li class="navelem"><a class="el" href="dir_2151caada9eddb57703a29b50b7782f8.html">kernels</a></li><li class="navelem"><a class="el" href="ie__kernels__cuda_8cu.html">ie_kernels_cuda.cu</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
