{
  "format": "iebin_v1_raw_safetensors",
  "hf_dir": "/home/ricardomag/Desktop/Clocher/models/gpt-oss-20b/hf/original",
  "tensors": [
    {
      "name": "model.layers.0.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 0,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 0
    },
    {
      "name": "model.layers.0.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 5760,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 5760
    },
    {
      "name": "model.layers.0.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 190080,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 562106240
    },
    {
      "name": "model.layers.0.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 132900480,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 694816640
    },
    {
      "name": "model.layers.0.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 141194880,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 190080
    },
    {
      "name": "model.layers.0.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 141563520,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 703111040
    },
    {
      "name": "model.layers.0.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 406984320,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 968531840
    },
    {
      "name": "model.layers.0.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 423573120,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 558720
    },
    {
      "name": "model.layers.0.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 423573184,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 558784
    },
    {
      "name": "model.layers.0.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 423757504,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 743104
    },
    {
      "name": "model.layers.0.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 423763264,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 748864
    },
    {
      "name": "model.layers.0.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 423764288,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 749888
    },
    {
      "name": "model.layers.0.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 426713408,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 3699008
    },
    {
      "name": "model.layers.0.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 426719168,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 3704768
    },
    {
      "name": "model.layers.0.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 450312128,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 27297728
    },
    {
      "name": "model.layers.0.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 450320320,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 27305920
    },
    {
      "name": "model.layers.0.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 473913280,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 50898880
    },
    {
      "name": "model.layers.0.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 473913408,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 50899008
    },
    {
      "name": "model.layers.0.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 473914432,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 50900032
    },
    {
      "name": "model.layers.1.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 476863552,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 53849152
    },
    {
      "name": "model.layers.1.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 476869312,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 53854912
    },
    {
      "name": "model.layers.1.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 477053632,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 985120640
    },
    {
      "name": "model.layers.1.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 609764032,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 1117831040
    },
    {
      "name": "model.layers.1.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 618058432,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 54039232
    },
    {
      "name": "model.layers.1.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 618427072,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 1126125440
    },
    {
      "name": "model.layers.1.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 883847872,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 1391546240
    },
    {
      "name": "model.layers.1.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 900436672,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 54407872
    },
    {
      "name": "model.layers.1.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 900436736,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 54407936
    },
    {
      "name": "model.layers.1.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 900621056,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 54592256
    },
    {
      "name": "model.layers.1.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 900626816,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 54598016
    },
    {
      "name": "model.layers.1.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 900627840,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 54599040
    },
    {
      "name": "model.layers.1.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 903576960,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 57548160
    },
    {
      "name": "model.layers.1.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 903582720,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 57553920
    },
    {
      "name": "model.layers.1.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 927175680,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 81146880
    },
    {
      "name": "model.layers.1.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 927183872,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 81155072
    },
    {
      "name": "model.layers.1.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 950776832,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 104748032
    },
    {
      "name": "model.layers.1.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 950776960,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 104748160
    },
    {
      "name": "model.layers.1.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 950777984,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 104749184
    },
    {
      "name": "model.layers.10.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 953727104,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 107698304
    },
    {
      "name": "model.layers.10.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 953732864,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 107704064
    },
    {
      "name": "model.layers.10.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 953917184,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 1408135040
    },
    {
      "name": "model.layers.10.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 1086627584,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 1540845440
    },
    {
      "name": "model.layers.10.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 1094921984,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 107888384
    },
    {
      "name": "model.layers.10.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 1095290624,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 1549139840
    },
    {
      "name": "model.layers.10.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 1360711424,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 1814560640
    },
    {
      "name": "model.layers.10.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 1377300224,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 108257024
    },
    {
      "name": "model.layers.10.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 1377300288,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 108257088
    },
    {
      "name": "model.layers.10.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 1377484608,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 108441408
    },
    {
      "name": "model.layers.10.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 1377490368,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 108447168
    },
    {
      "name": "model.layers.10.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 1377491392,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 108448192
    },
    {
      "name": "model.layers.10.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 1380440512,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 111397312
    },
    {
      "name": "model.layers.10.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 1380446272,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 111403072
    },
    {
      "name": "model.layers.10.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 1404039232,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 134996032
    },
    {
      "name": "model.layers.10.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 1404047424,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 135004224
    },
    {
      "name": "model.layers.10.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 1427640384,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 158597184
    },
    {
      "name": "model.layers.10.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 1427640512,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 158597312
    },
    {
      "name": "model.layers.10.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 1427641536,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 158598336
    },
    {
      "name": "model.layers.11.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 1430590656,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 161547456
    },
    {
      "name": "model.layers.11.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 1430596416,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 161553216
    },
    {
      "name": "model.layers.11.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 1430780736,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 1831149440
    },
    {
      "name": "model.layers.11.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 1563491136,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 1963859840
    },
    {
      "name": "model.layers.11.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 1571785536,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 161737536
    },
    {
      "name": "model.layers.11.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 1572154176,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 1972154240
    },
    {
      "name": "model.layers.11.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 1837574976,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 2237575040
    },
    {
      "name": "model.layers.11.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 1854163776,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 162106176
    },
    {
      "name": "model.layers.11.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 1854163840,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 162106240
    },
    {
      "name": "model.layers.11.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 1854348160,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 162290560
    },
    {
      "name": "model.layers.11.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 1854353920,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 162296320
    },
    {
      "name": "model.layers.11.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 1854354944,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 162297344
    },
    {
      "name": "model.layers.11.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 1857304064,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 165246464
    },
    {
      "name": "model.layers.11.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 1857309824,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 165252224
    },
    {
      "name": "model.layers.11.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 1880902784,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 188845184
    },
    {
      "name": "model.layers.11.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 1880910976,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 188853376
    },
    {
      "name": "model.layers.11.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 1904503936,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 212446336
    },
    {
      "name": "model.layers.11.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 1904504064,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 212446464
    },
    {
      "name": "model.layers.11.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 1904505088,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 212447488
    },
    {
      "name": "model.layers.12.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 1907454208,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 215396608
    },
    {
      "name": "model.layers.12.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 1907459968,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 215402368
    },
    {
      "name": "model.layers.12.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 1907644288,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 2254163840
    },
    {
      "name": "model.layers.12.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 2040354688,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 2386874240
    },
    {
      "name": "model.layers.12.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 2048649088,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 215586688
    },
    {
      "name": "model.layers.12.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 2049017728,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 2395168640
    },
    {
      "name": "model.layers.12.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 2314438528,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 2660589440
    },
    {
      "name": "model.layers.12.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 2331027328,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 215955328
    },
    {
      "name": "model.layers.12.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 2331027392,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 215955392
    },
    {
      "name": "model.layers.12.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 2331211712,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 216139712
    },
    {
      "name": "model.layers.12.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 2331217472,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 216145472
    },
    {
      "name": "model.layers.12.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 2331218496,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 216146496
    },
    {
      "name": "model.layers.12.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 2334167616,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 219095616
    },
    {
      "name": "model.layers.12.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 2334173376,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 219101376
    },
    {
      "name": "model.layers.12.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 2357766336,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 242694336
    },
    {
      "name": "model.layers.12.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 2357774528,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 242702528
    },
    {
      "name": "model.layers.12.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 2381367488,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 266295488
    },
    {
      "name": "model.layers.12.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 2381367616,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 266295616
    },
    {
      "name": "model.layers.12.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 2381368640,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 266296640
    },
    {
      "name": "model.layers.13.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 2384317760,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 269245760
    },
    {
      "name": "model.layers.13.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 2384323520,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 269251520
    },
    {
      "name": "model.layers.13.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 2384507840,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 2677178240
    },
    {
      "name": "model.layers.13.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 2517218240,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 2809888640
    },
    {
      "name": "model.layers.13.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 2525512640,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 269435840
    },
    {
      "name": "model.layers.13.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 2525881280,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 2818183040
    },
    {
      "name": "model.layers.13.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 2791302080,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 3083603840
    },
    {
      "name": "model.layers.13.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 2807890880,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 269804480
    },
    {
      "name": "model.layers.13.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 2807890944,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 269804544
    },
    {
      "name": "model.layers.13.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 2808075264,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 269988864
    },
    {
      "name": "model.layers.13.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 2808081024,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 269994624
    },
    {
      "name": "model.layers.13.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 2808082048,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 269995648
    },
    {
      "name": "model.layers.13.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 2811031168,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 272944768
    },
    {
      "name": "model.layers.13.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 2811036928,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 272950528
    },
    {
      "name": "model.layers.13.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 2834629888,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 296543488
    },
    {
      "name": "model.layers.13.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 2834638080,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 296551680
    },
    {
      "name": "model.layers.13.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 2858231040,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 320144640
    },
    {
      "name": "model.layers.13.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 2858231168,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 320144768
    },
    {
      "name": "model.layers.13.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 2858232192,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 320145792
    },
    {
      "name": "model.layers.14.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 2861181312,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 323094912
    },
    {
      "name": "model.layers.14.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 2861187072,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 323100672
    },
    {
      "name": "model.layers.14.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 2861371392,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 3100192640
    },
    {
      "name": "model.layers.14.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 2994081792,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 3232903040
    },
    {
      "name": "model.layers.14.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 3002376192,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 323284992
    },
    {
      "name": "model.layers.14.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 3002744832,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 3241197440
    },
    {
      "name": "model.layers.14.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 3268165632,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 3506618240
    },
    {
      "name": "model.layers.14.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 3284754432,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 323653632
    },
    {
      "name": "model.layers.14.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 3284754496,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 323653696
    },
    {
      "name": "model.layers.14.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 3284938816,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 323838016
    },
    {
      "name": "model.layers.14.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 3284944576,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 323843776
    },
    {
      "name": "model.layers.14.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 3284945600,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 323844800
    },
    {
      "name": "model.layers.14.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 3287894720,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 326793920
    },
    {
      "name": "model.layers.14.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 3287900480,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 326799680
    },
    {
      "name": "model.layers.14.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 3311493440,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 350392640
    },
    {
      "name": "model.layers.14.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 3311501632,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 350400832
    },
    {
      "name": "model.layers.14.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 3335094592,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 373993792
    },
    {
      "name": "model.layers.14.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 3335094720,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 373993920
    },
    {
      "name": "model.layers.14.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 3335095744,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 373994944
    },
    {
      "name": "model.layers.15.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 3338044864,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 376944064
    },
    {
      "name": "model.layers.15.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 3338050624,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 376949824
    },
    {
      "name": "model.layers.15.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 3338234944,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 3523207040
    },
    {
      "name": "model.layers.15.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 3470945344,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 3655917440
    },
    {
      "name": "model.layers.15.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 3479239744,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 377134144
    },
    {
      "name": "model.layers.15.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 3479608384,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 3664211840
    },
    {
      "name": "model.layers.15.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 3745029184,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 3929632640
    },
    {
      "name": "model.layers.15.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 3761617984,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 377502784
    },
    {
      "name": "model.layers.15.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 3761618048,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 377502848
    },
    {
      "name": "model.layers.15.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 3761802368,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 377687168
    },
    {
      "name": "model.layers.15.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 3761808128,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 377692928
    },
    {
      "name": "model.layers.15.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 3761809152,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 377693952
    },
    {
      "name": "model.layers.15.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 3764758272,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 380643072
    },
    {
      "name": "model.layers.15.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 3764764032,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 380648832
    },
    {
      "name": "model.layers.15.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 3788356992,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 404241792
    },
    {
      "name": "model.layers.15.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 3788365184,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 404249984
    },
    {
      "name": "model.layers.15.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 3811958144,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 427842944
    },
    {
      "name": "model.layers.15.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 3811958272,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 427843072
    },
    {
      "name": "model.layers.15.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 3811959296,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 427844096
    },
    {
      "name": "model.layers.16.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 3814908416,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 430793216
    },
    {
      "name": "model.layers.16.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 3814914176,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 430798976
    },
    {
      "name": "model.layers.16.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 3815098496,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 3946221440
    },
    {
      "name": "model.layers.16.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 3947808896,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 4078931840
    },
    {
      "name": "model.layers.16.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 3956103296,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 430983296
    },
    {
      "name": "model.layers.16.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 3956471936,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 4087226240
    },
    {
      "name": "model.layers.16.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 4221892736,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 4352647040
    },
    {
      "name": "model.layers.16.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 4238481536,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 431351936
    },
    {
      "name": "model.layers.16.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 4238481600,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 431352000
    },
    {
      "name": "model.layers.16.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 4238665920,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 431536320
    },
    {
      "name": "model.layers.16.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 4238671680,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 431542080
    },
    {
      "name": "model.layers.16.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 4238672704,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 431543104
    },
    {
      "name": "model.layers.16.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 4241621824,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 434492224
    },
    {
      "name": "model.layers.16.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 4241627584,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 434497984
    },
    {
      "name": "model.layers.16.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 4265220544,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 458090944
    },
    {
      "name": "model.layers.16.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 4265228736,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 458099136
    },
    {
      "name": "model.layers.16.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 4288821696,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 481692096
    },
    {
      "name": "model.layers.16.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 4288821824,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 481692224
    },
    {
      "name": "model.layers.16.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 4288822848,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 481693248
    },
    {
      "name": "model.layers.17.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 4291771968,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 484642368
    },
    {
      "name": "model.layers.17.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 4291777728,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 484648128
    },
    {
      "name": "model.layers.17.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 4291962048,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 4369235840
    },
    {
      "name": "model.layers.17.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 4424672448,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 4501946240
    },
    {
      "name": "model.layers.17.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 4432966848,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 484832448
    },
    {
      "name": "model.layers.17.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 4433335488,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 4510240640
    },
    {
      "name": "model.layers.17.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 4698756288,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 4775661440
    },
    {
      "name": "model.layers.17.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 4715345088,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 485201088
    },
    {
      "name": "model.layers.17.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 4715345152,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 485201152
    },
    {
      "name": "model.layers.17.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 4715529472,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 485385472
    },
    {
      "name": "model.layers.17.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 4715535232,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 485391232
    },
    {
      "name": "model.layers.17.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 4715536256,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 485392256
    },
    {
      "name": "model.layers.17.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 4718485376,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 488341376
    },
    {
      "name": "model.layers.17.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 4718491136,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 488347136
    },
    {
      "name": "model.layers.17.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 4742084096,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 511940096
    },
    {
      "name": "model.layers.17.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 4742092288,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 511948288
    },
    {
      "name": "model.layers.17.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 4765685248,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 535541248
    },
    {
      "name": "model.layers.17.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 4765685376,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 535541376
    },
    {
      "name": "model.layers.17.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 4765686400,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 535542400
    },
    {
      "name": "model.layers.18.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 4768635520,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 538491520
    },
    {
      "name": "model.layers.18.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 4768641280,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 538497280
    },
    {
      "name": "model.layers.18.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 4768642304,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 538498304
    },
    {
      "name": "model.layers.18.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 4768648064,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 538504064
    },
    {
      "name": "model.layers.18.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 4792241024,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 562097024
    },
    {
      "name": "model.layers.18.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 4792249216,
      "shard": "model-00000-of-00002.safetensors",
      "shard_data_offset": 562105216
    },
    {
      "name": "model.layers.18.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 4792250240,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 0
    },
    {
      "name": "model.layers.18.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 4792434560,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 568535872
    },
    {
      "name": "model.layers.18.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 4925144960,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 701246272
    },
    {
      "name": "model.layers.18.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 4933439360,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 184320
    },
    {
      "name": "model.layers.18.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 4933808000,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 709540672
    },
    {
      "name": "model.layers.18.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 5199228800,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 974961472
    },
    {
      "name": "model.layers.18.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 5215817600,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 552960
    },
    {
      "name": "model.layers.18.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 5215817664,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 553024
    },
    {
      "name": "model.layers.18.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 5216001984,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 737344
    },
    {
      "name": "model.layers.18.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 5216007744,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 743104
    },
    {
      "name": "model.layers.18.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 5218956864,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 3692224
    },
    {
      "name": "model.layers.18.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 5242549824,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 27285184
    },
    {
      "name": "model.layers.18.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 5242549952,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 27285312
    },
    {
      "name": "model.layers.19.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 5245499072,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 30234432
    },
    {
      "name": "model.layers.19.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 5245504832,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 30240192
    },
    {
      "name": "model.layers.19.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 5245689152,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 991550272
    },
    {
      "name": "model.layers.19.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 5378399552,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 1124260672
    },
    {
      "name": "model.layers.19.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 5386693952,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 30424512
    },
    {
      "name": "model.layers.19.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 5387062592,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 1132555072
    },
    {
      "name": "model.layers.19.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 5652483392,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 1397975872
    },
    {
      "name": "model.layers.19.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 5669072192,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 30793152
    },
    {
      "name": "model.layers.19.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 5669072256,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 30793216
    },
    {
      "name": "model.layers.19.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 5669256576,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 30977536
    },
    {
      "name": "model.layers.19.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 5669262336,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 30983296
    },
    {
      "name": "model.layers.19.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 5669263360,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 30984320
    },
    {
      "name": "model.layers.19.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 5672212480,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 33933440
    },
    {
      "name": "model.layers.19.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 5672218240,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 33939200
    },
    {
      "name": "model.layers.19.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 5695811200,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 57532160
    },
    {
      "name": "model.layers.19.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 5695819392,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 57540352
    },
    {
      "name": "model.layers.19.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 5719412352,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 81133312
    },
    {
      "name": "model.layers.19.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 5719412480,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 81133440
    },
    {
      "name": "model.layers.19.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 5719413504,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 81134464
    },
    {
      "name": "model.layers.2.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 5722362624,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 84083584
    },
    {
      "name": "model.layers.2.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 5722368384,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 84089344
    },
    {
      "name": "model.layers.2.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 5722552704,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 1414564672
    },
    {
      "name": "model.layers.2.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 5855263104,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 1547275072
    },
    {
      "name": "model.layers.2.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 5863557504,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 84273664
    },
    {
      "name": "model.layers.2.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 5863926144,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 1555569472
    },
    {
      "name": "model.layers.2.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 6129346944,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 1820990272
    },
    {
      "name": "model.layers.2.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 6145935744,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 84642304
    },
    {
      "name": "model.layers.2.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 6145935808,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 84642368
    },
    {
      "name": "model.layers.2.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 6146120128,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 84826688
    },
    {
      "name": "model.layers.2.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 6146125888,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 84832448
    },
    {
      "name": "model.layers.2.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 6146126912,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 84833472
    },
    {
      "name": "model.layers.2.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 6149076032,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 87782592
    },
    {
      "name": "model.layers.2.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 6149081792,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 87788352
    },
    {
      "name": "model.layers.2.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 6172674752,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 111381312
    },
    {
      "name": "model.layers.2.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 6172682944,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 111389504
    },
    {
      "name": "model.layers.2.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 6196275904,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 134982464
    },
    {
      "name": "model.layers.2.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 6196276032,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 134982592
    },
    {
      "name": "model.layers.2.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 6196277056,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 134983616
    },
    {
      "name": "model.layers.20.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 6199226176,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 137932736
    },
    {
      "name": "model.layers.20.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 6199231936,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 137938496
    },
    {
      "name": "model.layers.20.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 6199416256,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 1837579072
    },
    {
      "name": "model.layers.20.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 6332126656,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 1970289472
    },
    {
      "name": "model.layers.20.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 6340421056,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 138122816
    },
    {
      "name": "model.layers.20.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 6340789696,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 1978583872
    },
    {
      "name": "model.layers.20.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 6606210496,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 2244004672
    },
    {
      "name": "model.layers.20.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 6622799296,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 138491456
    },
    {
      "name": "model.layers.20.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 6622799360,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 138491520
    },
    {
      "name": "model.layers.20.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 6622983680,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 138675840
    },
    {
      "name": "model.layers.20.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 6622989440,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 138681600
    },
    {
      "name": "model.layers.20.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 6622990464,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 138682624
    },
    {
      "name": "model.layers.20.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 6625939584,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 141631744
    },
    {
      "name": "model.layers.20.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 6625945344,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 141637504
    },
    {
      "name": "model.layers.20.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 6649538304,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 165230464
    },
    {
      "name": "model.layers.20.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 6649546496,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 165238656
    },
    {
      "name": "model.layers.20.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 6673139456,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 188831616
    },
    {
      "name": "model.layers.20.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 6673139584,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 188831744
    },
    {
      "name": "model.layers.20.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 6673140608,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 188832768
    },
    {
      "name": "model.layers.21.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 6676089728,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 191781888
    },
    {
      "name": "model.layers.21.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 6676095488,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 191787648
    },
    {
      "name": "model.layers.21.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 6676279808,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 2260593472
    },
    {
      "name": "model.layers.21.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 6808990208,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 2393303872
    },
    {
      "name": "model.layers.21.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 6817284608,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 191971968
    },
    {
      "name": "model.layers.21.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 6817653248,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 2401598272
    },
    {
      "name": "model.layers.21.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 7083074048,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 2667019072
    },
    {
      "name": "model.layers.21.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 7099662848,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 192340608
    },
    {
      "name": "model.layers.21.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 7099662912,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 192340672
    },
    {
      "name": "model.layers.21.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 7099847232,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 192524992
    },
    {
      "name": "model.layers.21.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 7099852992,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 192530752
    },
    {
      "name": "model.layers.21.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 7099854016,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 192531776
    },
    {
      "name": "model.layers.21.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 7102803136,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 195480896
    },
    {
      "name": "model.layers.21.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 7102808896,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 195486656
    },
    {
      "name": "model.layers.21.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 7126401856,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 219079616
    },
    {
      "name": "model.layers.21.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 7126410048,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 219087808
    },
    {
      "name": "model.layers.21.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 7150003008,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 242680768
    },
    {
      "name": "model.layers.21.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 7150003136,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 242680896
    },
    {
      "name": "model.layers.21.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 7150004160,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 242681920
    },
    {
      "name": "model.layers.22.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 7152953280,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 245631040
    },
    {
      "name": "model.layers.22.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 7152959040,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 245636800
    },
    {
      "name": "model.layers.22.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 7153143360,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 2683607872
    },
    {
      "name": "model.layers.22.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 7285853760,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 2816318272
    },
    {
      "name": "model.layers.22.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 7294148160,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 245821120
    },
    {
      "name": "model.layers.22.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 7294516800,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 2824612672
    },
    {
      "name": "model.layers.22.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 7559937600,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 3090033472
    },
    {
      "name": "model.layers.22.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 7576526400,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 246189760
    },
    {
      "name": "model.layers.22.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 7576526464,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 246189824
    },
    {
      "name": "model.layers.22.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 7576710784,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 246374144
    },
    {
      "name": "model.layers.22.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 7576716544,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 246379904
    },
    {
      "name": "model.layers.22.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 7576717568,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 246380928
    },
    {
      "name": "model.layers.22.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 7579666688,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 249330048
    },
    {
      "name": "model.layers.22.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 7579672448,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 249335808
    },
    {
      "name": "model.layers.22.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 7603265408,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 272928768
    },
    {
      "name": "model.layers.22.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 7603273600,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 272936960
    },
    {
      "name": "model.layers.22.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 7626866560,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 296529920
    },
    {
      "name": "model.layers.22.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 7626866688,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 296530048
    },
    {
      "name": "model.layers.22.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 7626867712,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 296531072
    },
    {
      "name": "model.layers.23.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 7629816832,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 299480192
    },
    {
      "name": "model.layers.23.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 7629822592,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 299485952
    },
    {
      "name": "model.layers.23.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 7630006912,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 3106622272
    },
    {
      "name": "model.layers.23.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 7762717312,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 3239332672
    },
    {
      "name": "model.layers.23.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 7771011712,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 299670272
    },
    {
      "name": "model.layers.23.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 7771380352,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 3247627072
    },
    {
      "name": "model.layers.23.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 8036801152,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 3513047872
    },
    {
      "name": "model.layers.23.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 8053389952,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 300038912
    },
    {
      "name": "model.layers.23.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 8053390016,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 300038976
    },
    {
      "name": "model.layers.23.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 8053574336,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 300223296
    },
    {
      "name": "model.layers.23.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 8053580096,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 300229056
    },
    {
      "name": "model.layers.23.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 8053581120,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 300230080
    },
    {
      "name": "model.layers.23.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 8056530240,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 303179200
    },
    {
      "name": "model.layers.23.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 8056536000,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 303184960
    },
    {
      "name": "model.layers.23.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 8080128960,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 326777920
    },
    {
      "name": "model.layers.23.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 8080137152,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 326786112
    },
    {
      "name": "model.layers.23.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 8103730112,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 350379072
    },
    {
      "name": "model.layers.23.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 8103730240,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 350379200
    },
    {
      "name": "model.layers.23.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 8103731264,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 350380224
    },
    {
      "name": "model.layers.3.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 8106680384,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 353329344
    },
    {
      "name": "model.layers.3.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 8106686144,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 353335104
    },
    {
      "name": "model.layers.3.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 8106870464,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 3529636672
    },
    {
      "name": "model.layers.3.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 8239580864,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 3662347072
    },
    {
      "name": "model.layers.3.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 8247875264,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 353519424
    },
    {
      "name": "model.layers.3.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 8248243904,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 3670641472
    },
    {
      "name": "model.layers.3.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 8513664704,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 3936062272
    },
    {
      "name": "model.layers.3.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 8530253504,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 353888064
    },
    {
      "name": "model.layers.3.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 8530253568,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 353888128
    },
    {
      "name": "model.layers.3.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 8530437888,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 354072448
    },
    {
      "name": "model.layers.3.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 8530443648,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 354078208
    },
    {
      "name": "model.layers.3.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 8530444672,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 354079232
    },
    {
      "name": "model.layers.3.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 8533393792,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 357028352
    },
    {
      "name": "model.layers.3.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 8533399552,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 357034112
    },
    {
      "name": "model.layers.3.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 8556992512,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 380627072
    },
    {
      "name": "model.layers.3.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 8557000704,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 380635264
    },
    {
      "name": "model.layers.3.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 8580593664,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 404228224
    },
    {
      "name": "model.layers.3.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 8580593792,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 404228352
    },
    {
      "name": "model.layers.3.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 8580594816,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 404229376
    },
    {
      "name": "model.layers.4.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 8583543936,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 407178496
    },
    {
      "name": "model.layers.4.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 8583549696,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 407184256
    },
    {
      "name": "model.layers.4.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 8583734016,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 3952651072
    },
    {
      "name": "model.layers.4.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 8716444416,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 4085361472
    },
    {
      "name": "model.layers.4.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 8724738816,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 407368576
    },
    {
      "name": "model.layers.4.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 8725107456,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 4093655872
    },
    {
      "name": "model.layers.4.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 8990528256,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 4359076672
    },
    {
      "name": "model.layers.4.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 9007117056,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 407737216
    },
    {
      "name": "model.layers.4.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 9007117120,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 407737280
    },
    {
      "name": "model.layers.4.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 9007301440,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 407921600
    },
    {
      "name": "model.layers.4.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 9007307200,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 407927360
    },
    {
      "name": "model.layers.4.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 9007308224,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 407928384
    },
    {
      "name": "model.layers.4.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 9010257344,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 410877504
    },
    {
      "name": "model.layers.4.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 9010263104,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 410883264
    },
    {
      "name": "model.layers.4.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 9033856064,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 434476224
    },
    {
      "name": "model.layers.4.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 9033864256,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 434484416
    },
    {
      "name": "model.layers.4.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 9057457216,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 458077376
    },
    {
      "name": "model.layers.4.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 9057457344,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 458077504
    },
    {
      "name": "model.layers.4.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 9057458368,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 458078528
    },
    {
      "name": "model.layers.5.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 9060407488,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 461027648
    },
    {
      "name": "model.layers.5.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 9060413248,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 461033408
    },
    {
      "name": "model.layers.5.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 9060597568,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 4375665472
    },
    {
      "name": "model.layers.5.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 9193307968,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 4508375872
    },
    {
      "name": "model.layers.5.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 9201602368,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 461217728
    },
    {
      "name": "model.layers.5.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 9201971008,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 4516670272
    },
    {
      "name": "model.layers.5.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 9467391808,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 4782091072
    },
    {
      "name": "model.layers.5.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 9483980608,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 461586368
    },
    {
      "name": "model.layers.5.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 9483980672,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 461586432
    },
    {
      "name": "model.layers.5.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 9484164992,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 461770752
    },
    {
      "name": "model.layers.5.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 9484170752,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 461776512
    },
    {
      "name": "model.layers.5.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 9484171776,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 461777536
    },
    {
      "name": "model.layers.5.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 9487120896,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 464726656
    },
    {
      "name": "model.layers.5.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 9487126656,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 464732416
    },
    {
      "name": "model.layers.5.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 9510719616,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 488325376
    },
    {
      "name": "model.layers.5.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 9510727808,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 488333568
    },
    {
      "name": "model.layers.5.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 9534320768,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 511926528
    },
    {
      "name": "model.layers.5.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 9534320896,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 511926656
    },
    {
      "name": "model.layers.5.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 9534321920,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 511927680
    },
    {
      "name": "model.layers.6.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 9537271040,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 514876800
    },
    {
      "name": "model.layers.6.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 9537276800,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 514882560
    },
    {
      "name": "model.layers.6.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 9537645440,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 515251200
    },
    {
      "name": "model.layers.6.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 9537645504,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 515251264
    },
    {
      "name": "model.layers.6.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 9537829824,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 515435584
    },
    {
      "name": "model.layers.6.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 9537830848,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 515436608
    },
    {
      "name": "model.layers.6.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 9540779968,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 518385728
    },
    {
      "name": "model.layers.6.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 9540785728,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 518391488
    },
    {
      "name": "model.layers.6.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 9564378688,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 541984448
    },
    {
      "name": "model.layers.6.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 9564386880,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 541992640
    },
    {
      "name": "model.layers.6.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 9587979840,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 565585600
    },
    {
      "name": "model.layers.6.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 9587979968,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 565585728
    },
    {
      "name": "model.layers.6.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 9587980992,
      "shard": "model-00001-of-00002.safetensors",
      "shard_data_offset": 565586752
    },
    {
      "name": "lm_head.weight",
      "dtype": "BF16",
      "shape": [
        201088,
        2880
      ],
      "nbytes": 1158266880,
      "offset": 9590930112,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 0
    },
    {
      "name": "model.embed_tokens.weight",
      "dtype": "BF16",
      "shape": [
        201088,
        2880
      ],
      "nbytes": 1158266880,
      "offset": 10749196992,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 1158266880
    },
    {
      "name": "model.layers.6.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 11907463872,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2316533760
    },
    {
      "name": "model.layers.6.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 11907648192,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2478277056
    },
    {
      "name": "model.layers.6.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 12040358592,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2610987456
    },
    {
      "name": "model.layers.6.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 12048652992,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2619281856
    },
    {
      "name": "model.layers.6.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 12314073792,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2884702656
    },
    {
      "name": "model.layers.6.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 12330662592,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2316718080
    },
    {
      "name": "model.layers.7.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 12330668352,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2316723840
    },
    {
      "name": "model.layers.7.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 12330674112,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2316729600
    },
    {
      "name": "model.layers.7.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 12330858432,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2901291456
    },
    {
      "name": "model.layers.7.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 12463568832,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 3034001856
    },
    {
      "name": "model.layers.7.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 12471863232,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2316913920
    },
    {
      "name": "model.layers.7.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 12472231872,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 3042296256
    },
    {
      "name": "model.layers.7.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 12737652672,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 3307717056
    },
    {
      "name": "model.layers.7.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 12754241472,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2317282560
    },
    {
      "name": "model.layers.7.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 12754241536,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2317282624
    },
    {
      "name": "model.layers.7.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 12754425856,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2317466944
    },
    {
      "name": "model.layers.7.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 12754431616,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2317472704
    },
    {
      "name": "model.layers.7.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 12754432640,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2317473728
    },
    {
      "name": "model.layers.7.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 12757381760,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2320422848
    },
    {
      "name": "model.layers.7.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 12757387520,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2320428608
    },
    {
      "name": "model.layers.7.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 12780980480,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2344021568
    },
    {
      "name": "model.layers.7.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 12780988672,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2344029760
    },
    {
      "name": "model.layers.7.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 12804581632,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2367622720
    },
    {
      "name": "model.layers.7.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 12804581760,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2367622848
    },
    {
      "name": "model.layers.7.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 12804582784,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2367623872
    },
    {
      "name": "model.layers.8.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 12807531904,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2370572992
    },
    {
      "name": "model.layers.8.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 12807537664,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2370578752
    },
    {
      "name": "model.layers.8.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 12807721984,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 3324305856
    },
    {
      "name": "model.layers.8.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 12940432384,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 3457016256
    },
    {
      "name": "model.layers.8.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 12948726784,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2370763072
    },
    {
      "name": "model.layers.8.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 12949095424,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 3465310656
    },
    {
      "name": "model.layers.8.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 13214516224,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 3730731456
    },
    {
      "name": "model.layers.8.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 13231105024,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2371131712
    },
    {
      "name": "model.layers.8.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 13231105088,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2371131776
    },
    {
      "name": "model.layers.8.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 13231289408,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2371316096
    },
    {
      "name": "model.layers.8.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 13231295168,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2371321856
    },
    {
      "name": "model.layers.8.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 13231296192,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2371322880
    },
    {
      "name": "model.layers.8.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 13234245312,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2374272000
    },
    {
      "name": "model.layers.8.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 13234251072,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2374277760
    },
    {
      "name": "model.layers.8.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 13257844032,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2397870720
    },
    {
      "name": "model.layers.8.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 13257852224,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2397878912
    },
    {
      "name": "model.layers.8.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 13281445184,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2421471872
    },
    {
      "name": "model.layers.8.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 13281445312,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2421472000
    },
    {
      "name": "model.layers.8.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 13281446336,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2421473024
    },
    {
      "name": "model.layers.9.input_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 13284395456,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2424422144
    },
    {
      "name": "model.layers.9.mlp.experts.down_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 13284401216,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2424427904
    },
    {
      "name": "model.layers.9.mlp.experts.down_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90,
        16
      ],
      "nbytes": 132710400,
      "offset": 13284585536,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 3747320256
    },
    {
      "name": "model.layers.9.mlp.experts.down_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        2880,
        90
      ],
      "nbytes": 8294400,
      "offset": 13417295936,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 3880030656
    },
    {
      "name": "model.layers.9.mlp.experts.gate_up_proj_bias",
      "dtype": "BF16",
      "shape": [
        32,
        5760
      ],
      "nbytes": 368640,
      "offset": 13425590336,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2424612224
    },
    {
      "name": "model.layers.9.mlp.experts.gate_up_proj_blocks",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90,
        16
      ],
      "nbytes": 265420800,
      "offset": 13425958976,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 3888325056
    },
    {
      "name": "model.layers.9.mlp.experts.gate_up_proj_scales",
      "dtype": "U8",
      "shape": [
        32,
        5760,
        90
      ],
      "nbytes": 16588800,
      "offset": 13691379776,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 4153745856
    },
    {
      "name": "model.layers.9.mlp.router.bias",
      "dtype": "BF16",
      "shape": [
        32
      ],
      "nbytes": 64,
      "offset": 13707968576,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2424980864
    },
    {
      "name": "model.layers.9.mlp.router.weight",
      "dtype": "BF16",
      "shape": [
        32,
        2880
      ],
      "nbytes": 184320,
      "offset": 13707968640,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2424980928
    },
    {
      "name": "model.layers.9.post_attention_layernorm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 13708152960,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2425165248
    },
    {
      "name": "model.layers.9.self_attn.k_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 13708158720,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2425171008
    },
    {
      "name": "model.layers.9.self_attn.k_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 13708159744,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2425172032
    },
    {
      "name": "model.layers.9.self_attn.o_proj.bias",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 13711108864,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2428121152
    },
    {
      "name": "model.layers.9.self_attn.o_proj.weight",
      "dtype": "BF16",
      "shape": [
        2880,
        4096
      ],
      "nbytes": 23592960,
      "offset": 13711114624,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2428126912
    },
    {
      "name": "model.layers.9.self_attn.q_proj.bias",
      "dtype": "BF16",
      "shape": [
        4096
      ],
      "nbytes": 8192,
      "offset": 13734707584,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2451719872
    },
    {
      "name": "model.layers.9.self_attn.q_proj.weight",
      "dtype": "BF16",
      "shape": [
        4096,
        2880
      ],
      "nbytes": 23592960,
      "offset": 13734715776,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2451728064
    },
    {
      "name": "model.layers.9.self_attn.sinks",
      "dtype": "BF16",
      "shape": [
        64
      ],
      "nbytes": 128,
      "offset": 13758308736,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2475321024
    },
    {
      "name": "model.layers.9.self_attn.v_proj.bias",
      "dtype": "BF16",
      "shape": [
        512
      ],
      "nbytes": 1024,
      "offset": 13758308864,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2475321152
    },
    {
      "name": "model.layers.9.self_attn.v_proj.weight",
      "dtype": "BF16",
      "shape": [
        512,
        2880
      ],
      "nbytes": 2949120,
      "offset": 13758309888,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2475322176
    },
    {
      "name": "model.norm.weight",
      "dtype": "BF16",
      "shape": [
        2880
      ],
      "nbytes": 5760,
      "offset": 13761259008,
      "shard": "model-00002-of-00002.safetensors",
      "shard_data_offset": 2478271296
    }
  ]
}