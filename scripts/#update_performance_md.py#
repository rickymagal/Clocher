#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
update_performance_md.py (robusto)

Uso:
  python3 scripts/update_performance_md.py --strict-json benchmarks/reports/fp32_latest.json
  (também aceita summary_latest.json; se houver runs válidos, atualiza assim mesmo)

O script:
- Varre o arquivo passado e tenta dar json.loads linha a linha.
- Considera um "run" qualquer objeto com pelo menos:
    tokens_generated (int), wall_time_s (float)
  Campos opcionais (se existirem) melhoram as métricas:
    tps_true, latency_p50_ms, latency_p95_ms, rss_peak_mb, kv_hits, kv_misses
- Se achar um objeto-resumo {"runs": N, ...}, usa como fallback para totals.
- Lê variáveis de ambiente para parâmetros de execução se o log não as trouxer.
- Atualiza seções específicas dentro de docs/PERFORMANCE.md.
"""

import argparse
import json
import os
import re
import sys
from pathlib import Path
from statistics import mean

MD_PATH = Path("docs/PERFORMANCE.md")

def human_bytes(n):
    if n is None:
        return "n/a"
    units = ["B","KB","MB","GB","TB"]
    x = float(n)
    i = 0
    while x >= 1024.0 and i < len(units)-1:
        x /= 1024.0
        i += 1
    if i == 0:
        return f"{int(x)} {units[i]}"
    return f"{x:.1f} {units[i]}"

def parse_json_lines(p: Path):
    objs = []
    with p.open("r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            line = line.strip()
            if not line or not line.startswith("{"):
                continue
            try:
                obj = json.loads(line)
                objs.append(obj)
            except Exception:
                # algumas linhas podem ser prints/ruído; ignorar
                continue
    return objs

def collect_runs(objs):
    runs = []
    summary_obj = None
    for o in objs:
        if isinstance(o, dict) and "tokens_generated" in o and "wall_time_s" in o:
            runs.append(o)
        if isinstance(o, dict) and "runs" in o and summary_obj is None:
            summary_obj = o
    return runs, summary_obj

def read_env_or(defaults: dict):
    # pega tudo relevante do ambiente; se não tiver, cai no default
    get = os.environ.get
    env = {}
    for k,v in defaults.items():
        env[k] = get(k, v)
    # normalizações numéricas
    for k in ["THREADS","BATCH","MAX_NEW","IE_REQUIRE_MODEL","IE_BYTES_PER_TOKEN","IE_STRIDE_BYTES","IE_VERIFY_TOUCH"]:
        if env.get(k) in (None, "", "****"):
            continue
        try:
            env[k] = int(str(env[k]))
        except Exception:
            pass
    return env

def parse_size_from_md(md_text):
    # tenta achar linha do model.ie.bin: "(77.3 GB, mtime ...)"
    m = re.search(r"model\.ie\.bin:.*\(([\d\.]+)\s*(KB|MB|GB|TB)\b", md_text)
    if not m:
        return None
    val = float(m.group(1))
    unit = m.group(2)
    mult = {"KB":1024, "MB":1024**2, "GB":1024**3, "TB":1024**4}[unit]
    return int(val * mult)

def load_md():
    if MD_PATH.exists():
        return MD_PATH.read_text(encoding="utf-8")
    # se não existir, criar um esqueleto simples
    return (
        "# Performance Notes\n\n"
        "_Last updated: **(pending)**_\n\n"
        "## Summary (latest benchmark)\n- No benchmark report found under `benchmarks/reports/`\n\n"
        "## Latency\n- Latency p50 (mean across runs): **n/a**\n- Latency p95 (mean across runs): **n/a**\n\n"
        "## Spatial Complexity (Memory & Cache)\n"
        "- RSS peak (mean): **n/a**\n- RSS peak (max): **n/a**\n"
        "- KV cache: **0 hits / 0 misses** (no measurements)\n"
        "- IE_BYTES_PER_TOKEN: **0.0 B**/token\n"
        "- Bytes touched (Σ): **0.0 B**\n"
        "- Working-set coverage (bytes_per_token / model.bin): **0.00%**\n"
        "- Effective bandwidth: **n/a**\n\n"
        "## Run Parameters & Conditions\n"
        "- Engine bin: ``\n- Prompts file: ``\n"
        "- Threads: ****\n- Precision: ****\n- Batch: ****\n- Prefetch: ****\n- Pretranspose: ****\n- Affinity: ****\n- Max new tokens: ****\n"
        "- IE_REQUIRE_MODEL: ****\n- IE_BYTES_PER_TOKEN: ****\n- IE_STRIDE_BYTES: ****\n- IE_VERIFY_TOUCH: ****\n\n"
        "## System & Model Info\n"
        "(fill once elsewhere)\n"
    )

def write_md(md_text):
    MD_PATH.parent.mkdir(parents=True, exist_ok=True)
    MD_PATH.write_text(md_text, encoding="utf-8")

def patch_section(md, header, lines):
    """
    Substitui a seção que começa com '## {header}' até a próxima '## ' ou EOF.
    """
    pattern = re.compile(rf"(## {re.escape(header)}\n)(.*?)(?=\n## |\Z)", re.S)
    block = f"## {header}\n" + "\n".join(lines) + "\n"
    if pattern.search(md):
        return pattern.sub(block, md)
    # não achou; apenda no fim
    return md.rstrip() + "\n\n" + block

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--strict-json", required=True, help="Caminho do relatório (bruto ou summary).")
    args = ap.parse_args()

    rpt_path = Path(args.strict_json)
    if not rpt_path.exists():
        print(f"[error] not found: {rpt_path}", file=sys.stderr)
        sys.exit(1)

    objs = parse_json_lines(rpt_path)
    runs, summary = collect_runs(objs)

    # fallback: se não houver runs, mas houver summary, tenta usar
    if not runs and summary and isinstance(summary, dict):
        # sintetiza um run “agregado” suficiente para preencher TPS etc.
        tokens_total = int(summary.get("tokens_generated", 0))
        wall_total = float(summary.get("wall_time_s", 0.0))
        tps_true = float(summary.get("tps_true", tokens_total / wall_total if wall_total > 0 else 0.0))
        runs_count = int(summary.get("runs", 0))
        if runs_count > 0:
            # cria N runs artificiais iguais (ok para médias)
            for _ in range(runs_count):
                runs.append({
                    "tokens_generated": tokens_total / runs_count if runs_count else 0,
                    "wall_time_s": wall_total / runs_count if runs_count else 0.0,
                    "tps_true": tps_true,
                    "latency_p50_ms": float(summary.get("latency_p50_ms", 0.0)),
                    "latency_p95_ms": float(summary.get("latency_p95_ms", 0.0)),
                    "rss_peak_mb": int(summary.get("rss_peak_mb", 0)),
                    "kv_hits": int(summary.get("kv_hits", 0)),
                    "kv_misses": int(summary.get("kv_misses", 0)),
                })

    # Se ainda não há runs, não tem o que resumir.
    if not runs:
        print("[warn] strict JSON has zero tokens; leaving PERFORMANCE.md unchanged")
        sys.exit(0)

    # ---- estatísticas
    toks = [int(max(0, r.get("tokens_generated", 0))) for r in runs]
    walls = [float(max(0.0, r.get("wall_time_s", 0.0))) for r in runs]
    tps_list = []
    for r in runs:
        if r.get("tps_true", 0) and r.get("tps_true", 0) > 0:
            tps_list.append(float(r["tps_true"]))

    tokens_total = sum(toks)
    wall_total = sum(walls)
    tps_overall = (tokens_total / wall_total) if wall_total > 0 else 0.0

    lat_p50 = [float(r.get("latency_p50_ms", 0.0)) for r in runs if float(r.get("latency_p50_ms", 0.0)) > 0.0]
    lat_p95 = [float(r.get("latency_p95_ms", 0.0)) for r in runs if float(r.get("latency_p95_ms", 0.0)) > 0.0]
    rss_list = [int(r.get("rss_peak_mb", 0)) for r in runs if int(r.get("rss_peak_mb", 0)) > 0]
    kv_hits = sum(int(r.get("kv_hits", 0)) for r in runs)
    kv_miss = sum(int(r.get("kv_misses", 0)) for r in runs)

    p50_mean = mean(lat_p50) if lat_p50 else None
    p95_mean = mean(lat_p95) if lat_p95 else None
    rss_mean = mean(rss_list) if rss_list else None
    rss_max  = max(rss_list) if rss_list else None

    # ---- env/params (pega do ambiente agora; se não tiver, deixa texto claro)
    env = read_env_or({
        "ENGINE_BIN": "",
        "PROMPTS": "",
        "THREADS": "****",
        "PRECISION": "****",
        "BATCH": "****",
        "PREFETCH": "****",
        "PRETRANSPOSE": "****",
        "AFFINITY": "****",
        "MAX_NEW": "****",
        "IE_REQUIRE_MODEL": "****",
        "IE_BYTES_PER_TOKEN": "****",
        "IE_STRIDE_BYTES": "****",
        "IE_VERIFY_TOUCH": "****",
    })

    # ---- bytes tocados e bandwidth
    bpt = env.get("IE_BYTES_PER_TOKEN")
    if isinstance(bpt, str):
        try:
            bpt = int(bpt)
        except Exception:
            bpt = 0
    bytes_touched = (tokens_total * bpt) if (bpt and isinstance(bpt, int)) else 0
    bw_bytes_per_s = (bytes_touched / wall_total) if wall_total > 0 else 0.0

    # ---- coverage (usa tamanho do model.bin já presente no MD)
    md_old = load_md()
    model_bin_bytes = parse_size_from_md(md_old)
    coverage = ( (bpt / model_bin_bytes) * 100.0 ) if (bpt and model_bin_bytes and model_bin_bytes > 0) else None

    # ---- montar seções
    from datetime import datetime, timezone
    now_utc = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S %Z")

    # Atualiza a linha “Last updated”
    md = re.sub(
        r"_Last updated: \*\*.*?\*\*_",
        f"_Last updated: **{now_utc}**_",
        md_old if md_old else "",
        flags=re.S
    )

    # Summary
    summary_lines = [
        f"- Runs: **{len(runs)}**",
        f"- Tokens gerados (Σ): **{tokens_total}**",
        f"- Tempo de parede (Σ): **{wall_total:.3f} s**",
        f"- TPS verdadeiro (Σ tokens / Σ tempo): **{tps_overall:.3f}**",
    ]
    md = patch_section(md, "Summary (latest benchmark)", summary_lines)

    # Latency
    lat_lines = [
        f"- Latency p50 (mean across runs): **{(f'{p50_mean:.3f} ms' if p50_mean is not None else 'n/a')}**",
        f"- Latency p95 (mean across runs): **{(f'{p95_mean:.3f} ms' if p95_mean is not None else 'n/a')}**",
    ]
    md = patch_section(md, "Latency", lat_lines)

    # Spatial Complexity
    spat_lines = [
        f"- RSS peak (mean): **{(f'{int(rss_mean)} MB' if rss_mean is not None else 'n/a')}**",
        f"- RSS peak (max): **{(f'{int(rss_max)} MB' if rss_max  is not None else 'n/a')}**",
        f"- KV cache: **{kv_hits} hits / {kv_miss} misses**",
        f"- IE_BYTES_PER_TOKEN: **{human_bytes(bpt) if bpt else '0.0 B'}**/token",
        f"- Bytes touched (Σ): **{human_bytes(bytes_touched)}**",
        f"- Working-set coverage (bytes_per_token / model.bin): **{(f'{coverage:.2f}%' if coverage is not None else 'n/a')}**",
        f"- Effective bandwidth: **{(f'{bw_bytes_per_s/1e9:.2f} GB/s' if bw_bytes_per_s>0 else 'n/a')}**",
    ]
    md = patch_section(md, "Spatial Complexity (Memory & Cache)", spat_lines)

    # Run Parameters & Conditions
    rpc_lines = [
        f"- Engine bin: `{env.get('ENGINE_BIN') or ''}`",
        f"- Prompts file: `{env.get('PROMPTS') or ''}`",
        f"- Threads: **{env.get('THREADS')}**",
        f"- Precision: **{env.get('PRECISION')}**",
        f"- Batch: **{env.get('BATCH')}**",
        f"- Prefetch: **{env.get('PREFETCH')}**",
        f"- Pretranspose: **{env.get('PRETRANSPOSE')}**",
        f"- Affinity: **{env.get('AFFINITY')}**",
        f"- Max new tokens: **{env.get('MAX_NEW')}**",
        f"- IE_REQUIRE_MODEL: **{env.get('IE_REQUIRE_MODEL')}**",
        f"- IE_BYTES_PER_TOKEN: **{env.get('IE_BYTES_PER_TOKEN')}**",
        f"- IE_STRIDE_BYTES: **{env.get('IE_STRIDE_BYTES')}**",
        f"- IE_VERIFY_TOUCH: **{env.get('IE_VERIFY_TOUCH')}**",
    ]
    md = patch_section(md, "Run Parameters & Conditions", rpc_lines)

    write_md(md)

    # também escrever/atualizar um summary_latest.json compatível
    summary_out = {
        "runs": len(runs),
        "tokens_generated": tokens_total,
        "wall_time_s": wall_total,
        "tps_true": tps_overall,
        "latency_p50_ms": p50_mean or 0.0,
        "latency_p95_ms": p95_mean or 0.0,
        "rss_peak_mb_mean": rss_mean or 0,
        "rss_peak_mb_max": rss_max or 0,
        "kv_hits": kv_hits,
        "kv_misses": kv_miss,
        "ie_bytes_per_token": bpt or 0,
        "bytes_touched_total": bytes_touched,
        "effective_bandwidth_Bps": bw_bytes_per_s,
    }
    out_path = Path("benchmarks/reports/summary_latest.json")
    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text(json.dumps(summary_out, indent=2), encoding="utf-8")

    print(f"[ok] wrote: {MD_PATH}")
    print(f"[ok] summary: {out_path}")

if __name__ == "__main__":
    main()
