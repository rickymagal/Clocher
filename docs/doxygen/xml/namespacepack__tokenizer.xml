<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.9.8" xml:lang="en-US">
  <compounddef id="namespacepack__tokenizer" kind="namespace" language="Python">
    <compoundname>pack_tokenizer</compoundname>
    <sectiondef kind="var">
      <memberdef kind="variable" id="namespacepack__tokenizer_1a7d8523b2b1ec62dbfbef376cf89d275f" prot="public" static="no" mutable="no">
        <type>str</type>
        <definition>str pack_tokenizer.MAGIC</definition>
        <argsstring></argsstring>
        <name>MAGIC</name>
        <qualifiedname>pack_tokenizer.MAGIC</qualifiedname>
        <initializer>=  b&quot;IETOK1&quot;</initializer>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="scripts/pack_tokenizer.py" line="42" column="1" bodyfile="scripts/pack_tokenizer.py" bodystart="42" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="namespacepack__tokenizer_1a2db161c3fc86c9ecc840aedea996d406" prot="public" static="no" mutable="no">
        <type>int</type>
        <definition>int pack_tokenizer.VERSION_U16</definition>
        <argsstring></argsstring>
        <name>VERSION_U16</name>
        <qualifiedname>pack_tokenizer.VERSION_U16</qualifiedname>
        <initializer>=  1</initializer>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="scripts/pack_tokenizer.py" line="43" column="1" bodyfile="scripts/pack_tokenizer.py" bodystart="43" bodyend="-1"/>
      </memberdef>
    </sectiondef>
    <sectiondef kind="func">
      <memberdef kind="function" id="namespacepack__tokenizer_1ace8e93fd9b4669a890b11b6654466881" prot="protected" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>None</type>
        <definition> None pack_tokenizer._die</definition>
        <argsstring>(str msg)</argsstring>
        <name>_die</name>
        <qualifiedname>pack_tokenizer._die</qualifiedname>
        <param>
          <type>str</type>
          <declname>msg</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="scripts/pack_tokenizer.py" line="46" column="1" bodyfile="scripts/pack_tokenizer.py" bodystart="46" bodyend="50"/>
        <referencedby refid="namespacepack__tokenizer_1a2a1790ad10324a1a6448b2e94c3a076b" compoundref="pack__tokenizer_8py" startline="96" endline="138">_build_id_to_tok</referencedby>
        <referencedby refid="namespacepack__tokenizer_1ace94cf504c2c83b667b3f7c226627eed" compoundref="pack__tokenizer_8py" startline="56" endline="66">_extract_vocab_and_merges</referencedby>
        <referencedby refid="namespacepack__tokenizer_1a5ef29190dd0da7e4634005186b75ba01" compoundref="pack__tokenizer_8py" startline="139" endline="214">main</referencedby>
      </memberdef>
      <memberdef kind="function" id="namespacepack__tokenizer_1ad6fd649fcc73c13cb1df2aefb5b67919" prot="protected" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>dict</type>
        <definition> dict pack_tokenizer._load_json</definition>
        <argsstring>(str path)</argsstring>
        <name>_load_json</name>
        <qualifiedname>pack_tokenizer._load_json</qualifiedname>
        <param>
          <type>str</type>
          <declname>path</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="scripts/pack_tokenizer.py" line="51" column="1" bodyfile="scripts/pack_tokenizer.py" bodystart="51" bodyend="55"/>
        <referencedby refid="namespacepack__tokenizer_1a5ef29190dd0da7e4634005186b75ba01" compoundref="pack__tokenizer_8py" startline="139" endline="214">main</referencedby>
      </memberdef>
      <memberdef kind="function" id="namespacepack__tokenizer_1ace94cf504c2c83b667b3f7c226627eed" prot="protected" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>Tuple[Dict[str, int], List]</type>
        <definition> Tuple[Dict[str, int], List] pack_tokenizer._extract_vocab_and_merges</definition>
        <argsstring>(dict j)</argsstring>
        <name>_extract_vocab_and_merges</name>
        <qualifiedname>pack_tokenizer._extract_vocab_and_merges</qualifiedname>
        <param>
          <type>dict</type>
          <declname>j</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="scripts/pack_tokenizer.py" line="56" column="1" bodyfile="scripts/pack_tokenizer.py" bodystart="56" bodyend="66"/>
        <references refid="namespacepack__tokenizer_1ace8e93fd9b4669a890b11b6654466881" compoundref="pack__tokenizer_8py" startline="46" endline="50">_die</references>
        <referencedby refid="namespacepack__tokenizer_1a5ef29190dd0da7e4634005186b75ba01" compoundref="pack__tokenizer_8py" startline="139" endline="214">main</referencedby>
      </memberdef>
      <memberdef kind="function" id="namespacepack__tokenizer_1aaacc2a04450b71a20bb79e8c03029125" prot="protected" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>List[Tuple[int, str]]</type>
        <definition> List[Tuple[int, str]] pack_tokenizer._extract_added_tokens</definition>
        <argsstring>(dict j)</argsstring>
        <name>_extract_added_tokens</name>
        <qualifiedname>pack_tokenizer._extract_added_tokens</qualifiedname>
        <param>
          <type>dict</type>
          <declname>j</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="scripts/pack_tokenizer.py" line="67" column="1" bodyfile="scripts/pack_tokenizer.py" bodystart="67" bodyend="81"/>
        <referencedby refid="namespacepack__tokenizer_1a5ef29190dd0da7e4634005186b75ba01" compoundref="pack__tokenizer_8py" startline="139" endline="214">main</referencedby>
      </memberdef>
      <memberdef kind="function" id="namespacepack__tokenizer_1a1929cab4e5f1f3377e0aa46ade9af4ba" prot="protected" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>List[Tuple[str, str]]</type>
        <definition> List[Tuple[str, str]] pack_tokenizer._normalize_merges</definition>
        <argsstring>(List merges)</argsstring>
        <name>_normalize_merges</name>
        <qualifiedname>pack_tokenizer._normalize_merges</qualifiedname>
        <param>
          <type>List</type>
          <declname>merges</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="scripts/pack_tokenizer.py" line="82" column="1" bodyfile="scripts/pack_tokenizer.py" bodystart="82" bodyend="95"/>
        <referencedby refid="namespacepack__tokenizer_1a5ef29190dd0da7e4634005186b75ba01" compoundref="pack__tokenizer_8py" startline="139" endline="214">main</referencedby>
      </memberdef>
      <memberdef kind="function" id="namespacepack__tokenizer_1a2a1790ad10324a1a6448b2e94c3a076b" prot="protected" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>Tuple[List[str], int, int]</type>
        <definition> Tuple[List[str], int, int] pack_tokenizer._build_id_to_tok</definition>
        <argsstring>(Dict[str, int] vocab, List[Tuple[int, str]] added)</argsstring>
        <name>_build_id_to_tok</name>
        <qualifiedname>pack_tokenizer._build_id_to_tok</qualifiedname>
        <param>
          <type>Dict</type>
          <declname>vocab</declname>
          <array>[str, int]</array>
        </param>
        <param>
          <type>List]</type>
          <declname>added</declname>
          <array>[Tuple[int, str]</array>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="scripts/pack_tokenizer.py" line="96" column="1" bodyfile="scripts/pack_tokenizer.py" bodystart="96" bodyend="138"/>
        <references refid="namespacepack__tokenizer_1ace8e93fd9b4669a890b11b6654466881" compoundref="pack__tokenizer_8py" startline="46" endline="50">_die</references>
        <referencedby refid="namespacepack__tokenizer_1a5ef29190dd0da7e4634005186b75ba01" compoundref="pack__tokenizer_8py" startline="139" endline="214">main</referencedby>
      </memberdef>
      <memberdef kind="function" id="namespacepack__tokenizer_1a5ef29190dd0da7e4634005186b75ba01" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>None</type>
        <definition> None pack_tokenizer.main</definition>
        <argsstring>()</argsstring>
        <name>main</name>
        <qualifiedname>pack_tokenizer.main</qualifiedname>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="scripts/pack_tokenizer.py" line="139" column="1" bodyfile="scripts/pack_tokenizer.py" bodystart="139" bodyend="214"/>
        <references refid="namespacepack__tokenizer_1a2a1790ad10324a1a6448b2e94c3a076b" compoundref="pack__tokenizer_8py" startline="96" endline="138">_build_id_to_tok</references>
        <references refid="namespacepack__tokenizer_1ace8e93fd9b4669a890b11b6654466881" compoundref="pack__tokenizer_8py" startline="46" endline="50">_die</references>
        <references refid="namespacepack__tokenizer_1aaacc2a04450b71a20bb79e8c03029125" compoundref="pack__tokenizer_8py" startline="67" endline="81">_extract_added_tokens</references>
        <references refid="namespacepack__tokenizer_1ace94cf504c2c83b667b3f7c226627eed" compoundref="pack__tokenizer_8py" startline="56" endline="66">_extract_vocab_and_merges</references>
        <references refid="namespacepack__tokenizer_1ad6fd649fcc73c13cb1df2aefb5b67919" compoundref="pack__tokenizer_8py" startline="51" endline="55">_load_json</references>
        <references refid="namespacepack__tokenizer_1a1929cab4e5f1f3377e0aa46ade9af4ba" compoundref="pack__tokenizer_8py" startline="82" endline="95">_normalize_merges</references>
        <references refid="namespacepack__tokenizer_1a5ef29190dd0da7e4634005186b75ba01" compoundref="pack__tokenizer_8py" startline="139" endline="214">main</references>
        <referencedby refid="namespacepack__tokenizer_1a5ef29190dd0da7e4634005186b75ba01" compoundref="pack__tokenizer_8py" startline="139" endline="214">main</referencedby>
      </memberdef>
    </sectiondef>
    <briefdescription>
    </briefdescription>
    <detaileddescription>
<para><verbatim>Packs a HuggingFace-style tokenizer.json into the engine&apos;s packed tokenizer
format (IETOK1) expected by engine/src/io/tokenizer_gptoss.c.

This packer writes:
  magic:      6 bytes  &quot;IETOK1&quot;
  version:    u16le    1
  vocab_size: u32le
  merges_cnt: u32le
  off_vocab:  u32le
  off_merges: u32le
  off_special:u32le
  reserved:   u32le

Then at off_vocab:
  repeated vocab_size times:
    u32le len
    len bytes (utf-8 token text)

Then at off_merges:
  repeated merges_cnt times:
    u32le len_a
    len_a bytes
    u32le len_b
    len_b bytes

No special-tokens section is currently written; off_special == end of file.
</verbatim> </para>
    </detaileddescription>
    <location file="scripts/pack_tokenizer.py" line="1" column="1"/>
  </compounddef>
</doxygen>
