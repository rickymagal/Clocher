<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Inference Engine (Clocher): engine/src/kernels/ie_kernels_cuda.cu File Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Inference Engine (Clocher)<span id="projectnumber">&#160;0.2</span>
   </div>
   <div id="projectbrief">C11 CPU/GPU inference baseline with strict metrics &amp; harness</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search',true);
  $(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('ie__kernels__cuda_8cu.html',''); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#define-members">Macros</a> &#124;
<a href="#typedef-members">Typedefs</a> &#124;
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle"><div class="title">ie_kernels_cuda.cu File Reference</div></div>
</div><!--header-->
<div class="contents">

<p>CUDA implementations of GEMV/activation/packing kernels + C-ABI launchers.  
<a href="#details">More...</a></p>
<div class="textblock"><code>#include &lt;cuda_runtime.h&gt;</code><br />
<code>#include &lt;math_constants.h&gt;</code><br />
<code>#include &lt;stddef.h&gt;</code><br />
<code>#include &lt;stdint.h&gt;</code><br />
<code>#include &quot;<a class="el" href="ie__kernels__cuda_8h_source.html">ie_kernels_cuda.h</a>&quot;</code><br />
<code>#include &quot;<a class="el" href="ie__quant__act_8h_source.html">ie_quant_act.h</a>&quot;</code><br />
</div><div class="textblock"><div class="dynheader">
Include dependency graph for ie_kernels_cuda.cu:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu__incl.svg" width="667" height="207"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>
</div>
<p><a href="ie__kernels__cuda_8cu_source.html">Go to the source code of this file.</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="define-members" name="define-members"></a>
Macros</h2></td></tr>
<tr class="memitem:afb3284f12e7d5038612ece3185920449" id="r_afb3284f12e7d5038612ece3185920449"><td class="memItemLeft" align="right" valign="top">#define&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#afb3284f12e7d5038612ece3185920449">IE_CUDA_OK</a>&#160;&#160;&#160;0</td></tr>
<tr class="separator:afb3284f12e7d5038612ece3185920449"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad253e9c439228a67be0e55b0f4c81edf" id="r_ad253e9c439228a67be0e55b0f4c81edf"><td class="memItemLeft" align="right" valign="top">#define&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ad253e9c439228a67be0e55b0f4c81edf">IE_CUDA_STREAM_T_DEFINED</a></td></tr>
<tr class="separator:ad253e9c439228a67be0e55b0f4c81edf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af4c083462b1e956e6461aa246d2af9a6" id="r_af4c083462b1e956e6461aa246d2af9a6"><td class="memItemLeft" align="right" valign="top">#define&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#af4c083462b1e956e6461aa246d2af9a6">CUDA_GUARD</a>(call)</td></tr>
<tr class="memdesc:af4c083462b1e956e6461aa246d2af9a6"><td class="mdescLeft">&#160;</td><td class="mdescRight">CUDA error guard macro that stores the error string and returns.  <br /></td></tr>
<tr class="separator:af4c083462b1e956e6461aa246d2af9a6"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="typedef-members" name="typedef-members"></a>
Typedefs</h2></td></tr>
<tr class="memitem:aa036ed43d83d669bfc8c15c81a7efe73" id="r_aa036ed43d83d669bfc8c15c81a7efe73"><td class="memItemLeft" align="right" valign="top">typedef cudaStream_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#aa036ed43d83d669bfc8c15c81a7efe73">ie_cuda_stream_t</a></td></tr>
<tr class="separator:aa036ed43d83d669bfc8c15c81a7efe73"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a841d35ecb02f9850f311a5724d21deac" id="r_a841d35ecb02f9850f311a5724d21deac"><td class="memItemLeft" align="right" valign="top">static void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a841d35ecb02f9850f311a5724d21deac">ie_cuda_set_err</a> (const char *msg)</td></tr>
<tr class="memdesc:a841d35ecb02f9850f311a5724d21deac"><td class="mdescLeft">&#160;</td><td class="mdescRight">Set or clear the per-thread CUDA error string buffer.  <br /></td></tr>
<tr class="separator:a841d35ecb02f9850f311a5724d21deac"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga6656c0ddd9df382048e7cd162e0c1d64" id="r_ga6656c0ddd9df382048e7cd162e0c1d64"><td class="memItemLeft" align="right" valign="top">const char *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__IE__GPU.html#ga6656c0ddd9df382048e7cd162e0c1d64">ie_cuda_last_error_string</a> (void)</td></tr>
<tr class="memdesc:ga6656c0ddd9df382048e7cd162e0c1d64"><td class="mdescLeft">&#160;</td><td class="mdescRight">Retrieve the last CUDA error string set by a launcher in this TU.  <br /></td></tr>
<tr class="separator:ga6656c0ddd9df382048e7cd162e0c1d64"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a603115e5bb835d21609518d0af053e8d" id="r_a603115e5bb835d21609518d0af053e8d"><td class="memItemLeft" align="right" valign="top">__device__ float&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a603115e5bb835d21609518d0af053e8d">ie_apply_activation</a> (float x, <a class="el" href="group__IE__GPU.html#ga08ba1d3777f64c23e99124d2e460e701">ie_act_kind_t</a> act)</td></tr>
<tr class="memdesc:a603115e5bb835d21609518d0af053e8d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Apply a simple activation kind to a scalar.  <br /></td></tr>
<tr class="separator:a603115e5bb835d21609518d0af053e8d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8d15fdbcd086d7ef5932207fa5b5d91b" id="r_a8d15fdbcd086d7ef5932207fa5b5d91b"><td class="memItemLeft" align="right" valign="top">__device__ float&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a8d15fdbcd086d7ef5932207fa5b5d91b">ie_decode_fp8_e4m3_u8</a> (uint8_t v)</td></tr>
<tr class="memdesc:a8d15fdbcd086d7ef5932207fa5b5d91b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Decode one E4M3 FP8 byte on device (subnormals flushed to zero).  <br /></td></tr>
<tr class="separator:a8d15fdbcd086d7ef5932207fa5b5d91b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a98464ed99207f2ed392f751bae2ea10e" id="r_a98464ed99207f2ed392f751bae2ea10e"><td class="memItemLeft" align="right" valign="top">__device__ float&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a98464ed99207f2ed392f751bae2ea10e">ie_decode_fp8_e5m2_u8</a> (uint8_t v)</td></tr>
<tr class="memdesc:a98464ed99207f2ed392f751bae2ea10e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Decode one E5M2 FP8 byte on device (IEEE-like special cases).  <br /></td></tr>
<tr class="separator:a98464ed99207f2ed392f751bae2ea10e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af00402674d56c833c92c62df1cef1e84" id="r_af00402674d56c833c92c62df1cef1e84"><td class="memItemLeft" align="right" valign="top">__global__ void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#af00402674d56c833c92c62df1cef1e84">k_gemv_rowwise_f32</a> (const float *__restrict__ W, const float *__restrict__ x, float *__restrict__ y, int rows, int cols, int ldW, float alpha, float beta)</td></tr>
<tr class="memdesc:af00402674d56c833c92c62df1cef1e84"><td class="mdescLeft">&#160;</td><td class="mdescRight">Row-wise GEMV kernel: y = alpha * W * x + beta * y.  <br /></td></tr>
<tr class="separator:af00402674d56c833c92c62df1cef1e84"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab9150fe6fc7da507974bfdd38b262ce2" id="r_ab9150fe6fc7da507974bfdd38b262ce2"><td class="memItemLeft" align="right" valign="top">__global__ void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ab9150fe6fc7da507974bfdd38b262ce2">k_gemv_bias_act_f32</a> (const float *__restrict__ W, const float *__restrict__ x, const float *__restrict__ bias, float *__restrict__ y, int rows, int cols, int ldW, float alpha, float beta, <a class="el" href="group__IE__GPU.html#ga08ba1d3777f64c23e99124d2e460e701">ie_act_kind_t</a> act)</td></tr>
<tr class="memdesc:ab9150fe6fc7da507974bfdd38b262ce2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Fused GEMV + bias + activation kernel (float activations).  <br /></td></tr>
<tr class="separator:ab9150fe6fc7da507974bfdd38b262ce2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abb0d57e76ef0b1dea1ae1b8bb55e1c02" id="r_abb0d57e76ef0b1dea1ae1b8bb55e1c02"><td class="memItemLeft" align="right" valign="top">__global__ void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#abb0d57e76ef0b1dea1ae1b8bb55e1c02">k_gemv_rowwise_qi8_f32</a> (const float *__restrict__ W, const int8_t *__restrict__ xq, float *__restrict__ y, int rows, int cols, int ldW, float scale, int zp, float alpha, float beta)</td></tr>
<tr class="memdesc:abb0d57e76ef0b1dea1ae1b8bb55e1c02"><td class="mdescLeft">&#160;</td><td class="mdescRight">Row-wise GEMV with INT8 activations (per-tensor), fused dequantization.  <br /></td></tr>
<tr class="separator:abb0d57e76ef0b1dea1ae1b8bb55e1c02"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a74040aacee610aa771ee11bb0bc74022" id="r_a74040aacee610aa771ee11bb0bc74022"><td class="memItemLeft" align="right" valign="top">__global__ void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a74040aacee610aa771ee11bb0bc74022">k_gemv_rowwise_qfp8_f32</a> (const float *__restrict__ W, const uint8_t *__restrict__ x8, float *__restrict__ y, int rows, int cols, int ldW, <a class="el" href="ie__quant__act_8h.html#a67a1bdaa8281d1c69b6b76ecba79c52c">ie_fp8_format</a> fmt, float alpha, float beta)</td></tr>
<tr class="memdesc:a74040aacee610aa771ee11bb0bc74022"><td class="mdescLeft">&#160;</td><td class="mdescRight">Row-wise GEMV with FP8 activations (E4M3/E5M2), fused byte decode.  <br /></td></tr>
<tr class="separator:a74040aacee610aa771ee11bb0bc74022"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aea063f21d4bdf3ba4108f9677476aad4" id="r_aea063f21d4bdf3ba4108f9677476aad4"><td class="memItemLeft" align="right" valign="top">__global__ void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#aea063f21d4bdf3ba4108f9677476aad4">k_vec_tanh_f32</a> (float *__restrict__ y, const float *__restrict__ x, int n)</td></tr>
<tr class="memdesc:aea063f21d4bdf3ba4108f9677476aad4"><td class="mdescLeft">&#160;</td><td class="mdescRight">Elementwise hyperbolic tangent on a vector.  <br /></td></tr>
<tr class="separator:aea063f21d4bdf3ba4108f9677476aad4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac440d51163bc9f16e835d00aedcb4b79" id="r_ac440d51163bc9f16e835d00aedcb4b79"><td class="memItemLeft" align="right" valign="top">__global__ void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ac440d51163bc9f16e835d00aedcb4b79">k_pack_w_blockedk_f32</a> (float *__restrict__ Wp, const float *__restrict__ W, int rows, int cols, int ldW, int block_k)</td></tr>
<tr class="memdesc:ac440d51163bc9f16e835d00aedcb4b79"><td class="mdescLeft">&#160;</td><td class="mdescRight">Pack row-major W into Blocked-K layout on device (see header docs).  <br /></td></tr>
<tr class="separator:ac440d51163bc9f16e835d00aedcb4b79"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga92e383308a681db6bb757c3add5906f0" id="r_ga92e383308a681db6bb757c3add5906f0"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__IE__GPU.html#ga92e383308a681db6bb757c3add5906f0">ie_cuda_launch_gemv_rowwise_f32</a> (const float *W, const float *x, float *y, int rows, int cols, int ldW, float alpha, float beta, <a class="el" href="group__IE__GPU.html#gaefb3d7d282a514152960b6c3d812355b">ie_cuda_stream_t</a> stream)</td></tr>
<tr class="memdesc:ga92e383308a681db6bb757c3add5906f0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Launch row-wise GEMV (float path).  <br /></td></tr>
<tr class="separator:ga92e383308a681db6bb757c3add5906f0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaa2441c827e95b20c0413b0372ffc262b" id="r_gaa2441c827e95b20c0413b0372ffc262b"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__IE__GPU.html#gaa2441c827e95b20c0413b0372ffc262b">ie_cuda_launch_gemv_bias_act_f32</a> (const float *W, const float *x, const float *bias, float *y, int rows, int cols, int ldW, float alpha, float beta, <a class="el" href="group__IE__GPU.html#ga08ba1d3777f64c23e99124d2e460e701">ie_act_kind_t</a> act, <a class="el" href="group__IE__GPU.html#gaefb3d7d282a514152960b6c3d812355b">ie_cuda_stream_t</a> stream)</td></tr>
<tr class="memdesc:gaa2441c827e95b20c0413b0372ffc262b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Launch fused GEMV + bias + activation (float).  <br /></td></tr>
<tr class="separator:gaa2441c827e95b20c0413b0372ffc262b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a525e3ca0d9d0030847aef26b70fa0cb9" id="r_a525e3ca0d9d0030847aef26b70fa0cb9"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a525e3ca0d9d0030847aef26b70fa0cb9">ie_cuda_launch_gemv_rowwise_qi8_f32</a> (const float *W, const int8_t *xq, float *y, int rows, int cols, int ldW, float scale, int zp, float alpha, float beta, <a class="el" href="group__IE__GPU.html#gaefb3d7d282a514152960b6c3d812355b">ie_cuda_stream_t</a> stream)</td></tr>
<tr class="memdesc:a525e3ca0d9d0030847aef26b70fa0cb9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Launch GEMV with INT8 activations (per-tensor), fused dequantization.  <br /></td></tr>
<tr class="separator:a525e3ca0d9d0030847aef26b70fa0cb9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a78cbe5a89ec13c5a9edca717f79caab8" id="r_a78cbe5a89ec13c5a9edca717f79caab8"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a78cbe5a89ec13c5a9edca717f79caab8">ie_cuda_launch_gemv_rowwise_qfp8_f32</a> (const float *W, const uint8_t *x8, float *y, int rows, int cols, int ldW, <a class="el" href="ie__quant__act_8h.html#a67a1bdaa8281d1c69b6b76ecba79c52c">ie_fp8_format</a> fmt, float alpha, float beta, <a class="el" href="group__IE__GPU.html#gaefb3d7d282a514152960b6c3d812355b">ie_cuda_stream_t</a> stream)</td></tr>
<tr class="memdesc:a78cbe5a89ec13c5a9edca717f79caab8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Launch GEMV with FP8 activations (E4M3/E5M2), fused byte decode.  <br /></td></tr>
<tr class="separator:a78cbe5a89ec13c5a9edca717f79caab8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga4e8445935545f1b5bbb671826543ffab" id="r_ga4e8445935545f1b5bbb671826543ffab"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__IE__GPU.html#ga4e8445935545f1b5bbb671826543ffab">ie_cuda_launch_vec_tanh_f32</a> (float *y, const float *x, int n, <a class="el" href="group__IE__GPU.html#gaefb3d7d282a514152960b6c3d812355b">ie_cuda_stream_t</a> stream)</td></tr>
<tr class="memdesc:ga4e8445935545f1b5bbb671826543ffab"><td class="mdescLeft">&#160;</td><td class="mdescRight">Launch elementwise tanh on device.  <br /></td></tr>
<tr class="separator:ga4e8445935545f1b5bbb671826543ffab"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga72b6dfd858bd993a18d9c5dfb3112389" id="r_ga72b6dfd858bd993a18d9c5dfb3112389"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__IE__GPU.html#ga72b6dfd858bd993a18d9c5dfb3112389">ie_cuda_launch_pack_w_blockedk_f32</a> (float *Wp, const float *W, int rows, int cols, int ldW, int block_k, <a class="el" href="group__IE__GPU.html#gaefb3d7d282a514152960b6c3d812355b">ie_cuda_stream_t</a> stream)</td></tr>
<tr class="memdesc:ga72b6dfd858bd993a18d9c5dfb3112389"><td class="mdescLeft">&#160;</td><td class="mdescRight">Launch packing of row-major W into Blocked-K layout on device.  <br /></td></tr>
<tr class="separator:ga72b6dfd858bd993a18d9c5dfb3112389"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="var-members" name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:a5ab38bcd2467e982c68d46e66acfa9ef" id="r_a5ab38bcd2467e982c68d46e66acfa9ef"><td class="memItemLeft" align="right" valign="top">static __thread char&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a5ab38bcd2467e982c68d46e66acfa9ef">g_ie_cuda_err</a> [256] = {0}</td></tr>
<tr class="separator:a5ab38bcd2467e982c68d46e66acfa9ef"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>CUDA implementations of GEMV/activation/packing kernels + C-ABI launchers. </p>
<p>This is the <b>only</b> translation unit that includes CUDA headers. The public header <code><a class="el" href="ie__kernels__cuda_8h.html" title="CUDA GPU kernels and C-ABI launchers for hot-path vector/matrix ops.">ie_kernels_cuda.h</a></code> keeps the rest of the codebase CUDA-agnostic.</p>
<h2><a class="anchor" id="autotoc_md61"></a>
Kernel policy (baseline, safe defaults)</h2>
<ul>
<li><b>GEMV row-wise:</b> one block per output row, 256 threads per block, strided K loop with shared-memory reduction.</li>
<li><b>Fused GEMV+bias+activation:</b> same grid policy with a short epilogue.</li>
<li><b>Vector tanh:</b> grid-stride loop with 256-thread blocks, large grid.</li>
<li><b>Packing (Blocked-K):</b> 2D grid over (cols, rows) with 32x8 threads.</li>
</ul>
<p>Additions in this file:</p><ul>
<li>Fused INT8 activation GEMV (per-tensor parameters).</li>
<li>Fused FP8 activation GEMV (E4M3/E5M2) with on-device decoders.</li>
</ul>
<p>These settings favor portability and predictability. They are easy to tune per architecture once you profile on target GPUs. </p>

<p class="definition">Definition in file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
</div><h2 class="groupheader">Macro Definition Documentation</h2>
<a id="af4c083462b1e956e6461aa246d2af9a6" name="af4c083462b1e956e6461aa246d2af9a6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af4c083462b1e956e6461aa246d2af9a6">&#9670;&#160;</a></span>CUDA_GUARD</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">#define CUDA_GUARD</td>
          <td>(</td>
          <td class="paramtype"></td>          <td class="paramname"><span class="paramname"><em>call</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<b>Value:</b><div class="fragment"><div class="line">  <span class="keywordflow">do</span> {                                                                       \</div>
<div class="line">    cudaError_t _st = (call);                                                \</div>
<div class="line">    if (_st != cudaSuccess) {                                                \</div>
<div class="line">      ie_cuda_set_err(cudaGetErrorString(_st));                              \</div>
<div class="line">      <span class="keywordflow">return</span> -(int)_st;                                                      \</div>
<div class="line">    }                                                                        \</div>
<div class="line">  } <span class="keywordflow">while</span> (0)</div>
</div><!-- fragment -->
<p>CUDA error guard macro that stores the error string and returns. </p>
<p>Use this after launching kernels to convert CUDA errors into negative integer codes and a readable message retrievable via <a class="el" href="group__IE__GPU.html#ga6656c0ddd9df382048e7cd162e0c1d64" title="Retrieve the last CUDA error string set by a launcher in this TU.">ie_cuda_last_error_string()</a>. </p>

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00066">66</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">   66</span><span class="preprocessor">#define CUDA_GUARD(call)                                                     \</span></div>
<div class="line"><span class="lineno">   67</span><span class="preprocessor">  do {                                                                       \</span></div>
<div class="line"><span class="lineno">   68</span><span class="preprocessor">    cudaError_t _st = (call);                                                \</span></div>
<div class="line"><span class="lineno">   69</span><span class="preprocessor">    if (_st != cudaSuccess) {                                                \</span></div>
<div class="line"><span class="lineno">   70</span><span class="preprocessor">      ie_cuda_set_err(cudaGetErrorString(_st));                              \</span></div>
<div class="line"><span class="lineno">   71</span><span class="preprocessor">      return -(int)_st;                                                      \</span></div>
<div class="line"><span class="lineno">   72</span><span class="preprocessor">    }                                                                        \</span></div>
<div class="line"><span class="lineno">   73</span><span class="preprocessor">  } while (0)</span></div>
</div><!-- fragment -->
<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00387">ie_cuda_launch_gemv_bias_act_f32()</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00366">ie_cuda_launch_gemv_rowwise_f32()</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00433">ie_cuda_launch_gemv_rowwise_qfp8_f32()</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00410">ie_cuda_launch_gemv_rowwise_qi8_f32()</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00478">ie_cuda_launch_pack_w_blockedk_f32()</a>, and <a class="el" href="ie__kernels__cuda_8cu_source.html#l00458">ie_cuda_launch_vec_tanh_f32()</a>.</p>

</div>
</div>
<a id="afb3284f12e7d5038612ece3185920449" name="afb3284f12e7d5038612ece3185920449"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afb3284f12e7d5038612ece3185920449">&#9670;&#160;</a></span>IE_CUDA_OK</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">#define IE_CUDA_OK&#160;&#160;&#160;0</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00028">28</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>

<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00387">ie_cuda_launch_gemv_bias_act_f32()</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00366">ie_cuda_launch_gemv_rowwise_f32()</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00433">ie_cuda_launch_gemv_rowwise_qfp8_f32()</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00410">ie_cuda_launch_gemv_rowwise_qi8_f32()</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00478">ie_cuda_launch_pack_w_blockedk_f32()</a>, and <a class="el" href="ie__kernels__cuda_8cu_source.html#l00458">ie_cuda_launch_vec_tanh_f32()</a>.</p>

</div>
</div>
<a id="ad253e9c439228a67be0e55b0f4c81edf" name="ad253e9c439228a67be0e55b0f4c81edf"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad253e9c439228a67be0e55b0f4c81edf">&#9670;&#160;</a></span>IE_CUDA_STREAM_T_DEFINED</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">#define IE_CUDA_STREAM_T_DEFINED</td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00031">31</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>

</div>
</div>
<h2 class="groupheader">Typedef Documentation</h2>
<a id="aa036ed43d83d669bfc8c15c81a7efe73" name="aa036ed43d83d669bfc8c15c81a7efe73"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa036ed43d83d669bfc8c15c81a7efe73">&#9670;&#160;</a></span>ie_cuda_stream_t</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">typedef cudaStream_t <a class="el" href="group__IE__GPU.html#gaefb3d7d282a514152960b6c3d812355b">ie_cuda_stream_t</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00032">32</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>

</div>
</div>
<h2 class="groupheader">Function Documentation</h2>
<a id="a603115e5bb835d21609518d0af053e8d" name="a603115e5bb835d21609518d0af053e8d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a603115e5bb835d21609518d0af053e8d">&#9670;&#160;</a></span>ie_apply_activation()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">__device__ float ie_apply_activation </td>
          <td>(</td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>x</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group__IE__GPU.html#ga08ba1d3777f64c23e99124d2e460e701">ie_act_kind_t</a></td>          <td class="paramname"><span class="paramname"><em>act</em></span>&#160;)</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel inline">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Apply a simple activation kind to a scalar. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">x</td><td>Input value. </td></tr>
    <tr><td class="paramname">act</td><td>Activation selector (<a class="el" href="group__IE__GPU.html#ggab2c254b332e574ed55c2537075337f85a72bddb07e52c2dcecd390cda078cd33f" title="Identity.">IE_ACT_NONE</a>, <a class="el" href="group__IE__GPU.html#ggab2c254b332e574ed55c2537075337f85a4efb020b6e30222eac585f53f1d3e47f" title="ReLU: max(0, x).">IE_ACT_RELU</a>, <a class="el" href="group__IE__GPU.html#ggab2c254b332e574ed55c2537075337f85a5c3ab96a97ea7aa059b44e7508c28a08" title="Hyperbolic tangent.">IE_ACT_TANH</a>). </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Activated output. </dd></dl>

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00085">85</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">   85</span>                                                                        {</div>
<div class="line"><span class="lineno">   86</span>  <span class="keywordflow">if</span> (act == <a class="code hl_enumvalue" href="group__IE__GPU.html#ggab2c254b332e574ed55c2537075337f85a4efb020b6e30222eac585f53f1d3e47f">IE_ACT_RELU</a>) {</div>
<div class="line"><span class="lineno">   87</span>    <span class="keywordflow">return</span> x &gt; 0.f ? x : 0.f;</div>
<div class="line"><span class="lineno">   88</span>  } <span class="keywordflow">else</span> <span class="keywordflow">if</span> (act == <a class="code hl_enumvalue" href="group__IE__GPU.html#ggab2c254b332e574ed55c2537075337f85a5c3ab96a97ea7aa059b44e7508c28a08">IE_ACT_TANH</a>) {</div>
<div class="line"><span class="lineno">   89</span>    <span class="keywordflow">return</span> tanhf(x);</div>
<div class="line"><span class="lineno">   90</span>  }</div>
<div class="line"><span class="lineno">   91</span>  <span class="keywordflow">return</span> x; <span class="comment">/* IE_ACT_NONE */</span></div>
<div class="line"><span class="lineno">   92</span>}</div>
<div class="ttc" id="agroup__IE__GPU_html_ggab2c254b332e574ed55c2537075337f85a4efb020b6e30222eac585f53f1d3e47f"><div class="ttname"><a href="group__IE__GPU.html#ggab2c254b332e574ed55c2537075337f85a4efb020b6e30222eac585f53f1d3e47f">IE_ACT_RELU</a></div><div class="ttdeci">@ IE_ACT_RELU</div><div class="ttdoc">ReLU: max(0, x).</div><div class="ttdef"><b>Definition</b> <a href="ie__kernels__cuda_8h_source.html#l00057">ie_kernels_cuda.h:57</a></div></div>
<div class="ttc" id="agroup__IE__GPU_html_ggab2c254b332e574ed55c2537075337f85a5c3ab96a97ea7aa059b44e7508c28a08"><div class="ttname"><a href="group__IE__GPU.html#ggab2c254b332e574ed55c2537075337f85a5c3ab96a97ea7aa059b44e7508c28a08">IE_ACT_TANH</a></div><div class="ttdeci">@ IE_ACT_TANH</div><div class="ttdoc">Hyperbolic tangent.</div><div class="ttdef"><b>Definition</b> <a href="ie__kernels__cuda_8h_source.html#l00058">ie_kernels_cuda.h:58</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="ie__kernels__cuda_8h_source.html#l00057">IE_ACT_RELU</a>, and <a class="el" href="ie__kernels__cuda_8h_source.html#l00058">IE_ACT_TANH</a>.</p>

<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00188">k_gemv_bias_act_f32()</a>.</p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu_a603115e5bb835d21609518d0af053e8d_icgraph.svg" width="583" height="56"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>

</div>
</div>
<a id="a78cbe5a89ec13c5a9edca717f79caab8" name="a78cbe5a89ec13c5a9edca717f79caab8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a78cbe5a89ec13c5a9edca717f79caab8">&#9670;&#160;</a></span>ie_cuda_launch_gemv_rowwise_qfp8_f32()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int ie_cuda_launch_gemv_rowwise_qfp8_f32 </td>
          <td>(</td>
          <td class="paramtype">const float *</td>          <td class="paramname"><span class="paramname"><em>W</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const uint8_t *</td>          <td class="paramname"><span class="paramname"><em>x8</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *</td>          <td class="paramname"><span class="paramname"><em>y</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>rows</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>cols</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>ldW</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="ie__quant__act_8h.html#a67a1bdaa8281d1c69b6b76ecba79c52c">ie_fp8_format</a></td>          <td class="paramname"><span class="paramname"><em>fmt</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>alpha</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>beta</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group__IE__GPU.html#gaefb3d7d282a514152960b6c3d812355b">ie_cuda_stream_t</a></td>          <td class="paramname"><span class="paramname"><em>stream</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Launch GEMV with FP8 activations (E4M3/E5M2), fused byte decode. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">W</td><td>Float weights on device (row-major), size rows x ldW. </td></tr>
    <tr><td class="paramname">x8</td><td>FP8 activations on device (bytes), length cols. </td></tr>
    <tr><td class="paramname">y</td><td>Output on device, length rows. </td></tr>
    <tr><td class="paramname">rows</td><td>Rows. </td></tr>
    <tr><td class="paramname">cols</td><td>Cols. </td></tr>
    <tr><td class="paramname">ldW</td><td>Leading dimension (&gt;= cols). </td></tr>
    <tr><td class="paramname">fmt</td><td>FP8 format (<a class="el" href="ie__quant__act_8h.html#a67a1bdaa8281d1c69b6b76ecba79c52ca67cfe56fcb473bed9a2f92388f2fc347" title="1 sign, 4 exponent (bias 7), 3 mantissa, finite-only saturation.">IE_FP8_E4M3</a> or <a class="el" href="ie__quant__act_8h.html#a67a1bdaa8281d1c69b6b76ecba79c52caa1844f8dd595aba744eb3243b127a3e0" title="1 sign, 5 exponent (bias 15), 2 mantissa, Inf/NaN representable.">IE_FP8_E5M2</a>). </td></tr>
    <tr><td class="paramname">alpha</td><td>Scale for W*x. </td></tr>
    <tr><td class="paramname">beta</td><td>Scale for existing y.    </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>0 on success or negative CUDA error code on failure. </dd></dl>

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00433">433</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  436</span>                                                                              {</div>
<div class="line"><span class="lineno">  437</span>  <a class="code hl_function" href="#a841d35ecb02f9850f311a5724d21deac">ie_cuda_set_err</a>(NULL);</div>
<div class="line"><span class="lineno">  438</span>  <span class="keywordflow">if</span> (!W || !x8 || !y || rows &lt;= 0 || cols &lt;= 0 || ldW &lt; cols) {</div>
<div class="line"><span class="lineno">  439</span>    <a class="code hl_function" href="#a841d35ecb02f9850f311a5724d21deac">ie_cuda_set_err</a>(<span class="stringliteral">&quot;invalid arguments&quot;</span>);</div>
<div class="line"><span class="lineno">  440</span>    <span class="keywordflow">return</span> -1;</div>
<div class="line"><span class="lineno">  441</span>  }</div>
<div class="line"><span class="lineno">  442</span>  dim3 block(256, 1, 1);</div>
<div class="line"><span class="lineno">  443</span>  dim3 grid(rows, 1, 1);</div>
<div class="line"><span class="lineno">  444</span>  <a class="code hl_function" href="#a74040aacee610aa771ee11bb0bc74022">k_gemv_rowwise_qfp8_f32&lt;&lt;&lt;grid, block, 0, stream&gt;</a>&gt;&gt;(W, x8, y, rows, cols,</div>
<div class="line"><span class="lineno">  445</span>                                                      ldW, fmt, alpha, beta);</div>
<div class="line"><span class="lineno">  446</span>  <a class="code hl_define" href="#af4c083462b1e956e6461aa246d2af9a6">CUDA_GUARD</a>(cudaGetLastError());</div>
<div class="line"><span class="lineno">  447</span>  <span class="keywordflow">return</span> <a class="code hl_define" href="#afb3284f12e7d5038612ece3185920449">IE_CUDA_OK</a>;</div>
<div class="line"><span class="lineno">  448</span>}</div>
<div class="ttc" id="aie__kernels__cuda_8cu_html_a74040aacee610aa771ee11bb0bc74022"><div class="ttname"><a href="#a74040aacee610aa771ee11bb0bc74022">k_gemv_rowwise_qfp8_f32</a></div><div class="ttdeci">__global__ void k_gemv_rowwise_qfp8_f32(const float *__restrict__ W, const uint8_t *__restrict__ x8, float *__restrict__ y, int rows, int cols, int ldW, ie_fp8_format fmt, float alpha, float beta)</div><div class="ttdoc">Row-wise GEMV with FP8 activations (E4M3/E5M2), fused byte decode.</div><div class="ttdef"><b>Definition</b> <a href="ie__kernels__cuda_8cu_source.html#l00278">ie_kernels_cuda.cu:278</a></div></div>
<div class="ttc" id="aie__kernels__cuda_8cu_html_a841d35ecb02f9850f311a5724d21deac"><div class="ttname"><a href="#a841d35ecb02f9850f311a5724d21deac">ie_cuda_set_err</a></div><div class="ttdeci">static void ie_cuda_set_err(const char *msg)</div><div class="ttdoc">Set or clear the per-thread CUDA error string buffer.</div><div class="ttdef"><b>Definition</b> <a href="ie__kernels__cuda_8cu_source.html#l00043">ie_kernels_cuda.cu:43</a></div></div>
<div class="ttc" id="aie__kernels__cuda_8cu_html_af4c083462b1e956e6461aa246d2af9a6"><div class="ttname"><a href="#af4c083462b1e956e6461aa246d2af9a6">CUDA_GUARD</a></div><div class="ttdeci">#define CUDA_GUARD(call)</div><div class="ttdoc">CUDA error guard macro that stores the error string and returns.</div><div class="ttdef"><b>Definition</b> <a href="ie__kernels__cuda_8cu_source.html#l00066">ie_kernels_cuda.cu:66</a></div></div>
<div class="ttc" id="aie__kernels__cuda_8cu_html_afb3284f12e7d5038612ece3185920449"><div class="ttname"><a href="#afb3284f12e7d5038612ece3185920449">IE_CUDA_OK</a></div><div class="ttdeci">#define IE_CUDA_OK</div><div class="ttdef"><b>Definition</b> <a href="ie__kernels__cuda_8cu_source.html#l00028">ie_kernels_cuda.cu:28</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="ie__kernels__cuda_8cu_source.html#l00066">CUDA_GUARD</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00028">IE_CUDA_OK</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00043">ie_cuda_set_err()</a>, and <a class="el" href="ie__kernels__cuda_8cu_source.html#l00278">k_gemv_rowwise_qfp8_f32()</a>.</p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu_a78cbe5a89ec13c5a9edca717f79caab8_cgraph.svg" width="654" height="116"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>

</div>
</div>
<a id="a525e3ca0d9d0030847aef26b70fa0cb9" name="a525e3ca0d9d0030847aef26b70fa0cb9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a525e3ca0d9d0030847aef26b70fa0cb9">&#9670;&#160;</a></span>ie_cuda_launch_gemv_rowwise_qi8_f32()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int ie_cuda_launch_gemv_rowwise_qi8_f32 </td>
          <td>(</td>
          <td class="paramtype">const float *</td>          <td class="paramname"><span class="paramname"><em>W</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int8_t *</td>          <td class="paramname"><span class="paramname"><em>xq</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *</td>          <td class="paramname"><span class="paramname"><em>y</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>rows</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>cols</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>ldW</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>scale</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>zp</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>alpha</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>beta</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group__IE__GPU.html#gaefb3d7d282a514152960b6c3d812355b">ie_cuda_stream_t</a></td>          <td class="paramname"><span class="paramname"><em>stream</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Launch GEMV with INT8 activations (per-tensor), fused dequantization. </p>
<p>Dequantization model: real = scale * (q - zero_point).</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">W</td><td>Float weights on device (row-major), size rows x ldW. </td></tr>
    <tr><td class="paramname">xq</td><td>INT8 activations on device, length cols. </td></tr>
    <tr><td class="paramname">y</td><td>Output on device, length rows. </td></tr>
    <tr><td class="paramname">rows</td><td>Rows. </td></tr>
    <tr><td class="paramname">cols</td><td>Cols. </td></tr>
    <tr><td class="paramname">ldW</td><td>Leading dimension (&gt;= cols). </td></tr>
    <tr><td class="paramname">scale</td><td>Per-tensor scale. </td></tr>
    <tr><td class="paramname">zp</td><td>Per-tensor zero-point (int). </td></tr>
    <tr><td class="paramname">alpha</td><td>Scale for W*x. </td></tr>
    <tr><td class="paramname">beta</td><td>Scale for existing y.    </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>0 on success or negative CUDA error code on failure. </dd></dl>

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00410">410</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  413</span>                                                                             {</div>
<div class="line"><span class="lineno">  414</span>  <a class="code hl_function" href="#a841d35ecb02f9850f311a5724d21deac">ie_cuda_set_err</a>(NULL);</div>
<div class="line"><span class="lineno">  415</span>  <span class="keywordflow">if</span> (!W || !xq || !y || rows &lt;= 0 || cols &lt;= 0 || ldW &lt; cols) {</div>
<div class="line"><span class="lineno">  416</span>    <a class="code hl_function" href="#a841d35ecb02f9850f311a5724d21deac">ie_cuda_set_err</a>(<span class="stringliteral">&quot;invalid arguments&quot;</span>);</div>
<div class="line"><span class="lineno">  417</span>    <span class="keywordflow">return</span> -1;</div>
<div class="line"><span class="lineno">  418</span>  }</div>
<div class="line"><span class="lineno">  419</span>  dim3 block(256, 1, 1);</div>
<div class="line"><span class="lineno">  420</span>  dim3 grid(rows, 1, 1);</div>
<div class="line"><span class="lineno">  421</span>  <a class="code hl_function" href="#abb0d57e76ef0b1dea1ae1b8bb55e1c02">k_gemv_rowwise_qi8_f32&lt;&lt;&lt;grid, block, 0, stream&gt;</a>&gt;&gt;(W, xq, y, rows, cols,</div>
<div class="line"><span class="lineno">  422</span>                                                     ldW, scale, zp, alpha,</div>
<div class="line"><span class="lineno">  423</span>                                                     beta);</div>
<div class="line"><span class="lineno">  424</span>  <a class="code hl_define" href="#af4c083462b1e956e6461aa246d2af9a6">CUDA_GUARD</a>(cudaGetLastError());</div>
<div class="line"><span class="lineno">  425</span>  <span class="keywordflow">return</span> <a class="code hl_define" href="#afb3284f12e7d5038612ece3185920449">IE_CUDA_OK</a>;</div>
<div class="line"><span class="lineno">  426</span>}</div>
<div class="ttc" id="aie__kernels__cuda_8cu_html_abb0d57e76ef0b1dea1ae1b8bb55e1c02"><div class="ttname"><a href="#abb0d57e76ef0b1dea1ae1b8bb55e1c02">k_gemv_rowwise_qi8_f32</a></div><div class="ttdeci">__global__ void k_gemv_rowwise_qi8_f32(const float *__restrict__ W, const int8_t *__restrict__ xq, float *__restrict__ y, int rows, int cols, int ldW, float scale, int zp, float alpha, float beta)</div><div class="ttdoc">Row-wise GEMV with INT8 activations (per-tensor), fused dequantization.</div><div class="ttdef"><b>Definition</b> <a href="ie__kernels__cuda_8cu_source.html#l00235">ie_kernels_cuda.cu:235</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="ie__kernels__cuda_8cu_source.html#l00066">CUDA_GUARD</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00028">IE_CUDA_OK</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00043">ie_cuda_set_err()</a>, and <a class="el" href="ie__kernels__cuda_8cu_source.html#l00235">k_gemv_rowwise_qi8_f32()</a>.</p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu_a525e3ca0d9d0030847aef26b70fa0cb9_cgraph.svg" width="414" height="91"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>

</div>
</div>
<a id="a841d35ecb02f9850f311a5724d21deac" name="a841d35ecb02f9850f311a5724d21deac"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a841d35ecb02f9850f311a5724d21deac">&#9670;&#160;</a></span>ie_cuda_set_err()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">static void ie_cuda_set_err </td>
          <td>(</td>
          <td class="paramtype">const char *</td>          <td class="paramname"><span class="paramname"><em>msg</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel inline">inline</span><span class="mlabel static">static</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Set or clear the per-thread CUDA error string buffer. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">msg</td><td>NULL to clear; otherwise a short literal/diagnostic string. </td></tr>
  </table>
  </dd>
</dl>

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00043">43</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">   43</span>                                                    {</div>
<div class="line"><span class="lineno">   44</span>  <span class="keywordflow">if</span> (!msg) {</div>
<div class="line"><span class="lineno">   45</span>    <a class="code hl_variable" href="#a5ab38bcd2467e982c68d46e66acfa9ef">g_ie_cuda_err</a>[0] = <span class="charliteral">&#39;\0&#39;</span>;</div>
<div class="line"><span class="lineno">   46</span>    <span class="keywordflow">return</span>;</div>
<div class="line"><span class="lineno">   47</span>  }</div>
<div class="line"><span class="lineno">   48</span>  <span class="keywordtype">size_t</span> i = 0;</div>
<div class="line"><span class="lineno">   49</span>  <span class="keywordflow">for</span> (; i + 1 &lt; <span class="keyword">sizeof</span>(<a class="code hl_variable" href="#a5ab38bcd2467e982c68d46e66acfa9ef">g_ie_cuda_err</a>) &amp;&amp; msg[i]; ++i) <a class="code hl_variable" href="#a5ab38bcd2467e982c68d46e66acfa9ef">g_ie_cuda_err</a>[i] = msg[i];</div>
<div class="line"><span class="lineno">   50</span>  <a class="code hl_variable" href="#a5ab38bcd2467e982c68d46e66acfa9ef">g_ie_cuda_err</a>[i] = <span class="charliteral">&#39;\0&#39;</span>;</div>
<div class="line"><span class="lineno">   51</span>}</div>
<div class="ttc" id="aie__kernels__cuda_8cu_html_a5ab38bcd2467e982c68d46e66acfa9ef"><div class="ttname"><a href="#a5ab38bcd2467e982c68d46e66acfa9ef">g_ie_cuda_err</a></div><div class="ttdeci">static __thread char g_ie_cuda_err[256]</div><div class="ttdef"><b>Definition</b> <a href="ie__kernels__cuda_8cu_source.html#l00037">ie_kernels_cuda.cu:37</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="ie__kernels__cuda_8cu_source.html#l00037">g_ie_cuda_err</a>.</p>

<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00387">ie_cuda_launch_gemv_bias_act_f32()</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00366">ie_cuda_launch_gemv_rowwise_f32()</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00433">ie_cuda_launch_gemv_rowwise_qfp8_f32()</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00410">ie_cuda_launch_gemv_rowwise_qi8_f32()</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00478">ie_cuda_launch_pack_w_blockedk_f32()</a>, and <a class="el" href="ie__kernels__cuda_8cu_source.html#l00458">ie_cuda_launch_vec_tanh_f32()</a>.</p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu_a841d35ecb02f9850f311a5724d21deac_icgraph.svg" width="348" height="403"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>

</div>
</div>
<a id="a8d15fdbcd086d7ef5932207fa5b5d91b" name="a8d15fdbcd086d7ef5932207fa5b5d91b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8d15fdbcd086d7ef5932207fa5b5d91b">&#9670;&#160;</a></span>ie_decode_fp8_e4m3_u8()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">__device__ float ie_decode_fp8_e4m3_u8 </td>
          <td>(</td>
          <td class="paramtype">uint8_t</td>          <td class="paramname"><span class="paramname"><em>v</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel inline">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Decode one E4M3 FP8 byte on device (subnormals flushed to zero). </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">v</td><td>Encoded E4M3 byte. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Decoded 32-bit float. </dd></dl>

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00101">101</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  101</span>                                                         {</div>
<div class="line"><span class="lineno">  102</span>  <span class="keywordflow">if</span> (v == 0u) <span class="keywordflow">return</span> 0.0f;</div>
<div class="line"><span class="lineno">  103</span>  <span class="keyword">const</span> uint8_t sign = (v &gt;&gt; 7) &amp; 0x1;</div>
<div class="line"><span class="lineno">  104</span>  <span class="keyword">const</span> uint8_t exp = (v &gt;&gt; 3) &amp; 0xF;</div>
<div class="line"><span class="lineno">  105</span>  <span class="keyword">const</span> uint8_t man = (v &amp; 0x7);</div>
<div class="line"><span class="lineno">  106</span>  <span class="keywordflow">if</span> (exp == 0) <span class="keywordflow">return</span> sign ? -0.0f : 0.0f;</div>
<div class="line"><span class="lineno">  107</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> bias = 7;</div>
<div class="line"><span class="lineno">  108</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> e = (int)exp - bias;</div>
<div class="line"><span class="lineno">  109</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> frac = (float)man / 8.0f;</div>
<div class="line"><span class="lineno">  110</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> val = (1.0f + frac) * __int2float_rn(1 &lt;&lt; e);</div>
<div class="line"><span class="lineno">  111</span>  <span class="keywordflow">return</span> sign ? -val : val;</div>
<div class="line"><span class="lineno">  112</span>}</div>
</div><!-- fragment -->
<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00278">k_gemv_rowwise_qfp8_f32()</a>.</p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu_a8d15fdbcd086d7ef5932207fa5b5d91b_icgraph.svg" width="654" height="56"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>

</div>
</div>
<a id="a98464ed99207f2ed392f751bae2ea10e" name="a98464ed99207f2ed392f751bae2ea10e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a98464ed99207f2ed392f751bae2ea10e">&#9670;&#160;</a></span>ie_decode_fp8_e5m2_u8()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">__device__ float ie_decode_fp8_e5m2_u8 </td>
          <td>(</td>
          <td class="paramtype">uint8_t</td>          <td class="paramname"><span class="paramname"><em>v</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel inline">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Decode one E5M2 FP8 byte on device (IEEE-like special cases). </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">v</td><td>Encoded E5M2 byte. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>Decoded 32-bit float (NaN/Inf propagated). </dd></dl>

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00119">119</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  119</span>                                                         {</div>
<div class="line"><span class="lineno">  120</span>  <span class="keyword">const</span> uint8_t sign = (v &gt;&gt; 7) &amp; 0x1;</div>
<div class="line"><span class="lineno">  121</span>  <span class="keyword">const</span> uint8_t exp = (v &gt;&gt; 2) &amp; 0x1F;</div>
<div class="line"><span class="lineno">  122</span>  <span class="keyword">const</span> uint8_t man = (v &amp; 0x3);</div>
<div class="line"><span class="lineno">  123</span>  <span class="keywordflow">if</span> (exp == 0) <span class="keywordflow">return</span> sign ? -0.0f : 0.0f;</div>
<div class="line"><span class="lineno">  124</span>  <span class="keywordflow">if</span> (exp == 0x1F) {</div>
<div class="line"><span class="lineno">  125</span>    <span class="keywordflow">if</span> (man == 0) <span class="keywordflow">return</span> sign ? -CUDART_INF_F : CUDART_INF_F;</div>
<div class="line"><span class="lineno">  126</span>    <span class="keywordflow">return</span> CUDART_NAN_F;</div>
<div class="line"><span class="lineno">  127</span>  }</div>
<div class="line"><span class="lineno">  128</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> bias = 15;</div>
<div class="line"><span class="lineno">  129</span>  <span class="keyword">const</span> <span class="keywordtype">int</span> e = (int)exp - bias;</div>
<div class="line"><span class="lineno">  130</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> frac = (float)man / 4.0f;</div>
<div class="line"><span class="lineno">  131</span>  <span class="keyword">const</span> <span class="keywordtype">float</span> val = (1.0f + frac) * __int2float_rn(1 &lt;&lt; e);</div>
<div class="line"><span class="lineno">  132</span>  <span class="keywordflow">return</span> sign ? -val : val;</div>
<div class="line"><span class="lineno">  133</span>}</div>
</div><!-- fragment -->
<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00278">k_gemv_rowwise_qfp8_f32()</a>.</p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu_a98464ed99207f2ed392f751bae2ea10e_icgraph.svg" width="654" height="56"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>

</div>
</div>
<a id="ab9150fe6fc7da507974bfdd38b262ce2" name="ab9150fe6fc7da507974bfdd38b262ce2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab9150fe6fc7da507974bfdd38b262ce2">&#9670;&#160;</a></span>k_gemv_bias_act_f32()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">__global__ void k_gemv_bias_act_f32 </td>
          <td>(</td>
          <td class="paramtype">const float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>W</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>x</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>bias</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>y</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>rows</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>cols</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>ldW</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>alpha</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>beta</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="group__IE__GPU.html#ga08ba1d3777f64c23e99124d2e460e701">ie_act_kind_t</a></td>          <td class="paramname"><span class="paramname"><em>act</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Fused GEMV + bias + activation kernel (float activations). </p>
<p>Same mapping as <a class="el" href="#af00402674d56c833c92c62df1cef1e84" title="Row-wise GEMV kernel: y = alpha * W * x + beta * y.">k_gemv_rowwise_f32</a> with an epilogue that adds bias and applies a simple activation (none/ReLU/tanh). </p>

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00188">188</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  193</span>                                                       {</div>
<div class="line"><span class="lineno">  194</span>  <span class="keywordtype">int</span> r = blockIdx.x;</div>
<div class="line"><span class="lineno">  195</span>  <span class="keywordflow">if</span> (r &gt;= rows) <span class="keywordflow">return</span>;</div>
<div class="line"><span class="lineno">  196</span> </div>
<div class="line"><span class="lineno">  197</span>  <span class="keywordtype">float</span> acc = 0.f;</div>
<div class="line"><span class="lineno">  198</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> k = threadIdx.x; k &lt; cols; k += blockDim.x) {</div>
<div class="line"><span class="lineno">  199</span>    acc += W[(size_t)r * (<span class="keywordtype">size_t</span>)ldW + (size_t)k] * x[k];</div>
<div class="line"><span class="lineno">  200</span>  }</div>
<div class="line"><span class="lineno">  201</span> </div>
<div class="line"><span class="lineno">  202</span>  __shared__ <span class="keywordtype">float</span> buf[256];</div>
<div class="line"><span class="lineno">  203</span>  buf[threadIdx.x] = acc;</div>
<div class="line"><span class="lineno">  204</span>  __syncthreads();</div>
<div class="line"><span class="lineno">  205</span> </div>
<div class="line"><span class="lineno">  206</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> s = blockDim.x &gt;&gt; 1; s &gt; 0; s &gt;&gt;= 1) {</div>
<div class="line"><span class="lineno">  207</span>    <span class="keywordflow">if</span> (threadIdx.x &lt; s) buf[threadIdx.x] += buf[threadIdx.x + s];</div>
<div class="line"><span class="lineno">  208</span>    __syncthreads();</div>
<div class="line"><span class="lineno">  209</span>  }</div>
<div class="line"><span class="lineno">  210</span> </div>
<div class="line"><span class="lineno">  211</span>  <span class="keywordflow">if</span> (threadIdx.x == 0) {</div>
<div class="line"><span class="lineno">  212</span>    <span class="keywordtype">float</span> b = bias ? bias[r] : 0.f;</div>
<div class="line"><span class="lineno">  213</span>    <span class="keywordtype">float</span> out = alpha * buf[0] + b;</div>
<div class="line"><span class="lineno">  214</span>    <span class="keywordflow">if</span> (beta != 0.f) out += beta * y[r];</div>
<div class="line"><span class="lineno">  215</span>    y[r] = <a class="code hl_function" href="#a603115e5bb835d21609518d0af053e8d">ie_apply_activation</a>(out, act);</div>
<div class="line"><span class="lineno">  216</span>  }</div>
<div class="line"><span class="lineno">  217</span>}</div>
<div class="ttc" id="aie__kernels__cuda_8cu_html_a603115e5bb835d21609518d0af053e8d"><div class="ttname"><a href="#a603115e5bb835d21609518d0af053e8d">ie_apply_activation</a></div><div class="ttdeci">__device__ float ie_apply_activation(float x, ie_act_kind_t act)</div><div class="ttdoc">Apply a simple activation kind to a scalar.</div><div class="ttdef"><b>Definition</b> <a href="ie__kernels__cuda_8cu_source.html#l00085">ie_kernels_cuda.cu:85</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="ie__kernels__cuda_8cu_source.html#l00085">ie_apply_activation()</a>.</p>

<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00387">ie_cuda_launch_gemv_bias_act_f32()</a>.</p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu_ab9150fe6fc7da507974bfdd38b262ce2_cgraph.svg" width="370" height="39"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu_ab9150fe6fc7da507974bfdd38b262ce2_icgraph.svg" width="387" height="56"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>

</div>
</div>
<a id="af00402674d56c833c92c62df1cef1e84" name="af00402674d56c833c92c62df1cef1e84"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af00402674d56c833c92c62df1cef1e84">&#9670;&#160;</a></span>k_gemv_rowwise_f32()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">__global__ void k_gemv_rowwise_f32 </td>
          <td>(</td>
          <td class="paramtype">const float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>W</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>x</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>y</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>rows</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>cols</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>ldW</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>alpha</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>beta</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Row-wise GEMV kernel: y = alpha * W * x + beta * y. </p>
<p><b>Mapping:</b> 1 block per row. Threads accumulate a strided reduction over K and reduce via shared memory. Assumes <code class="param">ldW</code> &gt;= <code class="param">cols</code>.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">W</td><td>Row-major matrix on device, size rows x ldW. </td></tr>
    <tr><td class="paramname">x</td><td>Input vector on device, size cols. </td></tr>
    <tr><td class="paramname">y</td><td>Output vector on device, size rows. </td></tr>
    <tr><td class="paramname">rows</td><td>Number of rows / outputs. </td></tr>
    <tr><td class="paramname">cols</td><td>Number of columns / inputs. </td></tr>
    <tr><td class="paramname">ldW</td><td>Leading dimension (&gt;= cols). </td></tr>
    <tr><td class="paramname">alpha</td><td>Scale for W*x. </td></tr>
    <tr><td class="paramname">beta</td><td>Scale for existing y. </td></tr>
  </table>
  </dd>
</dl>

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00154">154</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  157</span>                                                                     {</div>
<div class="line"><span class="lineno">  158</span>  <span class="keywordtype">int</span> r = blockIdx.x;</div>
<div class="line"><span class="lineno">  159</span>  <span class="keywordflow">if</span> (r &gt;= rows) <span class="keywordflow">return</span>;</div>
<div class="line"><span class="lineno">  160</span> </div>
<div class="line"><span class="lineno">  161</span>  <span class="keywordtype">float</span> acc = 0.f;</div>
<div class="line"><span class="lineno">  162</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> k = threadIdx.x; k &lt; cols; k += blockDim.x) {</div>
<div class="line"><span class="lineno">  163</span>    acc += W[(size_t)r * (<span class="keywordtype">size_t</span>)ldW + (size_t)k] * x[k];</div>
<div class="line"><span class="lineno">  164</span>  }</div>
<div class="line"><span class="lineno">  165</span> </div>
<div class="line"><span class="lineno">  166</span>  __shared__ <span class="keywordtype">float</span> buf[256];</div>
<div class="line"><span class="lineno">  167</span>  buf[threadIdx.x] = acc;</div>
<div class="line"><span class="lineno">  168</span>  __syncthreads();</div>
<div class="line"><span class="lineno">  169</span> </div>
<div class="line"><span class="lineno">  170</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> s = blockDim.x &gt;&gt; 1; s &gt; 0; s &gt;&gt;= 1) {</div>
<div class="line"><span class="lineno">  171</span>    <span class="keywordflow">if</span> (threadIdx.x &lt; s) buf[threadIdx.x] += buf[threadIdx.x + s];</div>
<div class="line"><span class="lineno">  172</span>    __syncthreads();</div>
<div class="line"><span class="lineno">  173</span>  }</div>
<div class="line"><span class="lineno">  174</span> </div>
<div class="line"><span class="lineno">  175</span>  <span class="keywordflow">if</span> (threadIdx.x == 0) {</div>
<div class="line"><span class="lineno">  176</span>    <span class="keywordtype">float</span> out = alpha * buf[0];</div>
<div class="line"><span class="lineno">  177</span>    <span class="keywordflow">if</span> (beta != 0.f) out += beta * y[r];</div>
<div class="line"><span class="lineno">  178</span>    y[r] = out;</div>
<div class="line"><span class="lineno">  179</span>  }</div>
<div class="line"><span class="lineno">  180</span>}</div>
</div><!-- fragment -->
<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00366">ie_cuda_launch_gemv_rowwise_f32()</a>.</p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu_af00402674d56c833c92c62df1cef1e84_icgraph.svg" width="387" height="56"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>

</div>
</div>
<a id="a74040aacee610aa771ee11bb0bc74022" name="a74040aacee610aa771ee11bb0bc74022"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a74040aacee610aa771ee11bb0bc74022">&#9670;&#160;</a></span>k_gemv_rowwise_qfp8_f32()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">__global__ void k_gemv_rowwise_qfp8_f32 </td>
          <td>(</td>
          <td class="paramtype">const float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>W</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const uint8_t *__restrict__</td>          <td class="paramname"><span class="paramname"><em>x8</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>y</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>rows</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>cols</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>ldW</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="ie__quant__act_8h.html#a67a1bdaa8281d1c69b6b76ecba79c52c">ie_fp8_format</a></td>          <td class="paramname"><span class="paramname"><em>fmt</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>alpha</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>beta</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Row-wise GEMV with FP8 activations (E4M3/E5M2), fused byte decode. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">W</td><td>Float weights on device (row-major), size rows x ldW. </td></tr>
    <tr><td class="paramname">x8</td><td>FP8 activations on device (bytes), length cols. </td></tr>
    <tr><td class="paramname">y</td><td>Output on device, length rows. </td></tr>
    <tr><td class="paramname">rows</td><td>Rows. </td></tr>
    <tr><td class="paramname">cols</td><td>Cols. </td></tr>
    <tr><td class="paramname">ldW</td><td>Leading dimension (&gt;= cols). </td></tr>
    <tr><td class="paramname">fmt</td><td>FP8 format (<a class="el" href="ie__quant__act_8h.html#a67a1bdaa8281d1c69b6b76ecba79c52ca67cfe56fcb473bed9a2f92388f2fc347" title="1 sign, 4 exponent (bias 7), 3 mantissa, finite-only saturation.">IE_FP8_E4M3</a> or <a class="el" href="ie__quant__act_8h.html#a67a1bdaa8281d1c69b6b76ecba79c52caa1844f8dd595aba744eb3243b127a3e0" title="1 sign, 5 exponent (bias 15), 2 mantissa, Inf/NaN representable.">IE_FP8_E5M2</a>). </td></tr>
    <tr><td class="paramname">alpha</td><td>Scale for W*x. </td></tr>
    <tr><td class="paramname">beta</td><td>Scale for existing y. </td></tr>
  </table>
  </dd>
</dl>

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00278">278</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  282</span>                                                                 {</div>
<div class="line"><span class="lineno">  283</span>  <span class="keywordtype">int</span> r = blockIdx.x;</div>
<div class="line"><span class="lineno">  284</span>  <span class="keywordflow">if</span> (r &gt;= rows) <span class="keywordflow">return</span>;</div>
<div class="line"><span class="lineno">  285</span> </div>
<div class="line"><span class="lineno">  286</span>  <span class="keyword">const</span> <span class="keywordtype">bool</span> e4m3 = (fmt == <a class="code hl_enumvalue" href="ie__quant__act_8h.html#a67a1bdaa8281d1c69b6b76ecba79c52ca67cfe56fcb473bed9a2f92388f2fc347">IE_FP8_E4M3</a>);</div>
<div class="line"><span class="lineno">  287</span> </div>
<div class="line"><span class="lineno">  288</span>  <span class="keywordtype">float</span> acc = 0.f;</div>
<div class="line"><span class="lineno">  289</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> k = threadIdx.x; k &lt; cols; k += blockDim.x) {</div>
<div class="line"><span class="lineno">  290</span>    <span class="keyword">const</span> uint8_t b = x8[k];</div>
<div class="line"><span class="lineno">  291</span>    <span class="keyword">const</span> <span class="keywordtype">float</span> xv =</div>
<div class="line"><span class="lineno">  292</span>        e4m3 ? <a class="code hl_function" href="#a8d15fdbcd086d7ef5932207fa5b5d91b">ie_decode_fp8_e4m3_u8</a>(b) : <a class="code hl_function" href="#a98464ed99207f2ed392f751bae2ea10e">ie_decode_fp8_e5m2_u8</a>(b);</div>
<div class="line"><span class="lineno">  293</span>    acc += W[(size_t)r * (size_t)ldW + (size_t)k] * xv;</div>
<div class="line"><span class="lineno">  294</span>  }</div>
<div class="line"><span class="lineno">  295</span> </div>
<div class="line"><span class="lineno">  296</span>  __shared__ <span class="keywordtype">float</span> buf[256];</div>
<div class="line"><span class="lineno">  297</span>  buf[threadIdx.x] = acc;</div>
<div class="line"><span class="lineno">  298</span>  __syncthreads();</div>
<div class="line"><span class="lineno">  299</span> </div>
<div class="line"><span class="lineno">  300</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> s = blockDim.x &gt;&gt; 1; s &gt; 0; s &gt;&gt;= 1) {</div>
<div class="line"><span class="lineno">  301</span>    <span class="keywordflow">if</span> (threadIdx.x &lt; s) buf[threadIdx.x] += buf[threadIdx.x + s];</div>
<div class="line"><span class="lineno">  302</span>    __syncthreads();</div>
<div class="line"><span class="lineno">  303</span>  }</div>
<div class="line"><span class="lineno">  304</span> </div>
<div class="line"><span class="lineno">  305</span>  <span class="keywordflow">if</span> (threadIdx.x == 0) {</div>
<div class="line"><span class="lineno">  306</span>    <span class="keywordtype">float</span> out = alpha * buf[0];</div>
<div class="line"><span class="lineno">  307</span>    <span class="keywordflow">if</span> (beta != 0.f) out += beta * y[r];</div>
<div class="line"><span class="lineno">  308</span>    y[r] = out;</div>
<div class="line"><span class="lineno">  309</span>  }</div>
<div class="line"><span class="lineno">  310</span>}</div>
<div class="ttc" id="aie__kernels__cuda_8cu_html_a8d15fdbcd086d7ef5932207fa5b5d91b"><div class="ttname"><a href="#a8d15fdbcd086d7ef5932207fa5b5d91b">ie_decode_fp8_e4m3_u8</a></div><div class="ttdeci">__device__ float ie_decode_fp8_e4m3_u8(uint8_t v)</div><div class="ttdoc">Decode one E4M3 FP8 byte on device (subnormals flushed to zero).</div><div class="ttdef"><b>Definition</b> <a href="ie__kernels__cuda_8cu_source.html#l00101">ie_kernels_cuda.cu:101</a></div></div>
<div class="ttc" id="aie__kernels__cuda_8cu_html_a98464ed99207f2ed392f751bae2ea10e"><div class="ttname"><a href="#a98464ed99207f2ed392f751bae2ea10e">ie_decode_fp8_e5m2_u8</a></div><div class="ttdeci">__device__ float ie_decode_fp8_e5m2_u8(uint8_t v)</div><div class="ttdoc">Decode one E5M2 FP8 byte on device (IEEE-like special cases).</div><div class="ttdef"><b>Definition</b> <a href="ie__kernels__cuda_8cu_source.html#l00119">ie_kernels_cuda.cu:119</a></div></div>
<div class="ttc" id="aie__quant__act_8h_html_a67a1bdaa8281d1c69b6b76ecba79c52ca67cfe56fcb473bed9a2f92388f2fc347"><div class="ttname"><a href="ie__quant__act_8h.html#a67a1bdaa8281d1c69b6b76ecba79c52ca67cfe56fcb473bed9a2f92388f2fc347">IE_FP8_E4M3</a></div><div class="ttdeci">@ IE_FP8_E4M3</div><div class="ttdoc">1 sign, 4 exponent (bias 7), 3 mantissa, finite-only saturation.</div><div class="ttdef"><b>Definition</b> <a href="ie__quant__act_8h_source.html#l00041">ie_quant_act.h:41</a></div></div>
</div><!-- fragment -->
<p class="reference">References <a class="el" href="ie__kernels__cuda_8cu_source.html#l00101">ie_decode_fp8_e4m3_u8()</a>, <a class="el" href="ie__kernels__cuda_8cu_source.html#l00119">ie_decode_fp8_e5m2_u8()</a>, and <a class="el" href="ie__quant__act_8h_source.html#l00041">IE_FP8_E4M3</a>.</p>

<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00433">ie_cuda_launch_gemv_rowwise_qfp8_f32()</a>.</p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu_a74040aacee610aa771ee11bb0bc74022_cgraph.svg" width="440" height="91"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu_a74040aacee610aa771ee11bb0bc74022_icgraph.svg" width="423" height="56"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>

</div>
</div>
<a id="abb0d57e76ef0b1dea1ae1b8bb55e1c02" name="abb0d57e76ef0b1dea1ae1b8bb55e1c02"></a>
<h2 class="memtitle"><span class="permalink"><a href="#abb0d57e76ef0b1dea1ae1b8bb55e1c02">&#9670;&#160;</a></span>k_gemv_rowwise_qi8_f32()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">__global__ void k_gemv_rowwise_qi8_f32 </td>
          <td>(</td>
          <td class="paramtype">const float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>W</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int8_t *__restrict__</td>          <td class="paramname"><span class="paramname"><em>xq</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>y</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>rows</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>cols</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>ldW</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>scale</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>zp</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>alpha</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>beta</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Row-wise GEMV with INT8 activations (per-tensor), fused dequantization. </p>
<p>Dequantization model: real = scale * (q - zero_point).</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">W</td><td>Float weights on device (row-major), size rows x ldW. </td></tr>
    <tr><td class="paramname">xq</td><td>INT8 activations on device, length cols. </td></tr>
    <tr><td class="paramname">y</td><td>Output on device, length rows. </td></tr>
    <tr><td class="paramname">rows</td><td>Rows. </td></tr>
    <tr><td class="paramname">cols</td><td>Cols. </td></tr>
    <tr><td class="paramname">ldW</td><td>Leading dimension (&gt;= cols). </td></tr>
    <tr><td class="paramname">scale</td><td>Per-tensor scale. </td></tr>
    <tr><td class="paramname">zp</td><td>Per-tensor zero-point (int). </td></tr>
    <tr><td class="paramname">alpha</td><td>Scale for W*x. </td></tr>
    <tr><td class="paramname">beta</td><td>Scale for existing y. </td></tr>
  </table>
  </dd>
</dl>

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00235">235</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  239</span>                                                                {</div>
<div class="line"><span class="lineno">  240</span>  <span class="keywordtype">int</span> r = blockIdx.x;</div>
<div class="line"><span class="lineno">  241</span>  <span class="keywordflow">if</span> (r &gt;= rows) <span class="keywordflow">return</span>;</div>
<div class="line"><span class="lineno">  242</span> </div>
<div class="line"><span class="lineno">  243</span>  <span class="keywordtype">float</span> acc = 0.f;</div>
<div class="line"><span class="lineno">  244</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> k = threadIdx.x; k &lt; cols; k += blockDim.x) {</div>
<div class="line"><span class="lineno">  245</span>    <span class="keyword">const</span> <span class="keywordtype">float</span> xv = scale * ((int)xq[k] - zp);</div>
<div class="line"><span class="lineno">  246</span>    acc += W[(size_t)r * (<span class="keywordtype">size_t</span>)ldW + (size_t)k] * xv;</div>
<div class="line"><span class="lineno">  247</span>  }</div>
<div class="line"><span class="lineno">  248</span> </div>
<div class="line"><span class="lineno">  249</span>  __shared__ <span class="keywordtype">float</span> buf[256];</div>
<div class="line"><span class="lineno">  250</span>  buf[threadIdx.x] = acc;</div>
<div class="line"><span class="lineno">  251</span>  __syncthreads();</div>
<div class="line"><span class="lineno">  252</span> </div>
<div class="line"><span class="lineno">  253</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> s = blockDim.x &gt;&gt; 1; s &gt; 0; s &gt;&gt;= 1) {</div>
<div class="line"><span class="lineno">  254</span>    <span class="keywordflow">if</span> (threadIdx.x &lt; s) buf[threadIdx.x] += buf[threadIdx.x + s];</div>
<div class="line"><span class="lineno">  255</span>    __syncthreads();</div>
<div class="line"><span class="lineno">  256</span>  }</div>
<div class="line"><span class="lineno">  257</span> </div>
<div class="line"><span class="lineno">  258</span>  <span class="keywordflow">if</span> (threadIdx.x == 0) {</div>
<div class="line"><span class="lineno">  259</span>    <span class="keywordtype">float</span> out = alpha * buf[0];</div>
<div class="line"><span class="lineno">  260</span>    <span class="keywordflow">if</span> (beta != 0.f) out += beta * y[r];</div>
<div class="line"><span class="lineno">  261</span>    y[r] = out;</div>
<div class="line"><span class="lineno">  262</span>  }</div>
<div class="line"><span class="lineno">  263</span>}</div>
</div><!-- fragment -->
<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00410">ie_cuda_launch_gemv_rowwise_qi8_f32()</a>.</p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu_abb0d57e76ef0b1dea1ae1b8bb55e1c02_icgraph.svg" width="414" height="56"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>

</div>
</div>
<a id="ac440d51163bc9f16e835d00aedcb4b79" name="ac440d51163bc9f16e835d00aedcb4b79"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac440d51163bc9f16e835d00aedcb4b79">&#9670;&#160;</a></span>k_pack_w_blockedk_f32()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">__global__ void k_pack_w_blockedk_f32 </td>
          <td>(</td>
          <td class="paramtype">float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>Wp</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>W</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>rows</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>cols</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>ldW</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>block_k</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Pack row-major W into Blocked-K layout on device (see header docs). </p>
<p>Layout: for each (row r, column k) compute block index kb = k / block_k and offset ko = k % block_k, and store at: dst = ((kb * rows + r) * block_k + ko).</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">Wp</td><td>Destination (blocked) on device. </td></tr>
    <tr><td class="paramname">W</td><td>Source row-major on device (rows x ldW). </td></tr>
    <tr><td class="paramname">rows</td><td>Number of rows. </td></tr>
    <tr><td class="paramname">cols</td><td>Number of columns. </td></tr>
    <tr><td class="paramname">ldW</td><td>Leading dimension (&gt;= cols). </td></tr>
    <tr><td class="paramname">block_k</td><td>Block size in K (&gt;= 1). </td></tr>
  </table>
  </dd>
</dl>

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00340">340</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  342</span>                                                                      {</div>
<div class="line"><span class="lineno">  343</span>  <span class="keywordtype">int</span> r0 = blockIdx.y * blockDim.y + threadIdx.y;</div>
<div class="line"><span class="lineno">  344</span>  <span class="keywordtype">int</span> k0 = blockIdx.x * blockDim.x + threadIdx.x;</div>
<div class="line"><span class="lineno">  345</span> </div>
<div class="line"><span class="lineno">  346</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> r = r0; r &lt; rows; r += blockDim.y * gridDim.y) {</div>
<div class="line"><span class="lineno">  347</span>    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> k = k0; k &lt; cols; k += blockDim.x * gridDim.x) {</div>
<div class="line"><span class="lineno">  348</span>      <span class="keywordtype">int</span> kb = k / block_k;</div>
<div class="line"><span class="lineno">  349</span>      <span class="keywordtype">int</span> ko = k % block_k;</div>
<div class="line"><span class="lineno">  350</span>      <span class="keywordtype">size_t</span> dst = ((size_t)kb * (size_t)rows + (size_t)r) * (<span class="keywordtype">size_t</span>)block_k +</div>
<div class="line"><span class="lineno">  351</span>                   (size_t)ko;</div>
<div class="line"><span class="lineno">  352</span>      Wp[dst] = W[(size_t)r * (<span class="keywordtype">size_t</span>)ldW + (size_t)k];</div>
<div class="line"><span class="lineno">  353</span>    }</div>
<div class="line"><span class="lineno">  354</span>  }</div>
<div class="line"><span class="lineno">  355</span>}</div>
</div><!-- fragment -->
<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00478">ie_cuda_launch_pack_w_blockedk_f32()</a>.</p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu_ac440d51163bc9f16e835d00aedcb4b79_icgraph.svg" width="396" height="56"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>

</div>
</div>
<a id="aea063f21d4bdf3ba4108f9677476aad4" name="aea063f21d4bdf3ba4108f9677476aad4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aea063f21d4bdf3ba4108f9677476aad4">&#9670;&#160;</a></span>k_vec_tanh_f32()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">__global__ void k_vec_tanh_f32 </td>
          <td>(</td>
          <td class="paramtype">float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>y</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float *__restrict__</td>          <td class="paramname"><span class="paramname"><em>x</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>n</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Elementwise hyperbolic tangent on a vector. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">y</td><td>Output vector on device (length n). </td></tr>
    <tr><td class="paramname">x</td><td>Input vector on device (length n). </td></tr>
    <tr><td class="paramname">n</td><td>Number of elements. </td></tr>
  </table>
  </dd>
</dl>

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00318">318</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">  319</span>                                                                   {</div>
<div class="line"><span class="lineno">  320</span>  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = blockIdx.x * blockDim.x + threadIdx.x; i &lt; n;</div>
<div class="line"><span class="lineno">  321</span>       i += blockDim.x * gridDim.x) {</div>
<div class="line"><span class="lineno">  322</span>    y[i] = tanhf(x[i]);</div>
<div class="line"><span class="lineno">  323</span>  }</div>
<div class="line"><span class="lineno">  324</span>}</div>
</div><!-- fragment -->
<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00458">ie_cuda_launch_vec_tanh_f32()</a>.</p>
<div class="dynheader">
Here is the caller graph for this function:</div>
<div class="dyncontent">
<div class="center"><iframe scrolling="no" frameborder="0" src="ie__kernels__cuda_8cu_aea063f21d4bdf3ba4108f9677476aad4_icgraph.svg" width="334" height="56"><p><b>This browser is not able to show SVG: try Firefox, Chrome, Safari, or Opera instead.</b></p></iframe></div>
</div>

</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a id="a5ab38bcd2467e982c68d46e66acfa9ef" name="a5ab38bcd2467e982c68d46e66acfa9ef"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5ab38bcd2467e982c68d46e66acfa9ef">&#9670;&#160;</a></span>g_ie_cuda_err</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">__thread char g_ie_cuda_err[256] = {0}</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel static">static</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p class="definition">Definition at line <a class="el" href="ie__kernels__cuda_8cu_source.html#l00037">37</a> of file <a class="el" href="ie__kernels__cuda_8cu_source.html">ie_kernels_cuda.cu</a>.</p>
<div class="fragment"><div class="line"><span class="lineno">   37</span>{0};</div>
</div><!-- fragment -->
<p class="reference">Referenced by <a class="el" href="ie__kernels__cuda_8cu_source.html#l00057">ie_cuda_last_error_string()</a>, and <a class="el" href="ie__kernels__cuda_8cu_source.html#l00043">ie_cuda_set_err()</a>.</p>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="dir_996f45160da62e1a3d7f6046fad68f51.html">engine</a></li><li class="navelem"><a class="el" href="dir_3d9126aa00c041bc0b8f859d1965a0f4.html">src</a></li><li class="navelem"><a class="el" href="dir_2151caada9eddb57703a29b50b7782f8.html">kernels</a></li><li class="navelem"><a class="el" href="ie__kernels__cuda_8cu.html">ie_kernels_cuda.cu</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
