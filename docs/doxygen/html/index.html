<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Inference Engine (Clocher): Clocher — CPU LLM Inference Baseline (C11 core + Python stdlib harness)</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Inference Engine (Clocher)<span id="projectnumber">&#160;0.1</span>
   </div>
   <div id="projectbrief">C11 CPU inference baseline with strict metrics &amp; harness</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search',true);
  $(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('index.html',''); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Clocher — CPU LLM Inference Baseline (C11 core + Python stdlib harness) </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_README"></a></p>
<p>C11, zero-dependency <b>CPU inference baseline</b> with a minimal CLI and a stdlib-only Python harness for <b>reproducible throughput (TPS)</b> and <b>per-token latency</b>. The project emphasizes portability, determinism, rigorous tests, and turnkey automation (Makefile + CI + Doxygen).</p>
<hr  />
<h1><a class="anchor" id="autotoc_md6"></a>
Features</h1>
<ul>
<li><b>C11 core</b>, no external runtime deps (<code>-lpthread -lm</code> only)</li>
<li><b>Deterministic generator</b> (repeatable tokens for equal prompts)</li>
<li><b>Per-token latency ring</b> with p50/p95 in CLI JSON</li>
<li><b>Vectorization</b>: AVX2/FMA GEMV path (runtime-gated), light prefetch</li>
<li><b>Fast activations</b>: clamped <code>tanh</code> approx + vector helpers</li>
<li><b>Precision plumbing</b>: optional BF16/FP16 round-trip (FP32 accumulation)</li>
<li><b>INT8 PTQ pipeline</b>: min-max scales (per-tensor/per-row), calibration script with accuracy checks</li>
<li><b>Threading</b>: fixed thread pool, contiguous sharding, grainsize control</li>
<li><b>Affinity (Linux)</b>: opt-in CPU pinning via <code>IE_TP_USE_AFFINITY=1</code></li>
<li><b>Layout</b>: blocked-K packing + optional on-disk cache</li>
<li><b>Microbenchmarks</b> and <b>perf → flamegraph</b> pipeline</li>
<li><b>Unit tests</b> (C + Python), <b>Doxygen</b> comments everywhere, <b>Makefile</b> automation</li>
</ul>
<hr  />
<h1><a class="anchor" id="autotoc_md8"></a>
Repository layout</h1>
<div class="fragment"><div class="line">engine/</div>
<div class="line">  include/        # Public headers (Doxygen)</div>
<div class="line">  src/            # C sources: core, kernels, math, io, opt, quant</div>
<div class="line">benchmarks/</div>
<div class="line">  harness.py      # Python stdlib harness (CSV/JSON reports)</div>
<div class="line">  ptq_calib.py    # INT8 PTQ calibration/validation</div>
<div class="line">  reports/        # Generated results (timestamped)</div>
<div class="line">  src/            # Microbench sources</div>
<div class="line">docs/</div>
<div class="line">  Doxyfile        # Doxygen config (outputs to docs/doxygen/)</div>
<div class="line">scripts/</div>
<div class="line">  run_benchmark.sh</div>
<div class="line">  profile_flamegraph.sh</div>
<div class="line">  set_numa.sh     # NUMA policy helper (no libnuma link required)</div>
<div class="line">  make_baseline_md.py</div>
<div class="line">  update_performance_md.py</div>
<div class="line">tests/</div>
<div class="line">  c/              # C unit tests</div>
<div class="line">  python/         # Python tests (unittest)</div>
<div class="line">Makefile</div>
</div><!-- fragment --><hr  />
<h1><a class="anchor" id="autotoc_md10"></a>
Requirements</h1>
<ul>
<li><b>Build toolchain</b>: GCC/Clang with C11</li>
<li><b>Linux/macOS</b> (Windows via MSYS/WSL)</li>
<li><b>perf</b> (Linux) for flamegraphs (optional)</li>
<li><b>Python 3.8+</b> (stdlib only) for harness &amp; scripts</li>
<li><b>Doxygen</b> for HTML docs (optional)</li>
</ul>
<hr  />
<h1><a class="anchor" id="autotoc_md12"></a>
Quickstart</h1>
<div class="fragment"><div class="line">make build     # build the CLI binary</div>
<div class="line">make test      # run C + Python tests</div>
<div class="line">make bench     # run harness → CSV/JSON under benchmarks/reports/&lt;UTC_TS&gt;/</div>
</div><!-- fragment --><hr  />
<h1><a class="anchor" id="autotoc_md14"></a>
CLI usage</h1>
<div class="fragment"><div class="line">./build/inference-engine [--prompt TEXT] [--max-new N]</div>
<div class="line">                         [--threads N]</div>
<div class="line">                         [--precision fp32|bf16|fp16]</div>
<div class="line">                         [--affinity auto|compact|scatter]</div>
<div class="line">                         [--pretranspose none|woh|wxh|all]</div>
<div class="line">                         [--grainsize K]</div>
<div class="line">                         [--help]</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md15"></a>
Example</h2>
<div class="fragment"><div class="line">./build/inference-engine --prompt &quot;hello&quot; --max-new 8 --threads 4 --precision fp32</div>
</div><!-- fragment --><p><b>Output (JSON):</b> </p><div class="fragment"><div class="line">{</div>
<div class="line">  &quot;tokens_generated&quot;: 8,</div>
<div class="line">  &quot;tokens&quot;: [1705,1657,1537,1183,1704,1315,1917,1162],</div>
<div class="line">  &quot;wall_time_s&quot;: 0.001209,</div>
<div class="line">  &quot;tps_true&quot;: 7639.897996,</div>
<div class="line">  &quot;latency_p50_ms&quot;: 0.131,</div>
<div class="line">  &quot;latency_p95_ms&quot;: 0.143,</div>
<div class="line">  &quot;rss_peak_mb&quot;: 0,</div>
<div class="line">  &quot;kv_hits&quot;: 0,</div>
<div class="line">  &quot;kv_misses&quot;: 8</div>
<div class="line">}</div>
</div><!-- fragment --><hr  />
<h1><a class="anchor" id="autotoc_md17"></a>
Notable flags</h1>
<ul>
<li><code>--threads N</code> — number of worker threads.</li>
<li><code>--affinity {auto,compact,scatter}</code> — policy name; <b>effective on Linux</b> only and only if <code>IE_TP_USE_AFFINITY=1</code>.</li>
<li><code>--grainsize K</code> — chunk size for contiguous partitioning (used by parallel-for).</li>
<li><code>--precision {fp32,bf16,fp16}</code> — optional round-trip to BF16/FP16 with <b>FP32 accumulation</b>.</li>
<li><code>--pretranspose {none,woh,wxh,all}</code> — pretranspose/pack weights and optionally cache to disk; <code>all</code> applies to both projections if present.</li>
</ul>
<p><b>Linux CPU pinning toggle</b> </p><div class="fragment"><div class="line">IE_TP_USE_AFFINITY=1 ./build/inference-engine --prompt &quot;x&quot; --max-new 8 --threads 4 --affinity scatter</div>
</div><!-- fragment --><hr  />
<h1><a class="anchor" id="autotoc_md19"></a>
Benchmarks &amp; baseline</h1>
<p>Run the harness:</p>
<div class="fragment"><div class="line">make bench</div>
<div class="line"># Reports: benchmarks/reports/&lt;UTC_TS&gt;/{samples.csv,summary.json}</div>
</div><!-- fragment --><p>Create a baseline markdown from the latest report:</p>
<div class="fragment"><div class="line">make baseline-report</div>
<div class="line"># Generates: benchmarks/reports/&lt;UTC_TS&gt;/BASELINE.md</div>
</div><!-- fragment --><hr  />
<h1><a class="anchor" id="autotoc_md21"></a>
Profiling &amp; flamegraphs (Linux)</h1>
<div class="fragment"><div class="line">make profile</div>
<div class="line"># → perf.data, flamegraph.svg, and PERFORMANCE.md updated (English)</div>
</div><!-- fragment --><p>Re-emit/update performance notes:</p>
<div class="fragment"><div class="line">python3 scripts/update_performance_md.py</div>
</div><!-- fragment --><hr  />
<h1><a class="anchor" id="autotoc_md23"></a>
NUMA policies (Linux, no libnuma link)</h1>
<p>Set a process-wide policy, then launch the engine:</p>
<div class="fragment"><div class="line">scripts/set_numa.sh interleave -- ./build/inference-engine --prompt &quot;x&quot; --max-new 8</div>
<div class="line">scripts/set_numa.sh node:0     -- ./build/inference-engine --prompt &quot;x&quot; --max-new 8</div>
<div class="line">scripts/set_numa.sh strict     -- ./build/inference-engine --prompt &quot;x&quot; --max-new 8</div>
</div><!-- fragment --><hr  />
<h1><a class="anchor" id="autotoc_md25"></a>
INT8 PTQ pipeline</h1>
<p>Per-tensor or per-row min-max scaling with accuracy checks.</p>
<p><b>From an existing FP32 weight matrix (.bin, row-major):</b> </p><div class="fragment"><div class="line">python3 benchmarks/ptq_calib.py   --weights bin/W.bin --rows 768 --cols 768   --mode per_row --out-prefix out/W_int8 --accuracy-threshold 0.995</div>
</div><!-- fragment --><p><b>From Hugging Face (state_dict key):</b> </p><div class="fragment"><div class="line">make ptq-from-hf HF_MODEL=facebook/opt-125m   KEY=model.decoder.layers.0.self_attn.q_proj.weight   OUT_PREFIX=out/qproj_int8</div>
</div><!-- fragment --><p>Artifacts: <code>&lt;prefix&gt;.int8.bin</code>, <code>&lt;prefix&gt;.scales.bin</code>, <code>&lt;prefix&gt;.report.json</code>.</p>
<hr  />
<h1><a class="anchor" id="autotoc_md27"></a>
Testing</h1>
<div class="fragment"><div class="line">make test</div>
</div><!-- fragment --><ul>
<li><b>C tests</b>: tensors, kernels (generic + AVX2), thread-pool, math, tokenizer, weights, <b>int8_ptq</b>.</li>
<li><b>Python tests</b>: CLI, harness, metrics ring (p50/p95), determinism, <b>PTQ calibration pipeline</b>.</li>
</ul>
<hr  />
<h1><a class="anchor" id="autotoc_md29"></a>
Documentation (Doxygen)</h1>
<div class="fragment"><div class="line">make docs-doxygen</div>
<div class="line"># HTML at: docs/doxygen/html/index.html</div>
</div><!-- fragment --><p>All C functions (public and internal TUs) are documented with Doxygen-style comments.</p>
<hr  />
<h1><a class="anchor" id="autotoc_md31"></a>
Coding standards</h1>
<ul>
<li><b>C</b>: C11, warnings as errors (<code>-Wall -Wextra -Werror -pedantic</code>)</li>
<li><b>Formatting</b>: <code>make fmt</code> (uses <code>clang-format</code> if available)</li>
<li><b>Linting</b>: <code>make lint</code> (uses <code>clang-tidy</code> if available)</li>
<li><b>Commits</b>: semantic commit messages</li>
<li><b>Makefile</b>: single entry point for build, test, bench, profile, docs</li>
</ul>
<hr  />
<h1><a class="anchor" id="autotoc_md33"></a>
Roadmap</h1>
<ul>
<li><b>Step 6</b>: I/O &amp; batching — async prefetch, pinned workers, microbatch sizing, warmup policy.</li>
<li>Extras: GPU/iGPU variant (CUDA/oneAPI), INT4 exploration, Prometheus/Grafana dashboard.</li>
</ul>
<hr  />
 <h1><a class="anchor" id="autotoc_md35"></a>
Updates — 2025-10-17</h1>
<ul>
<li><b>CLI flags added:</b> <code>--prompts-file PATH</code>, <code>--batch N</code>, <code>--prefetch on|off|auto</code>, <code>--warmup N</code>.</li>
<li><b>Examples:</b><ul>
<li><code>inference-engine --prompts-file benchmarks/prompts_10.txt --batch 32 --max-new 8 --prefetch on --warmup 8</code></li>
<li><code>inference-engine --prompt "hello" --max-new 16 --threads 4 --precision fp32</code></li>
</ul>
</li>
<li><b>Stable JSON output:</b> ensure exact field formatting used by the test harness (e.g., <code>"tokens_generated": 0</code> with a space after the colon).</li>
<li><b>Batcher integration:</b> asynchronous prefetch + tokenization with order guarantees; micro-batch views from a ring buffer.</li>
<li><b>Bench &amp; perf:</b> <code>make bench</code> uses prompts file and batching; <code>make perf-report</code> autodetects FlameGraph tools or accepts <code>STACKCOLLAPSE</code>/<code>FLAMEGRAPH</code> env overrides. </li>
</ul>
</div></div><!-- PageDoc -->
<a href="doxygen_crawl.html"></a>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
