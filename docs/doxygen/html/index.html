<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Inference Engine (Clocher): Inference Engine (Clocher)</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Inference Engine (Clocher)
   &#160;<span id="projectnumber">0.2</span>
   </div>
   <div id="projectbrief">C11 CPU/GPU inference baseline with strict metrics &amp; harness</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('index.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Inference Engine (Clocher) </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_README"></a> <em>A minimal C11 inference baseline with strict metrics &amp; a Python harness.</em> <br  />
 <b>Last updated:</b> 2025-10-24 21:00:48 UTC</p>
<hr  />
<h1><a class="anchor" id="autotoc_md96"></a>
Quick start</h1>
<div class="fragment"><div class="line"># Build CPU and (optionally) CUDA binaries</div>
<div class="line">make build</div>
<div class="line">make build-cuda   # requires a CUDA toolchain; produces build/inference-engine.cuda</div>
<div class="line"> </div>
<div class="line"># Sanity: show CLI</div>
<div class="line">./build/inference-engine --help</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md97"></a>
Model format (IEBIN v1)</h2>
<p>The engine consumes a pair of files in the current working directory:</p>
<div class="fragment"><div class="line">model.ie.json   # metadata (tensor names, shapes, dtypes, scales)</div>
<div class="line">model.ie.bin    # raw, mmap‑friendly tensor blob</div>
</div><!-- fragment --><p>You can pack from a Hugging Face checkpoint using the helper script below.</p>
<hr  />
<h1><a class="anchor" id="autotoc_md99"></a>
NEW: INT4 &lt;strong&gt;weight‑only&lt;/strong&gt; PTQ path (Q4)</h1>
<p>This repository now includes a <b>4‑bit (INT4) weight‑only</b> path that trades a small accuracy budget for <b>4× smaller weights</b> and a substantial <b>memory‑bandwidth reduction</b>. It is designed to be drop‑in with the existing build and harness.</p>
<h2><a class="anchor" id="autotoc_md100"></a>
Pipeline overview</h2>
<p>1) <b>Prepare/locate a HF model</b> (already sharded is fine):</p>
<div class="fragment"><div class="line">models/gpt-oss-20b/hf/</div>
<div class="line">  ├── config.json</div>
<div class="line">  ├── tokenizer.json / vocab.json / merges.txt</div>
<div class="line">  └── pytorch_model-00001-of-00046.bin … pytorch_model-00046-of-00046.bin</div>
</div><!-- fragment --><p>2) **(Calibrate) produce the INT4 manifest** <br  />
 Use one of the PTQ helper scripts to create a manifest describing which tensors are quantized to 4‑bit and their per‑row/per‑tensor scales:</p>
<div class="fragment"><div class="line"># Choose one depending on your source of truth and available samples.</div>
<div class="line"># See: python3 scripts/ptq_from_hf.py --help</div>
<div class="line">#      python3 scripts/ptq_from_bin.py --help</div>
<div class="line">#      python3 scripts/ptq_from_source.py --help</div>
<div class="line"> </div>
<div class="line">python3 scripts/ptq_from_hf.py   --hf-dir models/gpt-oss-20b/hf   --out quant/q4_manifest.json   --bits 4 --weights-only 1</div>
</div><!-- fragment --><p>Notes:</p><ul>
<li>The manifest (JSON) maps tensor names to <b>INT4 packing + scale params</b>.</li>
<li>Calib options (group size, symmetric/affine, clamp, percentile) are exposed by the script; pick values that satisfy your accuracy budget.</li>
</ul>
<p>3) <b>Pack Hugging Face → IEBIN (with INT4)</b></p>
<div class="fragment"><div class="line">python3 scripts/hf_to_iebin.py   --hf-dir models/gpt-oss-20b/hf   --out-dir models/gpt-oss-20b   --q4-map quant/q4_manifest.json</div>
</div><!-- fragment --><p>This writes <code>model.ie.json</code> and <code>model.ie.bin</code> in <code>models/gpt-oss-20b/</code> (or as configured).</p>
<p>4) <b>Run the benchmark (CPU or CUDA) with INT4 weights</b></p>
<p>The engine selects kernels by <b>precision</b>. For INT4 <b>weight‑only</b>, use <code>int4w</code>.</p>
<blockquote class="doxtable">
<p><b>64 MB per token</b> = <code>IE_BYTES_PER_TOKEN=64000000</code> (decimal bytes). </p>
</blockquote>
<p><b>CPU:</b></p>
<div class="fragment"><div class="line">cd models/gpt-oss-20b</div>
<div class="line">PROMPTS=../../benchmarks/prompts_10..txt IE_PRECISION=int4w IE_REQUIRE_MODEL=1 IE_BYTES_PER_TOKEN=64000000 IE_STRIDE_BYTES=256 RUNS=3 make -C ../.. bench</div>
</div><!-- fragment --><p><b>CUDA:</b></p>
<div class="fragment"><div class="line">cd models/gpt-oss-20b</div>
<div class="line">PROMPTS=../../benchmarks/prompts_10..txt IE_PRECISION=int4w IE_REQUIRE_MODEL=1 IE_BYTES_PER_TOKEN=64000000 IE_STRIDE_BYTES=256 RUNS=3 make -C ../.. bench-cuda</div>
</div><!-- fragment --><p>The CLI will emit a <b>single JSON line</b> per run, which the harness merges into <code><a class="el" href="PERFORMANCE_8md.html">docs/PERFORMANCE.md</a></code>.</p>
<hr  />
<h1><a class="anchor" id="autotoc_md102"></a>
Operational notes</h1>
<ul>
<li>The <code>--device</code> flag is accepted by the CLI as a <b>no‑op hint</b> (selection is a build‑time concern). CPU: <code>build/inference-engine</code>, CUDA: <code>build/inference-engine.cuda</code>.</li>
<li><b>Strict mode</b>: set <code>IE_REQUIRE_MODEL=1</code> to require a valid <code>model.ie.json</code> + <code>model.ie.bin</code>. Otherwise the engine can operate in a deterministic <b>stub</b> mode for CI.</li>
<li><b>Work‑touch</b>: to emulate memory pressure, set <code>IE_BYTES_PER_TOKEN</code> (e.g. <code>64000000</code>) and <code>IE_STRIDE_BYTES</code> (e.g. <code>256</code>). The measured window includes generation <b>and</b> this per‑token touch over the mmap’d <code>model.ie.bin</code>.</li>
<li><b>RSS reporting</b>: the CLI samples peak RSS after the timed window (Linux: <code>/proc/self/status</code> <code>VmHWM</code>, fallback <code>getrusage</code>).</li>
</ul>
<hr  />
<h1><a class="anchor" id="autotoc_md104"></a>
Troubleshooting</h1>
<ul>
<li><code>ERROR: --q4-map not found</code>: pass a real path to your manifest, e.g. <code>quant/q4_manifest.json</code>.</li>
<li>&lsquo;error: unknown flag &rsquo;&ndash;model-dir'<code>or</code>'&ndash;rounds'<code>: you are using an older binary; rebuild with</code>make clean &amp;&amp; make build<code>. -</code>RSS peak = 0<code>: ensure the process reads enough memory while resident (set</code>IE_BYTES_PER_TOKEN<code>to a non‑zero value) and you are not running inside a constrained container namespace that suppresses</code>VmHWM<code>.</code></li>
<li><code>Prompts file not found:</code>PROMPTS=benchmarks/prompts_10..txt` (note the <b>double dot</b> in the demo file).</li>
</ul>
<hr  />
<h1><a class="anchor" id="autotoc_md106"></a>
What INT4 (weight‑only) means here</h1>
<ul>
<li><b>Activations</b> remain floating‑point; only <b>weights</b> are nibble‑packed (2 weights per byte) with per‑row scales.</li>
<li>Dequantization is fused in the matmul kernels (see <code><a class="el" href="int4__ptq_8c.html" title="Implementation of 4-bit (INT4) weight-only quantization utilities.">engine/src/quant/int4_ptq.c</a></code> and GPU equivalents). Pretranspose/blocked‑K packing are applied before/after quantization as configured.</li>
<li>Accuracy is safeguarded by a <b>calibration gate</b> (cosine similarity or task‑level eval), configurable in the PTQ scripts.</li>
</ul>
<hr  />
<h1><a class="anchor" id="autotoc_md108"></a>
See also</h1>
<ul>
<li>Detailed design and optimization path: <code><a class="el" href="DESIGN_8md.html">docs/DESIGN.md</a></code>, <code>docs/adr-00060-optimization-path.md</code></li>
<li>Decision log: <code><a class="el" href="DECISIONS_8md.html">docs/DECISIONS.md</a></code></li>
</ul>
<hr  />
 <h1><a class="anchor" id="autotoc_md110"></a>
INT4 Weight-Only — Addendum (2025-10-24 21:04:23 UTC)</h1>
<p>This addendum <b>adds</b> a new optional path to the existing pipeline for <b>INT4 (weight-only)</b> packing and benchmarking. It does <b>not</b> replace the FP32/BF16/FP16 flows.</p>
<h2><a class="anchor" id="autotoc_md111"></a>
Summary</h2>
<ul>
<li>Quantization: post-training <em>weight-only</em> INT4 (aka <code>int4w</code>) via a manifest.</li>
<li>Export: <code><a class="el" href="hf__to__iebin_8py.html">scripts/hf_to_iebin.py</a></code> supports <code>--q4-map</code> to pack tensors into <code>model.ie.bin</code> while preserving <code>model.ie.json</code> meta.</li>
<li>Runtime: select precision with <code>IE_PRECISION=int4w</code> (or <code>--precision int4w</code>), independent from host math precision.</li>
<li>Benchmarks: <code>make bench</code> / <code>make bench-cuda</code> support strict runs with realistic memory pressure via <code>IE_BYTES_PER_TOKEN</code>.</li>
</ul>
<h2><a class="anchor" id="autotoc_md112"></a>
Prerequisites</h2>
<ul>
<li>A working HF model directory (already used in your FP32 flow), e.g. <code>models/gpt-oss-20b/hf</code>.</li>
<li>A quantization manifest (example at <code>quant/q4_manifest.json</code>). See <b>Manifest template</b> below.</li>
<li>Python deps already used by <code><a class="el" href="hf__to__iebin_8py.html">scripts/hf_to_iebin.py</a></code> (torch, numpy).</li>
</ul>
<h2><a class="anchor" id="autotoc_md113"></a>
Export to IEBIN with INT4</h2>
<div class="fragment"><div class="line">python3 scripts/hf_to_iebin.py   --hf-dir models/gpt-oss-20b/hf   --out-dir models/gpt-oss-20b   --q4-map quant/q4_manifest.json</div>
</div><!-- fragment --><p> This produces (or updates) <code>models/gpt-oss-20b/model.ie.json</code> and <code>models/gpt-oss-20b/model.ie.bin</code> with weight-only INT4 packing per manifest.</p>
<blockquote class="doxtable">
<p>Tip: if you see <code>ERROR: --q4-map not found</code>, double‑check the path to your manifest. </p>
</blockquote>
<h2><a class="anchor" id="autotoc_md114"></a>
Run the strict benchmark (CPU)</h2>
<div class="fragment"><div class="line">PROMPTS=benchmarks/prompts_10..txt IE_PRECISION=int4w IE_REQUIRE_MODEL=1 IE_BYTES_PER_TOKEN=64000000 IE_STRIDE_BYTES=256 RUNS=3 make bench</div>
</div><!-- fragment --><ul>
<li><code>IE_REQUIRE_MODEL=1</code> makes the CLI fail if <code>model.ie.{json,bin}</code> are missing.</li>
<li><code>IE_BYTES_PER_TOKEN</code> enables the per-token <em>work-touch</em> loop over <code>model.ie.bin</code> to mimic bandwidth pressure.</li>
<li><code>IE_STRIDE_BYTES</code> controls the touch stride (256 is a good default).</li>
</ul>
<h2><a class="anchor" id="autotoc_md115"></a>
Run the strict benchmark (CUDA)</h2>
<div class="fragment"><div class="line">PROMPTS=benchmarks/prompts_10..txt IE_PRECISION=int4w IE_REQUIRE_MODEL=1 IE_BYTES_PER_TOKEN=64000000 IE_STRIDE_BYTES=256 RUNS=3 make bench-cuda</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md116"></a>
Manifest template (example)</h2>
<p>Save as <code>quant/q4_manifest.json</code>: </p><div class="fragment"><div class="line">{</div>
<div class="line">  &quot;version&quot;: 1,</div>
<div class="line">  &quot;rules&quot;: [</div>
<div class="line">    {</div>
<div class="line">      &quot;pattern&quot;: &quot;.*attn.*(q_proj|k_proj|v_proj|o_proj).*&quot;,</div>
<div class="line">      &quot;dtype&quot;:   &quot;int4w&quot;,</div>
<div class="line">      &quot;group&quot;:    64,</div>
<div class="line">      &quot;zero&quot;:     &quot;per-tensor&quot;,</div>
<div class="line">      &quot;scale&quot;:    &quot;per-channel&quot;</div>
<div class="line">    },</div>
<div class="line">    {</div>
<div class="line">      &quot;pattern&quot;: &quot;.*mlp.*(gate_proj|up_proj|down_proj).*&quot;,</div>
<div class="line">      &quot;dtype&quot;:   &quot;int4w&quot;,</div>
<div class="line">      &quot;group&quot;:    64,</div>
<div class="line">      &quot;zero&quot;:     &quot;per-tensor&quot;,</div>
<div class="line">      &quot;scale&quot;:    &quot;per-channel&quot;</div>
<div class="line">    }</div>
<div class="line">  ]</div>
<div class="line">}</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md117"></a>
Troubleshooting</h2>
<ul>
<li><b>Unknown flag errors</b>: The CLI now accepts <code>--device</code>, <code>--model-dir</code>, <code>--model-json</code>, <code>--model-bin</code>, and <code>--rounds</code> as documented. If your harness still injects older flags, update it, or run the engine directly.</li>
<li><b>RSS peak shows 0 MB</b>: Ensure you are on Linux and that the engine is built with the updated <code><a class="el" href="ie__metrics_8h.html#a0b083c9fbf90d02c9ef387ab34220a34" title="Best-effort sampling of peak RSS (resident set size).">ie_metrics_sample_rss_peak()</a></code> (reads <code>/proc/self/status</code> → <code>VmHWM</code>, then falls back to <code>getrusage</code>). The JSON is captured <em>after</em> the measured window; the value will be <code>0</code> only if the OS reports <code>0</code> or if the sampling failed.</li>
</ul>
<h2><a class="anchor" id="autotoc_md118"></a>
Notes</h2>
<ul>
<li>The <b>precision label</b> (<code>IE_PRECISION</code>) is a <em>storage / weight</em> hint, not necessarily the CPU math precision. You can still set <code>PRECISION=fp32</code> for host compute while using <code>IE_PRECISION=int4w</code> for storage.</li>
<li>Deterministic stub mode remains available when <code>IE_REQUIRE_MODEL</code> is not set; strict runs require a valid IEBIN pair.</li>
</ul>
<h1><a class="anchor" id="autotoc_md119"></a>
Repository Layout</h1>
<div class="fragment"><div class="line">.</div>
<div class="line">├── benchmarks</div>
<div class="line">│   ├── harness.py</div>
<div class="line">│   ├── prompts_10..txt</div>
<div class="line">│   ├── prompts.jsonl</div>
<div class="line">│   ├── ptq_calib.py</div>
<div class="line">│   ├── reports/</div>
<div class="line">│   └── src/</div>
<div class="line">│       └── microbench_gemv.c</div>
<div class="line">├── configs</div>
<div class="line">│   ├── bench.toml</div>
<div class="line">│   └── engine.toml</div>
<div class="line">├── docs</div>
<div class="line">│   ├── Doxyfile</div>
<div class="line">│   ├── DECISIONS.md</div>
<div class="line">│   ├── DESIGN.md</div>
<div class="line">│   ├── adr-00060-optimization-path.md</div>
<div class="line">│   └── doxygen/html/…</div>
<div class="line">├── engine</div>
<div class="line">│   ├── include/         # public headers</div>
<div class="line">│   └── src/             # C/CUDA sources</div>
<div class="line">│       ├── devices/</div>
<div class="line">│       ├── io/</div>
<div class="line">│       ├── kernels/</div>
<div class="line">│       ├── math/</div>
<div class="line">│       ├── opt/</div>
<div class="line">│       ├── quant/</div>
<div class="line">│       └── main_infer.c</div>
<div class="line">├── grafana</div>
<div class="line">│   └── dashboards/clocher.json</div>
<div class="line">├── models</div>
<div class="line">│   └── gpt-oss-20b</div>
<div class="line">│       ├── hf/          # original HF shards</div>
<div class="line">│       ├── model.ie.json</div>
<div class="line">│       └── model.ie.bin</div>
<div class="line">├── monitoring</div>
<div class="line">│   ├── docker-compose.yml</div>
<div class="line">│   └── prometheus.yml</div>
<div class="line">├── quant</div>
<div class="line">│   └── q4_manifest.json</div>
<div class="line">├── scripts</div>
<div class="line">│   ├── hf_to_iebin.py</div>
<div class="line">│   ├── ptq_from_hf.py</div>
<div class="line">│   ├── ptq_from_source.py</div>
<div class="line">│   ├── ptq_from_bin.py</div>
<div class="line">│   ├── true_tps_strict.sh</div>
<div class="line">│   ├── run_benchmark.sh</div>
<div class="line">│   └── make_baseline_md.py</div>
<div class="line">├── tests</div>
<div class="line">│   ├── c/</div>
<div class="line">│   └── python/</div>
<div class="line">├── Makefile</div>
<div class="line">└── README.md</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md120"></a>
Makefile — Complete Reference</h1>
<p>This project uses a single <code>Makefile</code> to build CPU/CUDA binaries and run reproducible benchmarks.</p>
<h2><a class="anchor" id="autotoc_md121"></a>
Common Targets</h2>
<ul>
<li><code>make build</code> — build <b>CPU</b> binary at <code>build/inference-engine</code>.</li>
<li><code>make build-cuda</code> — build <b>CUDA</b> binary at <code>build/inference-engine.cuda</code> (needs NVCC + CUDA toolkit).</li>
<li><code>make clean</code> — remove build artifacts and reports.</li>
<li><code>make bench</code> — run CPU benchmark harness (updates <code><a class="el" href="PERFORMANCE_8md.html">docs/PERFORMANCE.md</a></code>).</li>
<li><code>make bench-cuda</code> — run CUDA benchmark harness (updates <code><a class="el" href="PERFORMANCE_8md.html">docs/PERFORMANCE.md</a></code>).</li>
</ul>
<blockquote class="doxtable">
<p>Tip: these benchmarks call the <em>same</em> CLI under the hood (<code>build/inference-engine*</code>). </p>
</blockquote>
<h2><a class="anchor" id="autotoc_md122"></a>
Environment Variables (consumed by &lt;tt&gt;make bench*&lt;/tt&gt; and/or the CLI)</h2>
<ul>
<li><code>PROMPTS</code> : path to a prompts file (one prompt per line). Example: <code>benchmarks/prompts_10..txt</code>.</li>
<li><code>RUNS</code> : number of harness repetitions (default: 3).</li>
<li><code>IE_REQUIRE_MODEL</code> : <code>1</code> to enforce strict IEBIN loading (<code>model.ie.json</code> + <code>model.ie.bin</code>) or <code>0</code> to allow stub mode.</li>
<li><code>IE_BYTES_PER_TOKEN</code> : <b>bytes touched per generated token</b> during the work‑touch loop (simulates model working‑set).</li>
<li><code>IE_STRIDE_BYTES</code> : stride for the work‑touch pointer (default <code>256</code>).</li>
<li><code>IE_VERIFY_TOUCH</code> : <code>1</code> to prevent the compiler from optimizing the touch accumulator away.</li>
<li><code>PRECISION</code> : float precision hint to the CLI (<code>fp32|bf16|fp16</code>). (Alias of <code>IE_PRECISION</code> when using float modes.)</li>
<li><code>IE_PRECISION</code> : raw precision label passed to the engine (<code>fp32|bf16|fp16|int8w|int4|int4w</code>).</li>
<li><code>THREADS</code> : CPU threads hint (e.g., <code>12</code>).</li>
<li><code>BATCH</code> : batch size hint (default <code>1</code>).</li>
</ul>
<p>The CLI also accepts explicit flags that benchmarks may forward:</p><ul>
<li><code>--device auto|cpu|cuda|ze</code> (hint only; selection occurs at build/link)</li>
<li><code>--model-dir PATH</code> (chdir before loading IEBIN)</li>
<li><code>--model-json PATH</code>, <code>--model-bin PATH</code> (explicit file paths)</li>
<li><code>--pretranspose none|woh|wxh|all</code></li>
<li><code>--prefetch on|off|auto|N</code></li>
<li><code>--warmup N</code></li>
<li><code>--rounds N</code></li>
<li><code>--prompts-file PATH</code></li>
<li><code>--aggregate</code></li>
</ul>
<h2><a class="anchor" id="autotoc_md123"></a>
End‑to‑End Examples</h2>
<p><b>CPU, strict mode, 64 MB per token, int4w weights:</b> </p><div class="fragment"><div class="line">PROMPTS=benchmarks/prompts_10..txt IE_PRECISION=int4w IE_REQUIRE_MODEL=1 IE_BYTES_PER_TOKEN=64000000 IE_STRIDE_BYTES=256 RUNS=3 make bench</div>
</div><!-- fragment --><p><b>CUDA, same settings:</b> </p><div class="fragment"><div class="line">PROMPTS=benchmarks/prompts_10..txt IE_PRECISION=int4w IE_REQUIRE_MODEL=1 IE_BYTES_PER_TOKEN=64000000 IE_STRIDE_BYTES=256 RUNS=3 make bench-cuda</div>
</div><!-- fragment --><p><b>Direct CLI (bypass Makefile), model in <code>models/gpt-oss-20b</code>:</b> </p><div class="fragment"><div class="line">./build/inference-engine   --model-dir models/gpt-oss-20b   --precision fp32   --pretranspose all   --prompts-file benchmarks/prompts_10..txt   --max-new 128 --threads 12 --rounds 1</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md124"></a>
Return Codes</h2>
<ul>
<li><code>0</code> success, JSON line printed.</li>
<li><code>2</code> bad CLI usage / invalid integer.</li>
<li><code>3</code> strict IEBIN required but missing/unreadable.</li>
<li><code>5</code> engine creation failed.</li>
<li><code>6</code> OOM allocating token buffer.</li>
</ul>
<h1><a class="anchor" id="autotoc_md125"></a>
RSS Reporting</h1>
<h2><a class="anchor" id="autotoc_md126"></a>
RSS Reporting</h2>
<ul>
<li>We sample <b>peak resident set size (RSS)</b> after the measured window to avoid skewing TPS.</li>
<li>On Linux we prefer <code>/proc/self/status</code> → <code>VmHWM</code> (kB). Fallback is <code>getrusage(RUSAGE_SELF).ru_maxrss</code>.</li>
<li>On macOS we use <code>getrusage</code> where <code>ru_maxrss</code> is in <b>bytes</b>.</li>
<li>If neither is available, the sampler returns <b>0 MB</b>.</li>
</ul>
<h1><a class="anchor" id="autotoc_md127"></a>
Update Journal</h1>
<p>This running log summarizes meaningful doc/CLI changes for reproducibility.</p>
<ul>
<li><b>2025-10-24 21:08:40 UTC</b> — INT4 step added to docs; CLI grew <code>--device</code>, <code>--model-dir</code>, <code>--rounds</code>; strict RSS peak sampler now reads <code>/proc/self/status</code> <code>VmHWM</code> (Linux) with <code>getrusage</code> fallback; <code>bench</code>/<code>bench-cuda</code> examples updated for <b>64 MB/token</b> work‑touch.</li>
</ul>
<hr  />
 <h1><a class="anchor" id="autotoc_md129"></a>
What’s new — 2025-11-10</h1>
<h2><a class="anchor" id="autotoc_md130"></a>
Step 1 — NUMA‑aware topology &amp; thread binding</h2>
<ul>
<li>Added <code><a class="el" href="ie__topology_8h.html">engine/include/ie_topology.h</a></code> + <code><a class="el" href="topology_8c.html">engine/src/opt/topology.c</a></code> to discover sockets/CPUs from Linux sysfs and expose helpers:<ul>
<li><code>ie_topology_init/destroy</code>, <code><a class="el" href="ie__topology_8h.html#ae9fec2d245fb73d6c3e8e3cba38d4471" title="Return the number of sockets (physical packages).">ie_topology_sockets()</a></code>, <code><a class="el" href="ie__topology_8h.html#a69b006879441857f80d4c4f200376d11" title="Return a representative CPU id that belongs to the given socket.">ie_topology_first_cpu_on_socket()</a></code>.</li>
</ul>
</li>
<li>CLI/env integration (soft hints): <code>AFFINITY=auto|compact|scatter</code> and <code>IE_TP_USE_AFFINITY=1</code> (thread‑pool), plus <code>IE_HOT_REPLICATE</code> (see Step 2).</li>
<li>Fallback: on systems without sysfs or NUMA, we assume a single socket with <code>sysconf(_SC_NPROCESSORS_ONLN)</code> CPUs.</li>
</ul>
<h2><a class="anchor" id="autotoc_md131"></a>
Step 2 — “Hot” weights replication per socket</h2>
<ul>
<li>New module <code><a class="el" href="replicate__hot_8c.html" title="Replication of hot weights per socket using first-touch placement.">engine/src/opt/replicate_hot.c</a></code> replicates frequently‑touched (“hot”) weight pages <b>per socket</b> and pins worker threads on that socket for locality.</li>
<li>Optional prefetch hint via <code>madvise(..., MADV_WILLNEED)</code> (ignored if unsupported).</li>
<li>Enable at runtime: <div class="fragment"><div class="line">export IE_HOT_REPLICATE=1      # enable per‑socket replicas</div>
<div class="line">export IE_HOT_REPL_LIMIT_MB=0  # (optional) cap replica size in MiB; 0 = no cap</div>
</div><!-- fragment --></li>
</ul>
<h2><a class="anchor" id="autotoc_md132"></a>
Activation precision (INT8 / FP8) — soft hint</h2>
<ul>
<li>Expose activation precision as a soft runtime hint (host math remains FP32 accumulate): <div class="fragment"><div class="line">export IE_ACT_PREC=int8   # or: fp8 | fp16 | bf16 | fp32</div>
</div><!-- fragment --></li>
<li>Weight precision remains independent via <code>IE_PRECISION</code> (e.g., <code>int4w</code>, <code>int8w</code>, <code>fp32</code>, etc.).</li>
</ul>
<h2><a class="anchor" id="autotoc_md133"></a>
Strict timing rule (re‑stated)</h2>
<p>Only the following are counted in the measured window:</p><ul>
<li><code>ie_engine_generate(...)</code></li>
<li>optional “work‑touch” loop controlled by <code>IE_BYTES_PER_TOKEN</code>, <code>IE_STRIDE_BYTES</code>, <code>IE_VERIFY_TOUCH</code></li>
</ul>
<h2><a class="anchor" id="autotoc_md134"></a>
Quick run recipes</h2>
<p>Strict CPU run with INT4 weights, FP8 activations, NUMA‑aware binding + hot replication: </p><div class="fragment"><div class="line"># Model &amp; timing</div>
<div class="line">export MODEL_DIR=models/gpt-oss-20b</div>
<div class="line">export IE_REQUIRE_MODEL=1</div>
<div class="line">export IE_PRECISION=int4w</div>
<div class="line">export IE_ACT_PREC=fp8</div>
<div class="line">export IE_BYTES_PER_TOKEN=$((64*1024*1024))</div>
<div class="line">export IE_STRIDE_BYTES=256</div>
<div class="line">export IE_VERIFY_TOUCH=1</div>
<div class="line"> </div>
<div class="line"># NUMA &amp; replication</div>
<div class="line">export IE_HOT_REPLICATE=1</div>
<div class="line">export IE_TP_USE_AFFINITY=1</div>
<div class="line">export AFFINITY=compact</div>
<div class="line"> </div>
<div class="line"># Run (CPU)</div>
<div class="line">PROMPTS=benchmarks/prompts_10..txt RUNS=3 make bench</div>
</div><!-- fragment --><p>CUDA run (same semantics; binary is different): </p><div class="fragment"><div class="line">PROMPTS=benchmarks/prompts_10..txt RUNS=3 make bench-cuda</div>
</div><!-- fragment --><p>NUMA pinning from the shell (optional, only if multiple nodes exist): </p><div class="fragment"><div class="line">if numactl -H | grep -q &#39;available:&#39;; then</div>
<div class="line">  NODES=$(numactl -H | awk &#39;/available:/{print $2}&#39;)</div>
<div class="line">  [ &quot;$NODES&quot; -gt 1 ] &amp;&amp; exec numactl --cpunodebind=0 --membind=0 bash -lc &#39;PROMPTS=benchmarks/prompts_10..txt RUNS=3 make bench&#39;</div>
<div class="line">fi</div>
</div><!-- fragment --><blockquote class="doxtable">
<p>Note: If <code>numactl -H</code> shows a single node, skip <code>numactl --cpunodebind/--membind</code>. </p>
</blockquote>
<p>&mdash; </p>
<h1><a class="anchor" id="autotoc_md135"></a>
What's new — Memory Phase (updated 2025-11-12 18:01:19 UTC)</h1>
<h2><a class="anchor" id="autotoc_md136"></a>
New tuning knobs (memory/throughput)</h2>
<p>These are read by the CLI and/or harness. Backends may ignore unsupported hints safely.</p>
<ul>
<li><code>IE_PREFETCH_DISTANCE</code> — integer distance in bytes (or <code>auto</code>) for software prefetch of weight streams in GEMV.</li>
<li><code>IE_NT_LOADS</code> — <code>0|1|auto</code>. When enabled, kernels attempt <b>non‑temporal loads</b> (streaming) on large, one‑time read paths.</li>
<li><code>IE_L3_BYTES</code> — integer budget in bytes for L3‑resident “hot” slices (heuristic used by blocked‑K and replication).</li>
<li><code>IE_NT_RATIO</code> — <code>0..100</code> (percentage) hint for the fraction of loads to mark non‑temporal in mixed patterns.</li>
<li><code>IE_ACT_PREC</code> — activation precision hint: <code>int8|fp8|fp16|bf16|fp32</code> (host accumulators remain FP32).</li>
<li><code>NUMA_POLICY</code> — <code>auto|compact|scatter|socket:&lt;id&gt;</code>. Works with the new topology detector for thread pinning.</li>
<li><code>IE_HOT_REPLICATE</code> — <code>0|1</code> enable per‑socket replicas of “hot” weights; optional cap via <code>IE_HOT_REPL_LIMIT_MB</code> (MiB).</li>
</ul>
<h2><a class="anchor" id="autotoc_md137"></a>
Harness sweep (benchmarks)</h2>
<p>The harness can <b>sweep</b> combinations of memory options to find stable TPS under bandwidth pressure:</p>
<div class="fragment"><div class="line">python3 benchmarks/harness.py \</div>
<div class="line">  --sweep &#39;act_quant=int8,fp8|kv_quant=none|prefetch=off,auto,256|nt_loads=0,1|numa_policy=auto,compact,scatter&#39; \</div>
<div class="line">  --prompts benchmarks/prompts.jsonl</div>
</div><!-- fragment --><p> Each configuration is labeled and exported to CSV/JSONL; <code><a class="el" href="update__performance__md_8py.html">scripts/update_performance_md.py</a></code> now includes memory metrics (<em>MB/token, bytes touched, coverage vs model.bin, effective bandwidth GB/s</em>).</p>
<h2><a class="anchor" id="autotoc_md138"></a>
Monitoring</h2>
<ul>
<li>Added <code><a class="el" href="metrics__memory_8toml.html">monitoring/metrics_memory.toml</a></code> (recording rules/panels for <b>RSS peak</b>, <b>bytes touched</b>, <b>effective bandwidth</b>).</li>
<li><code><a class="el" href="metrics__exporter_8py.html">scripts/metrics_exporter.py</a></code> exposes the new fields when present (see <code>ie_rss_peak_mb_*</code>, and the <code>ie_build_info</code> labels).</li>
</ul>
<h2><a class="anchor" id="autotoc_md139"></a>
CUDA/CPU kernels</h2>
<ul>
<li>CUDA: added fused GEMV for <b>INT8 activations</b> and <b>FP8 activations</b> (E4M3/E5M2 decoders on‑device).</li>
<li>CPU: dispatcher keeps AVX2/FMA fast path; generic C path honors blocked‑K and activation dequant (INT8 per‑tensor / per‑group).</li>
</ul>
<blockquote class="doxtable">
<p>Tip: for strict, bandwidth‑bound tests use: <code>IE_REQUIRE_MODEL=1 IE_BYTES_PER_TOKEN=64000000 IE_STRIDE_BYTES=256</code>. </p>
</blockquote>
<h1><a class="anchor" id="autotoc_md140"></a>
&lt;/blockquote&gt;</h1>
<h1><a class="anchor" id="autotoc_md141"></a>
What's new — Block-sparse weights (CPU only, 2025‑11‑14)</h1>
<p>This phase adds a <b>block‑sparse representation for weight matrices</b> and a reference CPU implementation. The goal is to make sparsity experiments reproducible without disturbing the existing dense path.</p>
<h2><a class="anchor" id="autotoc_md142"></a>
New artifacts</h2>
<p>Code:</p>
<ul>
<li><code><a class="el" href="sparse__format_8h.html" title="In-memory representation and loader interface for block-sparse matrices.">engine/include/sparse_format.h</a></code> — public descriptor for block‑sparse matrices (<code>ie_block_sparse_matrix_t</code>) plus helpers and status codes.</li>
<li><code><a class="el" href="sparse__io_8c.html" title="Loader for block-sparse weight matrices from a compact binary format.">engine/src/sparse_io.c</a></code> — on‑disk header/loader for a compact block‑sparse binary format (<code>ie_block_sparse_load</code>).</li>
<li><code><a class="el" href="gemm__block__sparse_8c.html" title="Block-sparse GEMV kernel for single-precision weights.">engine/src/gemm_block_sparse.c</a></code> — single‑threaded reference GEMV (<code>ie_gemv_block_sparse_f32</code>) over the BSR structure.</li>
<li><code><a class="el" href="ie__device__common_8c.html" title="C11 implementation of the device abstraction with CPU backend and CUDA/Level Zero stubs resolved via ...">engine/src/devices/ie_device_common.c</a></code> — device vtable extended with a <code>gemv_block_sparse_f32</code> entry and a CPU fallback implementation.</li>
<li><code><a class="el" href="test__block__sparse_8c.html" title="Unit tests for block-sparse format helpers and GEMV kernel.">tests/c/test_block_sparse.c</a></code> — C unit tests that validate the loader and GEMV on small hand‑built matrices.</li>
<li><code>benchmarks/src/microbench_gemv_block_sparse.c</code> — standalone microbenchmark that compares dense vs block‑sparse GEMV on CPU.</li>
<li><code><a class="el" href="convert__to__block__sparse_8c.html" title="Offline converter from dense FP32 matrix to block-sparse binary file.">tools/convert_to_block_sparse.c</a></code> — offline converter from a dense row‑major weight matrix to the on‑disk block‑sparse format.</li>
</ul>
<p>Build/Makefile:</p>
<ul>
<li><code>Makefile</code> now compiles the new sources and wires the block‑sparse microbench under <code>make microbench-block-sparse</code> (CPU only).</li>
<li>The main <code>make test</code> target runs <code>test_block_sparse</code> together with the existing C unit tests.</li>
</ul>
<h2><a class="anchor" id="autotoc_md143"></a>
Scope and current limitations</h2>
<ul>
<li><b>Backend:</b> CPU only. CUDA/Level‑Zero code paths simply report “unimplemented” for block‑sparse GEMV and transparently fall back to the dense CPU implementation if called.</li>
<li><b>Layout:</b> fixed <b>block‑row CSR (BSR)</b> layout with uniform <code>block_rows x block_cols</code>. Tail blocks at the matrix edges are handled by clipping reads/writes using the dense <code>rows/cols</code> metadata.</li>
<li><b>Datatype:</b> FP32 weights only. Activations, quantized paths and mixed precision remain dense for now.</li>
<li><b>Integration:</b> the block‑sparse path is currently exercised via the C tests and the dedicated microbenchmark. The CLI still consumes dense <code>model.ie.bin</code>; wiring full‑model block‑sparse weights will be a separate ADR/phase.</li>
</ul>
<p>See <code><a class="el" href="DESIGN_8md.html">DESIGN.md</a></code> (Block‑sparse weights chapter) and <code><a class="el" href="DECISIONS_8md.html">DECISIONS.md</a></code> (ADR for block‑sparse) for full details. </p>
</div></div><!-- PageDoc -->
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1 </li>
  </ul>
</div>
</body>
</html>
