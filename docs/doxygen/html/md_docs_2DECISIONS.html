<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Inference Engine (Clocher): Architectural Decision Records</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Inference Engine (Clocher)<span id="projectnumber">&#160;0.2</span>
   </div>
   <div id="projectbrief">C11 CPU/GPU inference baseline with strict metrics &amp; harness</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('md_docs_2DECISIONS.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Architectural Decision Records</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="autotoc_md4"></a> <b>Last updated:</b> 2025-10-24 21:00:48 UTC</p>
<ul>
<li><b>ADR-0001 (2025-10-13)</b>: Core in C11, zero third-party runtime; Python stdlib harness.</li>
<li><b>ADR-0002 (2025-10-13)</b>: IEBIN v1 weights: <code>model.ie.json</code> + <code>model.ie.bin</code> + <code>vocab.json</code> (mmap-friendly).</li>
<li><b>ADR-0003 (2025-10-13)</b>: Baseline = FP32 naive path; TPS = generated_tokens / wall_time.</li>
<li><b>ADR-0006 (2025-10-14)</b>: Optimization path selection (CPU baseline) — AVX2 microkernels, thread-pool with affinity, fast math, layout packing. See <code>adr-00060-optimization-path.md</code>.</li>
<li><b>ADR-0010 (2025-10-16)</b>: INT8 PTQ min-max (per-tensor/per-row) with calibration script and accuracy budget gate (cosine ≥ threshold).</li>
<li><b>ADR-0011 (2025-10-24)</b>: Drop Level Zero from default benchmarking; GPU bench is CUDA-only; CPU/GPU share a unified reporting pipeline.</li>
</ul>
<hr  />
<h1><a class="anchor" id="autotoc_md6"></a>
&lt;strong&gt;ADR-0013 (2025-10-24): Adopt INT4 weight-only PTQ &amp; manifest-guided packing&lt;/strong&gt;</h1>
<p><b>Context.</b> The engine is primarily <b>memory-bandwidth bound</b> at inference time. Even with blocked‑K packing and prefetch, large models remain limited by weight fetch. INT8 provided a 2× compression; we want a <b>denser format</b> while preserving a simple compute path.</p>
<p><b>Decision.</b></p><ul>
<li>Introduce <b>INT4 weight‑only</b> quantization with:<ul>
<li><b>Nibble packing</b> (2 weights/byte) and <b>per‑row (or group) scales</b>.</li>
<li>A repository‑wide <b>manifest</b> (<code>q4_manifest.json</code>) describing which tensors are INT4 and how to dequantize them.</li>
<li>CLI/runtime selection via <code>IE_PRECISION=int4w</code> (or <code>--precision int4w</code>).</li>
</ul>
</li>
<li>Keep activations in floating point; accumulation stays FP32 for numerical stability.</li>
</ul>
<p><b>Consequences.</b></p><ul>
<li><b>4× smaller weights</b> → lower bandwidth → higher effective TPS under work‑touch or real inference.</li>
<li>Slight accuracy cost, controlled by calibration. PTQ scripts expose gates (cosine/task‑level) to accept/reject a manifest.</li>
<li>Kernel changes are incremental: dequant scale application in the matmul inner loop; packing integrates with existing pretranspose caches.</li>
</ul>
<p><b>Status.</b> Accepted. Implemented across CPU and CUDA builds; scripts support end‑to‑end HF→IEBIN packing with <code>--q4-map</code>.</p>
<hr  />
 <h1><a class="anchor" id="autotoc_md8"></a>
Decision: Add INT4 (Weight-Only) Optional Pipeline (2025-10-24 21:04:23 UTC)</h1>
<p><b>Context.</b> We already ship FP32/BF16/FP16 flows. We add a <em>weight-only</em> INT4 path to reduce bandwidth and model footprint while keeping API stability.</p>
<p><b>Decision.</b></p><ul>
<li>Introduce a manifest-driven INT4 packing step in export (<code>--q4-map</code>).</li>
<li>Expose runtime selection via <code>IE_PRECISION=int4w</code> (or <code>--precision int4w</code>).</li>
<li>Keep timing discipline: measure generation + work-touch only; sample metrics after.</li>
</ul>
<p><b>Consequences.</b></p><ul>
<li>Lower I/O pressure when <code>IE_BYTES_PER_TOKEN</code> simulates large working sets.</li>
<li>Slight decode overhead for dequantization (amortized in GEMV paths).</li>
<li>No change to tokenization or batching semantics.</li>
</ul>
<p><b>Status.</b> Accepted and implemented. Backward-compatible (FP paths unaffected).</p>
<hr  />
<h1><a class="anchor" id="autotoc_md10"></a>
Appendix — INT4 (Weight‑Only) Step (Summary)</h1>
<ul>
<li>Convert HF shards → IEBIN with an INT4 manifest: <div class="fragment"><div class="line">python3 scripts/hf_to_iebin.py     --hf-dir models/gpt-oss-20b/hf     --out-dir models/gpt-oss-20b     --q4-map quant/q4_manifest.json</div>
</div><!-- fragment --></li>
<li>Run benchmarks in strict mode with a <b>64 MB/token</b> work‑touch: <div class="fragment"><div class="line">PROMPTS=benchmarks/prompts_10..txt   IE_PRECISION=int4w IE_REQUIRE_MODEL=1   IE_BYTES_PER_TOKEN=64000000 IE_STRIDE_BYTES=256 RUNS=3   make bench           # or: make bench-cuda</div>
</div><!-- fragment --></li>
<li>Precision hints: <code>PRECISION=fp32</code> (activates float path) and <code>IE_PRECISION=int4w</code> (weight‑only path).</li>
</ul>
<hr  />
<p> <b>Last updated:</b> 2025-11-10 21:46:36 UTC</p>
<ul>
<li><b>ADR-0014 (2025-11-10)</b>: NUMA‑aware topology and thread binding<ul>
<li><b>Context:</b> We need consistent socket awareness to reduce cross‑socket traffic and stabilize TPS.</li>
<li><b>Decision:</b> Introduce <code><a class="el" href="structie__topology.html" title="Internal representation.">ie_topology</a></code> (sysfs‑backed) to detect sockets and map threads→CPUs with <code>AFFINITY</code> hints.</li>
<li><b>Consequences:</b> Lower LLC misses and cross‑node memory, improved repeatability; graceful single‑socket fallback.</li>
<li><b>Status:</b> Accepted. Implemented in <code><a class="el" href="ie__topology_8h.html">engine/include/ie_topology.h</a></code> and <code><a class="el" href="topology_8c.html">engine/src/opt/topology.c</a></code>.</li>
</ul>
</li>
<li><b>ADR-0015 (2025-11-10)</b>: Replicate “hot” weights per socket and bind workers<ul>
<li><b>Context:</b> For bandwidth‑bound models, remote‑node page faults throttle TPS. Replicating hot tensors near compute reduces latency.</li>
<li><b>Decision:</b> Add <code><a class="el" href="replicate__hot_8c.html" title="Replication of hot weight blobs per socket using first-touch placement.">replicate_hot.c</a></code> to create per‑socket replicas (mmap‑based), then bind per‑socket workers to CPUs on that socket.</li>
<li><b>Consequences:</b> Higher memory footprint when enabled, but improved locality and throughput on multi‑socket hosts.</li>
<li><b>Status:</b> Accepted. Guarded by <code>IE_HOT_REPLICATE=1</code>; optional <code>MADV_WILLNEED</code> prefetch.</li>
</ul>
</li>
<li><b>ADR-0016 (2025-11-10)</b>: Activation precision soft hint (INT8/FP8)<ul>
<li><b>Context:</b> Allow experimenting with lower‑precision activations without entangling storage precision for weights.</li>
<li><b>Decision:</b> Add <code>IE_ACT_PREC</code> (values: <code>int8|fp8|fp16|bf16|fp32</code>). Host accumulators remain FP32; backends may ignore unsupported hints.</li>
<li><b>Consequences:</b> Decouples storage (e.g., <code>int4w</code>) from activation math, enabling orthogonal tuning. </li>
</ul>
</li>
</ul>
<h1><a class="anchor" id="autotoc_md12"></a>
- &lt;strong&gt;Status:&lt;/strong&gt; Accepted. Backward‑compatible; default remains FP32 if unset.</h1>
<h1><a class="anchor" id="autotoc_md13"></a>
ADR‑0017 (Memory Streaming Heuristics): Prefetch &amp; Non‑Temporal Loads — 2025-11-12 18:01:19 UTC</h1>
<p><b>Context.</b> GEMV paths are bandwidth‑bound. Sequential blocked‑K layouts help, but cache pollution and late prefetch still cap TPS.</p>
<p><b>Decision.</b></p><ul>
<li>Adopt tunables <code>IE_PREFETCH_DISTANCE</code>, <code>IE_NT_LOADS</code>, <code>IE_NT_RATIO</code>, and <code>IE_L3_BYTES</code> to steer prefetch distance, streaming loads, and an approximate L3 budget for “hot” slices.</li>
<li>Default is <b>conservative</b> (<code>auto</code>) with architecture checks; explicit values override.</li>
</ul>
<p><b>Consequences.</b></p><ul>
<li>Wins on large models/long K with single‑touch patterns.</li>
<li>Small rows or re‑use‑heavy layers may regress if NT is forced → keep <code>auto</code> as default and expose per‑run sweeps in the harness.</li>
</ul>
<p><b>Status.</b> Accepted. Implemented in CPU AVX2 and CUDA pack/GEMV call sites with guarded paths.</p>
<hr  />
 <h1><a class="anchor" id="autotoc_md15"></a>
ADR‑0018 (Metrics &amp; Reporting): Spatial Metrics in PERFORMANCE.md — 2025-11-12 18:01:19 UTC</h1>
<p><b>Context.</b> Throughput alone hides memory behavior. We need bytes/token, coverage vs model, RSS peak, and effective bandwidth.</p>
<p><b>Decision.</b></p><ul>
<li>Extend the JSON summary and the Markdown generator to compute spatial fields from <code>IE_BYTES_PER_TOKEN</code>, run totals, and model size.</li>
<li>Update Prometheus exporter with a <code>ie_build_info</code> gauge (labels) and RSS gauges.</li>
</ul>
<p><b>Consequences.</b></p><ul>
<li>Comparable runs across machines now capture memory pressure explicitly.</li>
<li>Grafana dashboards can plot GB/s alongside TPS for stability analysis.</li>
</ul>
<p><b>Status.</b> Accepted. Shipped with <code><a class="el" href="metrics__memory_8toml.html">monitoring/metrics_memory.toml</a></code>, updated <code><a class="el" href="metrics__exporter_8py.html">scripts/metrics_exporter.py</a></code>, and harness sweep labels for memory knobs.</p>
<h1><a class="anchor" id="autotoc_md16"></a>
ADR‑0019 (Sparsity): Block‑sparse weights, CPU‑only prototype — 2025‑11‑14 23:00:00 UTC</h1>
<p><b>Context.</b></p><ul>
<li>Phase 1 of the memory work focused on <em>dense</em> optimizations: INT4/INT8 activations, NUMA‑aware topology, hot‑weights replication, and streaming loads.</li>
<li>The next natural axis is <b>algorithmic sparsity</b>: skip zero blocks instead of streaming everything.</li>
<li>We want a conservative, inspectable prototype that:<ul>
<li>lives entirely on the CPU path;</li>
<li>does <b>not</b> disturb the existing dense loading/CLI;</li>
<li>is testable in isolation (C unit tests + microbench).</li>
</ul>
</li>
</ul>
<p><b>Decision.</b></p><ul>
<li>Introduce a small, self‑contained <b>block‑sparse format</b> and CPU kernel:<ul>
<li>New descriptor type <code>ie_block_sparse_matrix_t</code> in <code><a class="el" href="sparse__format_8h.html" title="In-memory representation and loader interface for block-sparse matrices.">engine/include/sparse_format.h</a></code>.</li>
<li>Loader in <code><a class="el" href="sparse__io_8c.html" title="Loader for block-sparse weight matrices from a compact binary format.">engine/src/sparse_io.c</a></code> that reads a compact binary header and BSR payload.</li>
<li>Reference GEMV in <code><a class="el" href="gemm__block__sparse_8c.html" title="Block-sparse GEMV kernel for single-precision weights.">engine/src/gemm_block_sparse.c</a></code> operating on FP32 weights.</li>
</ul>
</li>
<li>Extend the device abstraction:<ul>
<li>Add <code>gemv_block_sparse_f32</code> to the <code><a class="el" href="structie__device.html" title="Public device object combining vtable and backend-specific impl pointer.">ie_device</a></code> vtable.</li>
<li>Provide a CPU implementation wired to <code>ie_gemv_block_sparse_f32</code>.</li>
<li>Keep CUDA/Level‑Zero entries as stubs that return “unimplemented”; the public helpers fall back to CPU.</li>
</ul>
</li>
<li>Add offline and benchmarking tools:<ul>
<li><code><a class="el" href="convert__to__block__sparse_8c.html" title="Offline converter from dense FP32 matrix to block-sparse binary file.">tools/convert_to_block_sparse.c</a></code> to convert dense row‑major weights into the on‑disk BSR format.</li>
<li><code><a class="el" href="test__block__sparse_8c.html" title="Unit tests for block-sparse format helpers and GEMV kernel.">tests/c/test_block_sparse.c</a></code> with small hand‑built matrices to validate loader + GEMV.</li>
<li><code>benchmarks/src/microbench_gemv_block_sparse.c</code> to compare dense vs block‑sparse GEMV on CPU.</li>
</ul>
</li>
<li>Restrict scope explicitly:<ul>
<li><b>CPU only</b> for this ADR; no GPU kernels are required to pass tests.</li>
<li><b>FP32 weights only</b>; quantized/specialized paths remain dense.</li>
</ul>
</li>
</ul>
<p><b>Consequences.</b></p><ul>
<li>We have a <b>concrete, repeatable</b> sparsity experiment:<ul>
<li>Same repository, same Makefile; add a dense <code>.bin</code>, run the converter, microbench the result.</li>
<li>Unit tests ensure correctness on small matrices.</li>
</ul>
</li>
<li>The device layer remains forward‑compatible:<ul>
<li>Future GPU support can fill in the existing vtable slot without touching the public API.</li>
<li>Callers can ask for block‑sparse GEMV and still get a correct CPU fallback today.</li>
</ul>
</li>
<li>No risk to existing users:<ul>
<li>CLI behavior and <code>model.ie.bin</code> format are unchanged.</li>
<li>All new code is opt‑in and exercised only by tests/microbench/scripts introduced in this phase.</li>
</ul>
</li>
</ul>
<p><b>Status.</b></p><ul>
<li>Accepted. Implemented as a <b>CPU‑only prototype</b>; further ADRs will cover wiring full‑model weights and any GPU/quantized variants.</li>
</ul>
<hr  />
<h1><a class="anchor" id="autotoc_md18"></a>
ADR-0020 (2025-12-22): Adopt lossless dedup artifacts and schema2 runtime loader</h1>
<p><b>Context.</b> The engine is primarily memory-bandwidth bound at inference time. We already attack the problem via INT4 weight-only storage and streaming heuristics, but dense IEBIN still requires reading large swaths of the weight blob repeatedly. We need a <b>lossless</b> dedup scheme that can reduce DRAM traffic without changing model outputs.</p>
<p><b>Decision.</b></p><ul>
<li>Adopt a three-blob dedup artifact set alongside IEBIN:<ul>
<li><code>model.defaults.bin</code></li>
<li><code>model.masks.bin</code></li>
<li><code>model.exceptions.bin</code></li>
</ul>
</li>
<li>Enable the loader behind runtime flags:<ul>
<li><code>IE_DEDUP=1</code>, <code>IE_DEDUP_POLICY=lossless</code></li>
<li><code>IE_DEDUP_STRICT=1</code> for “fail fast” correctness runs</li>
<li><code>IE_DEDUP_CACHE_MB</code> to cap reconstruction cache memory</li>
<li><code>IE_DEDUP_DEBUG=1</code> for verbose diagnostics</li>
</ul>
</li>
<li>Keep schema2 metadata parsing strict, but allow compatibility with HF-derived metadata by:<ul>
<li>requiring lowercase <code>dtype</code></li>
<li>supporting <code>file</code>/<code>file_data_offset</code> (and generating aliases from <code>shard</code>/<code>shard_data_offset</code> in the metadata pipeline when needed)</li>
</ul>
</li>
</ul>
<p><b>Consequences.</b></p><ul>
<li>Offline extraction time increases (diffing defaults vs targets), but runtime is simplified to patch-apply.</li>
<li>Production runs must ship three additional binary files next to <code>model.ie.json</code> / <code>model.ie.bin</code> (or symlink them).</li>
<li>Benchmarks can now report meaningful “dedup on/off” TPS deltas under strict work-touch conditions.</li>
</ul>
<p><b>Status.</b> Accepted. Implemented (CPU path) and integrated in strict harness runs. CUDA path uses the same artifact layout and flags where supported. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8 </li>
  </ul>
</div>
</body>
</html>
