<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Inference Engine (Clocher): Design (CPU baseline + INT4 path)</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Inference Engine (Clocher)<span id="projectnumber">&#160;0.2</span>
   </div>
   <div id="projectbrief">C11 CPU/GPU inference baseline with strict metrics &amp; harness</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('md_docs_2DESIGN.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Design (CPU baseline + INT4 path)</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="autotoc_md22"></a> This document describes the hot path, API boundaries, and precision modes. <br  />
 <b>Last updated:</b> 2025-10-24 21:00:48 UTC</p>
<h1><a class="anchor" id="autotoc_md30"></a>
Process and boundaries</h1>
<ul>
<li>Single binary <code>inference-engine</code>: create → generate → collect metrics → destroy.</li>
<li>CLI prints exactly one JSON line per run so the Python harness can ingest results.</li>
<li>No third‑party runtime dependencies; only <code>pthread</code> and <code>libm</code> (and CUDA for the GPU build).</li>
</ul>
<h1><a class="anchor" id="autotoc_md31"></a>
API surface (high level)</h1>
<ul>
<li><code>ie_engine_create(cfg)</code> → initializes state (weights, buffers, thread‑pool).</li>
<li><code>ie_engine_generate(prompt, max_new, params)</code> → produces tokens and updates metrics rings.</li>
<li><code>ie_engine_metrics(out)</code> → returns latency p50/p95, true TPS, <b>RSS peak</b>, KV hits/misses.</li>
<li><code><a class="el" href="ie__api_8h.html#a5a9a30f05ef6183f6a75811c616b3a0a" title="Destroy an engine instance.">ie_engine_destroy()</a></code> → frees all resources.</li>
</ul>
<h1><a class="anchor" id="autotoc_md32"></a>
Hot path layout</h1>
<ul>
<li>GEMV microkernel:<ul>
<li>Generic scalar reference implementation.</li>
<li>AVX2/FMA path with light prefetch and blocked‑K packing.</li>
</ul>
</li>
<li>Activation:<ul>
<li><code>tanh</code> fast path with clamp (accuracy‑bounded), vector helper.</li>
<li>Optional fused bias + activation to reduce memory traffic.</li>
</ul>
</li>
</ul>
<h1><a class="anchor" id="autotoc_md33"></a>
Precision modes</h1>
<h2><a class="anchor" id="autotoc_md34"></a>
Floating point</h2>
<ul>
<li>FP32 baseline, optional BF16/FP16 round‑trip (accumulate FP32).</li>
</ul>
<h2><a class="anchor" id="autotoc_md35"></a>
INT8 PTQ (reference)</h2>
<ul>
<li>Per‑tensor/per‑row scales (min‑max); (de)quant helpers; task gate in scripts.</li>
</ul>
<h2><a class="anchor" id="autotoc_md36"></a>
&lt;strong&gt;NEW — INT4 PTQ (weight‑only)&lt;/strong&gt;</h2>
<ul>
<li><b>Format</b>: weights are <b>nibble‑packed</b> (2 values per byte). Each row (or group) carries a scale (and optional zero‑point if affine).</li>
<li><b>Packing</b>: INT4 packing integrates with the existing <b>pretranspose / blocked‑K</b> pipeline. Packing order: float → (optional) pretranspose → quantize to int4 → pack.</li>
<li><b>Dequantization</b>: fused in the matmul path; scale is broadcast per row (or group) to recover FP32 accumulators.</li>
<li><b>Manifests</b>: a <code>q4_manifest.json</code> enumerates which tensors use INT4 and their scale metadata. <code><a class="el" href="hf__to__iebin_8py.html">scripts/hf_to_iebin.py</a></code> consumes this manifest via <code>--q4-map</code> to emit <code>model.ie.bin</code>/<code>.json</code>.</li>
<li><b>Selection</b>: at runtime choose <code>IE_PRECISION=int4w</code> (or <code>--precision int4w</code>), leaving activations in float. This is <b>bandwidth‑oriented</b> and preserves compute simplicity.</li>
</ul>
<h1><a class="anchor" id="autotoc_md37"></a>
Threading model</h1>
<ul>
<li>Fixed thread‑pool over <code>pthread</code>, contiguous sharding, grainsize control.</li>
<li>Affinity (Linux): <code>IE_TP_USE_AFFINITY=1</code> enables <code>auto|compact|scatter</code>.</li>
<li>NUMA:<ul>
<li><code><a class="el" href="set__numa_8sh.html">scripts/set_numa.sh</a></code> can set OS policy (<code>interleave|node:X|strict</code>).</li>
<li>In‑repo probe reads <code>/sys/devices/system/node/online</code> to annotate reports.</li>
</ul>
</li>
</ul>
<h1><a class="anchor" id="autotoc_md38"></a>
Layout and caching</h1>
<ul>
<li>Blocked‑K packing with optional on‑disk caching (content‑addressed by shape + block size).</li>
<li>CLI flag <code>--pretranspose</code> controls packing scope (<code>none|woh|wxh|all</code>). INT4 can be cached per layout variant.</li>
</ul>
<h1><a class="anchor" id="autotoc_md39"></a>
Metrics</h1>
<ul>
<li>Per‑token latency ring (p50/p95).</li>
<li>True TPS (<code>generated_tokens / wall_time_s</code>).</li>
<li><b>Peak RSS</b> (Linux <code>/proc/self/status:VmHWM</code> → MiB; fallback <code>getrusage</code>).</li>
<li>KV hits/misses counter stubs aggregated per round.</li>
</ul>
<h1><a class="anchor" id="autotoc_md40"></a>
GPU integration (CUDA path)</h1>
<ul>
<li>Device layer: <code><a class="el" href="ie__device__cuda_8cu.html" title="CUDA runtime implementation for the engine&#39;s C ABI CUDA wrapper layer.">engine/src/devices/ie_device_cuda.cu</a></code>, kernels in <code><a class="el" href="ie__kernels__cuda_8cu.html">engine/src/kernels/ie_kernels_cuda.cu</a></code>.</li>
<li>Build: <code>make build-cuda</code> → <code>build/inference-engine.cuda</code>.</li>
<li>INT4 weight‑only support mirrors the CPU path; packing and scales are shared in <code>model.ie.json</code>.</li>
</ul>
<hr  />
 <h1><a class="anchor" id="autotoc_md42"></a>
INT4 Weight-Only Path — Design Addendum (2025-10-24 21:04:23 UTC)</h1>
<h2><a class="anchor" id="autotoc_md43"></a>
Goals</h2>
<ul>
<li>Introduce an <b>optional</b> path for <em>weight-only</em> INT4 (<code>int4w</code>) that: 1) Packs HF weights into IEBIN using a <b>manifest-driven</b> policy. 2) Leaves tokenization, shapes, and scheduling untouched. 3) Preserves benchmark comparability via <em>work-touch</em> instrumentation.</li>
</ul>
<h2><a class="anchor" id="autotoc_md44"></a>
Design Choices</h2>
<ul>
<li><b>Manifest-driven packing</b>: <code>--q4-map</code> lets us target only GEMV-heavy matrices (attn/MLP projections) and keep sensitive tensors (embeddings, layernorms) in FP formats.</li>
<li><b>Separation of concerns</b>:<ul>
<li>Storage precision (<code>IE_PRECISION</code>) is passed through <code><a class="el" href="structie__engine__params.html#a92b02bad12cb4e8173728311c00960b7" title="Precision label (e.g., &quot;fp32&quot;, &quot;bf16&quot;, &quot;int4w&quot;).">ie_engine_params_t::precision</a></code> untouched.</li>
<li>Host math precision (FP32/BF16/FP16) remains a separate selection.</li>
</ul>
</li>
<li><b>Compat with harness</b>: The CLI accepts soft hints (<code>--device</code>, <code>--model-dir</code>, <code>--rounds</code>) so existing scripts don’t break.</li>
</ul>
<h2><a class="anchor" id="autotoc_md45"></a>
Data Flow (INT4 path)</h2>
<p>HF shards → <code><a class="el" href="hf__to__iebin_8py.html">hf_to_iebin.py</a> --q4-map</code> → IEBIN (<code>model.ie.json</code> + <code>model.ie.bin</code> with INT4-packed tensors) → Runtime <code>IE_PRECISION=int4w</code> → GEMV kernels read packed weights (or dequant on load), semantics unchanged.</p>
<h2><a class="anchor" id="autotoc_md46"></a>
Metrics Integrity</h2>
<ul>
<li>The timed window is <b>only</b>: <code>ie_engine_generate(...)</code> + optional <em>work-touch</em> loop.</li>
<li>RSS peak and KV counters are sampled <b>after</b> the window to avoid skewing TPS.</li>
</ul>
<hr  />
<h1><a class="anchor" id="autotoc_md48"></a>
Appendix — INT4 (Weight‑Only) Step (Summary)</h1>
<ul>
<li>Convert HF shards → IEBIN with an INT4 manifest: <div class="fragment"><div class="line">python3 scripts/hf_to_iebin.py     --hf-dir models/gpt-oss-20b/hf     --out-dir models/gpt-oss-20b     --q4-map quant/q4_manifest.json</div>
</div><!-- fragment --></li>
<li>Run benchmarks in strict mode with a <b>64 MB/token</b> work‑touch: <div class="fragment"><div class="line">PROMPTS=benchmarks/prompts_10..txt   IE_PRECISION=int4w IE_REQUIRE_MODEL=1   IE_BYTES_PER_TOKEN=64000000 IE_STRIDE_BYTES=256 RUNS=3   make bench           # or: make bench-cuda</div>
</div><!-- fragment --></li>
<li>Precision hints: <code>PRECISION=fp32</code> (activates float path) and <code>IE_PRECISION=int4w</code> (weight‑only path).</li>
</ul>
<hr  />
 <h1><a class="anchor" id="autotoc_md50"></a>
Updates — 2025-11-10</h1>
<h2><a class="anchor" id="autotoc_md51"></a>
NUMA‑aware topology (&lt;tt&gt;ie_topology&lt;/tt&gt;)</h2>
<ul>
<li>Discovers sockets and CPUs from Linux sysfs and exposes a compact API:<ul>
<li><code>ie_topology_init/destroy</code> — lifetime.</li>
<li><code><a class="el" href="ie__topology_8h.html#ae9fec2d245fb73d6c3e8e3cba38d4471" title="Return the number of sockets (physical packages).">ie_topology_sockets()</a></code> — number of sockets.</li>
<li><code>ie_topology_first_cpu_on_socket(s)</code> — first CPU index on socket <code>s</code> (for pinning).</li>
</ul>
</li>
<li>Integration:<ul>
<li>Thread‑pool honors <code>IE_TP_USE_AFFINITY=1</code> with <code>AFFINITY=auto|compact|scatter</code>.</li>
<li><code>ie_hot_replicate_by_socket(...)</code> uses topology for binding.</li>
</ul>
</li>
</ul>
<h2><a class="anchor" id="autotoc_md52"></a>
Hot weights replication</h2>
<ul>
<li>For “hot” tensors (frequently touched), we can replicate pages <b>per socket</b> to reduce remote memory hits.</li>
<li>Implementation sketch:<ul>
<li><code>mmap</code> replica per socket → optional <code>madvise(..., MADV_WILLNEED)</code> → worker binding → socket‑local access.</li>
<li>Controlled by <code>IE_HOT_REPLICATE=1</code> and an optional cap <code>IE_HOT_REPL_LIMIT_MB</code>.</li>
</ul>
</li>
<li>Trade‑offs: additional memory; on single‑socket machines, the feature is a no‑op with negligible overhead.</li>
</ul>
<h2><a class="anchor" id="autotoc_md53"></a>
Activation precision hint</h2>
<ul>
<li>Runtime hint <code>IE_ACT_PREC=int8|fp8|fp16|bf16|fp32</code> allows experimenting with lower‑precision activations while <b>keeping FP32 accumulators</b>.</li>
<li>Weight precision remains independent (e.g., <code>IE_PRECISION=int4w</code> for nibble‑packed weights).</li>
</ul>
<h2><a class="anchor" id="autotoc_md54"></a>
Timing discipline (unchanged semantics)</h2>
<ul>
<li>The benchmark <b>measured window</b> contains only <code>ie_engine_generate(...)</code> + optional work‑touch loop.</li>
<li>All metrics collection (RSS, KV, JSON print) happens <b>after</b> the window.</li>
</ul>
<h2><a class="anchor" id="autotoc_md55"></a>
Example configurations</h2>
<ul>
<li>INT4 weights, FP8 activations, NUMA‑aware with hot replication (CPU): <div class="fragment"><div class="line">export IE_REQUIRE_MODEL=1 IE_PRECISION=int4w IE_ACT_PREC=fp8</div>
<div class="line">export IE_BYTES_PER_TOKEN=$((64*1024*1024)) IE_STRIDE_BYTES=256 IE_VERIFY_TOUCH=1</div>
<div class="line">export IE_TP_USE_AFFINITY=1 AFFINITY=compact IE_HOT_REPLICATE=1</div>
<div class="line">PROMPTS=benchmarks/prompts_10..txt RUNS=3 make bench</div>
</div><!-- fragment --></li>
<li>Same with CUDA: </li>
</ul>
<h1><a class="anchor" id="autotoc_md56"></a>
@icode{bash}</h1>
<p>PROMPTS=benchmarks/prompts_10..txt RUNS=3 make bench-cuda </p>
<h1><a class="anchor" id="autotoc_md57"></a>
Memory Phase Design Addendum (updated 2025-11-12 18:01:19 UTC)</h1>
<h2><a class="anchor" id="autotoc_md58"></a>
Goals</h2>
<ol type="1">
<li>Reduce <b>DRAM traffic</b> on weight fetch via layout (<code>blocked‑K</code>), NUMA locality, and selective non‑temporal loads.</li>
<li>Enable <b>activation down‑precision</b> (INT8/FP8) orthogonally to weight storage (e.g., INT4 weight‑only).</li>
<li>Measure and visualize <b>spatial metrics</b> alongside TPS: MB/token, bytes touched, model coverage, effective bandwidth.</li>
</ol>
<h2><a class="anchor" id="autotoc_md59"></a>
Components</h2>
<ul>
<li>**Topology &amp; Binding (<code><a class="el" href="structie__topology.html" title="Internal representation.">ie_topology</a></code>)**: discovers sockets/CPUs from Linux sysfs. Exposes helpers for pinning (compact/scatter).</li>
<li>**Hot replication (<code><a class="el" href="replicate__hot_8c.html" title="Replication of hot weight blobs per socket using first-touch placement.">replicate_hot.c</a></code>)**: optional per‑socket replicas for frequently‑touched weights; uses <code>mmap</code> + <code>madvise</code>.</li>
<li><b>Blocked‑K Pretranspose</b>: builds and caches a row‑major, <b>K‑blocked</b> layout; improves sequentiality and prefetch efficacy.</li>
<li><b>Streaming heuristics</b>: <code>IE_PREFETCH_DISTANCE</code>, <code>IE_NT_LOADS</code>, and <code>IE_NT_RATIO</code> drive prefetch and non‑temporal load decisions.</li>
<li><b>Activation precision</b>: runtime hint <code>IE_ACT_PREC</code> selects decode path (INT8 per‑tensor/per‑group, FP8 E4M3/E5M2). Accumulation is FP32.</li>
</ul>
<h2><a class="anchor" id="autotoc_md60"></a>
Measurement</h2>
<p>The benchmark window includes <b>generation</b> and the <b>work‑touch</b> loop (controlled by <code>IE_BYTES_PER_TOKEN</code>, <code>IE_STRIDE_BYTES</code>). After the window, the engine samples <b>peak RSS (VmHWM)</b>. The docs generator now derives:</p><ul>
<li><b>MB/token</b> from <code>IE_BYTES_PER_TOKEN</code></li>
<li><b>Total bytes touched</b> = <code>tokens_sum * bytes_per_token</code></li>
<li><b>Coverage</b> = <code>bytes_per_token / size(model.ie.bin)</code></li>
<li><b>Effective bandwidth</b> = <code>bytes_touched / wall_time</code> (GB/s)</li>
</ul>
<h2><a class="anchor" id="autotoc_md61"></a>
Backward Compatibility &amp; Fallbacks</h2>
<ul>
<li>Single‑socket hosts: topology collapses to one socket; binding is a no‑op.</li>
<li>Unsupported backends ignore <code>IE_ACT_PREC</code>, <code>IE_NT_LOADS</code>, etc., without failing.</li>
<li>When <code>IE_REQUIRE_MODEL</code> is unset, CI stub mode continues to work (no mmap or spatial metrics).</li>
</ul>
<h2><a class="anchor" id="autotoc_md62"></a>
Risks &amp; Mitigations</h2>
<ul>
<li><b>Over‑eager NT loads</b> can hurt on small working sets → guard via <code>auto</code> heuristics and <code>IE_NT_RATIO</code> throttle.</li>
<li><b>Replica memory cost</b> on multi‑socket servers → gate with <code>IE_HOT_REPLICATE</code> and <code>IE_HOT_REPL_LIMIT_MB</code>.</li>
</ul>
<h1><a class="anchor" id="autotoc_md63"></a>
Block‑sparse weights (Phase 2, CPU only)</h1>
<p>This chapter describes the <b>block‑sparse weights prototype</b> implemented in the second phase of the memory work. The goal is to make <em>algorithmic sparsity</em> concrete and measurable while keeping the dense engine and IEBIN format intact.</p>
<p>At this stage the implementation is <b>CPU‑only</b>, FP32‑only, and is exercised through C unit tests and a dedicated microbenchmark. It is deliberately small and self‑contained so that we can iterate on formats and policies without destabilizing the main inference path.</p>
<h2><a class="anchor" id="autotoc_md64"></a>
Goals</h2>
<ul>
<li>Provide a <b>well‑defined in‑memory descriptor</b> for block‑sparse matrices (<code>ie_block_sparse_matrix_t</code>).</li>
<li>Define a <b>compact on‑disk format</b> that can be produced by a simple C tool (<code><a class="el" href="convert__to__block__sparse_8c.html" title="Offline converter from dense FP32 matrix to block-sparse binary file.">tools/convert_to_block_sparse.c</a></code>) and loaded by the engine.</li>
<li>Implement a <b>reference GEMV kernel</b> for block‑sparse matrices on CPU: numerically equivalent to dense GEMV (modulo FP roundoff).</li>
<li>Wire this into the device abstraction so that:<ul>
<li>the CPU backend can execute block‑sparse GEMV; and</li>
<li>other backends can safely report “unimplemented” and trigger a CPU fallback.</li>
</ul>
</li>
<li>Keep the feature fully <b>opt‑in</b>:<ul>
<li>No changes to <code>model.ie.bin</code> or the CLI.</li>
<li>No new runtime flags required for existing workflows.</li>
</ul>
</li>
</ul>
<h2><a class="anchor" id="autotoc_md65"></a>
In‑memory layout: &lt;tt&gt;ie_block_sparse_matrix_t&lt;/tt&gt;</h2>
<p>The new public descriptor lives in <code><a class="el" href="sparse__format_8h.html" title="In-memory representation and loader interface for block-sparse matrices.">engine/include/sparse_format.h</a></code>:</p>
<ul>
<li>Global dimensions:<ul>
<li><code>rows</code>, <code>cols</code> — dense matrix shape.</li>
<li><code>block_rows</code>, <code>block_cols</code> — tile shape (same for all blocks).</li>
</ul>
</li>
<li>Block‑row CSR (BSR) structure:<ul>
<li><code>n_block_rows</code> — number of block rows (typically <code>ceil(rows / block_rows)</code>).</li>
<li><code>row_ptr</code> — length <code>n_block_rows + 1</code>; <code>row_ptr[br]..row_ptr[br+1]-1</code> indexes the non‑zero blocks (<code>nnzb</code> total).</li>
<li><code>col_idx</code> — length <code>nnzb</code>; column index in block coordinates (<code>0..ceil(cols / block_cols)-1</code>).</li>
</ul>
</li>
<li>Values:<ul>
<li><code>values</code> — contiguous FP32 array of length <code>nnzb * block_rows * block_cols</code>, stored in row‑major order <em>within</em> each block.</li>
</ul>
</li>
</ul>
<p>Semantics:</p>
<ul>
<li>Conceptually the matrix is partitioned into <code>block_rows x block_cols</code> tiles. For each block row <code>br</code> we list all non‑zero tiles in ascending block column order.</li>
<li>The actual dense dimensions are always taken from <code>rows</code> / <code>cols</code>. Tail blocks near the bottom/right edges are automatically clipped in the GEMV kernel.</li>
</ul>
<p>The helpers in <code><a class="el" href="sparse__format_8h.html" title="In-memory representation and loader interface for block-sparse matrices.">sparse_format.h</a></code> cover:</p>
<ul>
<li>sanity checks for header fields;</li>
<li>allocation/free helpers; and</li>
<li>small utilities to compute block counts and strides.</li>
</ul>
<h2><a class="anchor" id="autotoc_md66"></a>
On‑disk format and loader (&lt;tt&gt;engine/src/sparse_io.c&lt;/tt&gt;)</h2>
<p>To keep experiments reproducible without touching the IEBIN format, we use a separate, compact binary format for block‑sparse matrices:</p>
<ul>
<li>A fixed‑size header that records:<ul>
<li>magic / version;</li>
<li><code>rows</code>, <code>cols</code>, <code>block_rows</code>, <code>block_cols</code>;</li>
<li><code>n_block_rows</code>, <code>nnzb</code>;</li>
<li>sizes of the three payload arrays (<code>row_ptr</code>, <code>col_idx</code>, <code>values</code>).</li>
</ul>
</li>
<li>Payload sections, tightly packed:<ol type="1">
<li><code>row_ptr</code> (<code>uint32_t</code> × <code>n_block_rows + 1</code>);</li>
<li><code>col_idx</code> (<code>uint32_t</code> × <code>nnzb</code>);</li>
<li><code>values</code> (<code>float</code> × <code>nnzb * block_rows * block_cols</code>).</li>
</ol>
</li>
</ul>
<p>The loader <code><a class="el" href="sparse__format_8h.html#a3f5428268012e4d5e4e9d8a58c01a008" title="Load a block-sparse matrix from a binary file.">ie_block_sparse_load(const char *path, ie_block_sparse_matrix_t *out)</a></code> performs:</p>
<ol type="1">
<li>open + read header;</li>
<li>validate fields (non‑zero dimensions, monotonically increasing <code>row_ptr</code>, etc.);</li>
<li>allocate arrays for <code>row_ptr</code>, <code>col_idx</code>, <code>values</code>;</li>
<li>read the three payload sections; and</li>
<li>on success, fill <code>out</code> with owned pointers and return <code>IE_SPARSE_OK</code>.</li>
</ol>
<p>Error paths:</p>
<ul>
<li>Any structural or I/O error returns a specific <code>ie_sparse_status_t</code> (e.g. <code>IE_SPARSE_ERR_IO</code>, <code>IE_SPARSE_ERR_FORMAT</code>).</li>
<li>On error, partially allocated buffers are freed and <code>out</code> is zeroed.</li>
</ul>
<h2><a class="anchor" id="autotoc_md67"></a>
CPU kernel (&lt;tt&gt;engine/src/gemm_block_sparse.c&lt;/tt&gt;)</h2>
<p>The reference GEMV implementation:</p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> <a class="code hl_function" href="sparse__format_8h.html#ae9a2aad0ec5332237097030acf034961">ie_gemv_block_sparse_f32</a>(<span class="keyword">const</span> <a class="code hl_struct" href="structie__block__sparse__matrix.html">ie_block_sparse_matrix_t</a> *<a class="code hl_function" href="ie__device__cuda_8cu.html#a5e3d9fee84981707669f5c2398f0c4ab">m</a>,</div>
<div class="line">                              <span class="keyword">const</span> <span class="keywordtype">float</span> *<a class="code hl_function" href="ie__device__cuda_8cu.html#a5e3d9fee84981707669f5c2398f0c4ab">x</a>,</div>
<div class="line">                              <span class="keywordtype">float</span> *<a class="code hl_function" href="ie__device__cuda_8cu.html#a5e3d9fee84981707669f5c2398f0c4ab">y</a>,</div>
<div class="line">                              <span class="keyword">const</span> <span class="keywordtype">float</span> *<a class="code hl_function" href="ie__device__cuda_8cu.html#a5e3d9fee84981707669f5c2398f0c4ab">bias</a>);</div>
<div class="ttc" id="aie__device__cuda_8cu_html_a5e3d9fee84981707669f5c2398f0c4ab"><div class="ttname"><a href="ie__device__cuda_8cu.html#a5e3d9fee84981707669f5c2398f0c4ab">ie_gemv_rowwise_f32_kernel</a></div><div class="ttdeci">__global__ void ie_gemv_rowwise_f32_kernel(const float *W, const float *x, float *y, const float *bias, size_t rows, size_t cols)</div><div class="ttdoc">Row-wise GEMV kernel: each thread computes one output row.</div><div class="ttdef"><b>Definition</b> <a href="ie__device__cuda_8cu_source.html#l00092">ie_device_cuda.cu:92</a></div></div>
<div class="ttc" id="asparse__format_8h_html_ae9a2aad0ec5332237097030acf034961"><div class="ttname"><a href="sparse__format_8h.html#ae9a2aad0ec5332237097030acf034961">ie_gemv_block_sparse_f32</a></div><div class="ttdeci">void ie_gemv_block_sparse_f32(const ie_block_sparse_matrix_t *m, const float *x, float *y, const float *bias)</div><div class="ttdoc">Perform a single-precision block-sparse matrix-vector product.</div><div class="ttdef"><b>Definition</b> <a href="gemm__block__sparse_8c_source.html#l00077">gemm_block_sparse.c:77</a></div></div>
<div class="ttc" id="astructie__block__sparse__matrix_html"><div class="ttname"><a href="structie__block__sparse__matrix.html">ie_block_sparse_matrix</a></div><div class="ttdoc">In-memory representation of a block-sparse matrix.</div><div class="ttdef"><b>Definition</b> <a href="sparse__format_8h_source.html#l00118">sparse_format.h:118</a></div></div>
</div><!-- fragment --><p>Design:</p>
<ul>
<li>Single‑threaded, straightforward loop structure:<ul>
<li>iterate over block rows (<code>br</code>);</li>
<li>for each local row in the block (<code>local_r</code>), compute the dense row index <code>row = br * block_rows + local_r</code>;</li>
<li>iterate over non‑zero blocks in that block row using <code>row_ptr[br]..row_ptr[br+1]</code>;</li>
<li>for each block, compute the starting column and take an inner product between the block row and the corresponding slice of <code>x</code>.</li>
</ul>
</li>
<li>Tail safety:<ul>
<li>rows with <code>row &gt;= rows</code> are skipped;</li>
<li>columns with <code>col &gt;= cols</code> are skipped inside the innermost loop.</li>
</ul>
</li>
<li>Bias:<ul>
<li><code>bias == NULL</code> is allowed; in that case the accumulator starts at <code>0.0f</code>;</li>
<li>otherwise we seed <code>acc</code> with <code>bias[row]</code>.</li>
</ul>
</li>
</ul>
<p>This function is small and easy to inspect, prioritizing correctness and debuggability over clever micro‑optimizations. Higher‑level code can decide whether and how to shard block rows across threads.</p>
<h2><a class="anchor" id="autotoc_md68"></a>
Device abstraction (&lt;tt&gt;engine/src/devices/ie_device_common.c&lt;/tt&gt;)</h2>
<p>The <code><a class="el" href="structie__device.html" title="Public device object combining vtable and backend-specific impl pointer.">ie_device</a></code> vtable gains a new entry:</p>
<div class="fragment"><div class="line"><a class="code hl_function" href="ie__device__cuda_8cu.html#a5e3d9fee84981707669f5c2398f0c4ab">int</a>  (*gemv_block_sparse_f32)(<span class="keywordtype">void</span> *<a class="code hl_function" href="ie__device__cuda_8cu.html#a5e3d9fee84981707669f5c2398f0c4ab">self</a>,</div>
<div class="line">                              <span class="keyword">const</span> <a class="code hl_struct" href="structie__block__sparse__matrix.html">ie_block_sparse_matrix_t</a> *<a class="code hl_function" href="ie__device__cuda_8cu.html#a5e3d9fee84981707669f5c2398f0c4ab">m</a>,</div>
<div class="line">                              <span class="keyword">const</span> <span class="keywordtype">float</span> *<a class="code hl_function" href="ie__device__cuda_8cu.html#a5e3d9fee84981707669f5c2398f0c4ab">x</a>,</div>
<div class="line">                              <span class="keywordtype">float</span> *<a class="code hl_function" href="ie__device__cuda_8cu.html#a5e3d9fee84981707669f5c2398f0c4ab">y</a>,</div>
<div class="line">                              <span class="keyword">const</span> <span class="keywordtype">float</span> *<a class="code hl_function" href="ie__device__cuda_8cu.html#a5e3d9fee84981707669f5c2398f0c4ab">bias</a>);</div>
</div><!-- fragment --><p>Backend implementations:</p>
<ul>
<li><b>CPU:</b><ul>
<li><code>cpu_gemv_block_sparse_f32</code> simply forwards to <code>ie_gemv_block_sparse_f32</code>.</li>
<li><code>ie_device_gemv_block_sparse_f32</code> (public helper) routes through the vtable and, on failure, can fall back to a CPU device if the current device is not already CPU.</li>
</ul>
</li>
<li><b>CUDA / Level Zero:</b><ul>
<li>for this phase they return an “unimplemented” error code;</li>
<li>callers that use <code>ie_device_gemv_block_sparse_f32</code> will see the error and can choose to fall back to CPU.</li>
<li>No GPU kernels are required for tests to pass.</li>
</ul>
</li>
</ul>
<p>This layout keeps the public API stable while allowing future ADRs to add true GPU implementations under the same method.</p>
<h2><a class="anchor" id="autotoc_md69"></a>
Tools and tests</h2>
<h3><a class="anchor" id="autotoc_md70"></a>
Offline converter (&lt;tt&gt;tools/convert_to_block_sparse.c&lt;/tt&gt;)</h3>
<p>The converter takes a dense, row‑major FP32 matrix and produces the block‑sparse binary format:</p>
<ul>
<li>Inputs:<ul>
<li>path to a dense <code>.bin</code> (raw <code>float</code> array of shape <code>rows × cols</code>);</li>
<li><code>rows</code>, <code>cols</code>, <code>block_rows</code>, <code>block_cols</code>;</li>
<li>optional threshold / sparsification policy (for now, the prototype typically uses <em>exact</em> sparsity patterns produced upstream).</li>
</ul>
</li>
<li>Steps:<ol type="1">
<li>read dense matrix into memory;</li>
<li>scan it by blocks, classify non‑zero blocks;</li>
<li>build <code>row_ptr</code> / <code>col_idx</code>;</li>
<li>emit the header + payload to an output path.</li>
</ol>
</li>
</ul>
<p>This keeps sparsification an explicit, offline step: the engine never modifies weights at load time.</p>
<h3><a class="anchor" id="autotoc_md71"></a>
C unit tests (&lt;tt&gt;tests/c/test_block_sparse.c&lt;/tt&gt;)</h3>
<p>The tests validate both the loader and the GEMV kernel:</p>
<ul>
<li>Small 4×4 and 8×8 matrices with hand‑written dense values and known products.</li>
<li>Construction of the corresponding block‑sparse structures in memory, followed by calls to <code>ie_gemv_block_sparse_f32</code>.</li>
<li>Round‑trip tests for the on‑disk format: write block‑sparse payloads, load via <code>ie_block_sparse_load</code>, compare against the in‑memory structures, and re‑run GEMV.</li>
</ul>
<p>The test target is wired into <code>make test</code> alongside the existing C unit tests so regressions are caught early.</p>
<h3><a class="anchor" id="autotoc_md72"></a>
Microbenchmark (&lt;tt&gt;benchmarks/src/microbench_gemv_block_sparse.c&lt;/tt&gt;)</h3>
<p>A dedicated microbenchmark compares dense vs block‑sparse GEMV on CPU:</p>
<ul>
<li>Synthesizes a dense matrix and a block‑sparse version with a chosen sparsity pattern.</li>
<li>Runs timed loops for both kernels and prints:<ul>
<li>runtime, ns/element, and effective GB/s;</li>
<li>any basic correctness statistics (e.g. max absolute difference).</li>
</ul>
</li>
<li>Built and run via <code>make microbench-block-sparse</code>.</li>
</ul>
<p>This is a <b>local</b> measurement tool; it does not depend on real model weights or the CLI.</p>
<h2><a class="anchor" id="autotoc_md73"></a>
Integration strategy and future work</h2>
<p>This phase intentionally stops short of wiring block‑sparse weights into <code>model.ie.bin</code> and the inference CLI:</p>
<ul>
<li>existing users see no change in behavior;</li>
<li>sparsity experiments use their own binaries and scripts; and</li>
<li>the code paths remain small enough to refactor without churn.</li>
</ul>
<p>Follow‑up work (future ADRs) may cover:</p>
<ul>
<li>extending the IEBIN format (or adding a sidecar) to carry block‑sparse weights for specific layers;</li>
<li>heuristics for which layers to sparsify (e.g. MLP vs attention);</li>
<li>GPU kernels for popular architectures; and</li>
<li>interaction with quantization and activation formats.</li>
</ul>
<p>For now, the block‑sparse prototype gives us a solid, CPU‑only baseline to reason about algorithmic sparsity independently of lower‑level memory and topology tricks introduced in the first phase.</p>
<hr  />
<h2><a class="anchor" id="autotoc_md75"></a>
Lossless Deduplication (Schema2): defaults + masks + exceptions</h2>
<p>This project includes a <b>lossless</b> dedup path that reduces DRAM reads by reusing repeated weight content. The runtime reconstructs bytes exactly (bit-for-bit) by applying a sparse patch stream over shared defaults.</p>
<h3><a class="anchor" id="autotoc_md76"></a>
Artifacts</h3>
<p>The runtime loader expects these files in the model directory (next to <code>model.ie.json</code> / <code>model.ie.bin</code>):</p>
<ul>
<li><code>model.defaults.bin</code> — concatenated default payload bytes</li>
<li><code>model.masks.bin</code> — exception mask bits/bytes (identifies where defaults differ)</li>
<li><code>model.exceptions.bin</code> — exception bytes (dense list of the differing bytes)</li>
</ul>
<p>In the current workflow, these are typically generated into <code>models/&lt;MODEL&gt;/dedup_out/</code> and then symlinked.</p>
<h3><a class="anchor" id="autotoc_md77"></a>
Offline generation: extract defaults/masks/exceptions</h3>
<p>The extractor takes:</p><ul>
<li><code>tensor_map.json</code> (maps tensor names to on-disk byte ranges / sources)</li>
<li><code>groups.indices.json</code> (dedup grouping and indices) and produces the three binary blobs:</li>
</ul>
<div class="fragment"><div class="line">python3 tools/dedup_extract_int4.py \</div>
<div class="line">  --model-dir &quot;models/gpt-oss-20b&quot; \</div>
<div class="line">  --tensor-map &quot;models/gpt-oss-20b/dedup_out/tensor_map.json&quot; \</div>
<div class="line">  --groups &quot;models/gpt-oss-20b/dedup_out/groups.indices.json&quot; \</div>
<div class="line">  --out-prefix &quot;models/gpt-oss-20b/dedup_out/model&quot;</div>
</div><!-- fragment --><p>Then link them into the model root:</p>
<div class="fragment"><div class="line">cd models/gpt-oss-20b</div>
<div class="line">ln -sf dedup_out/model.defaults.bin   model.defaults.bin</div>
<div class="line">ln -sf dedup_out/model.masks.bin      model.masks.bin</div>
<div class="line">ln -sf dedup_out/model.exceptions.bin model.exceptions.bin</div>
<div class="line">cd ../..</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md78"></a>
Runtime controls</h3>
<ul>
<li><code>IE_DEDUP=1</code> enables the dedup loader.</li>
<li><code>IE_DEDUP_STRICT=1</code> fails fast if dedup artifacts are missing or malformed.</li>
<li><code>IE_DEDUP_POLICY=lossless</code> selects the exact reconstruction policy (no approximations).</li>
<li><code>IE_DEDUP_CACHE_MB=&lt;N&gt;</code> configures the in-memory cache used by the dedup loader.</li>
<li><code>IE_DEDUP_DEBUG=1</code> enables verbose parsing/debug logs.</li>
</ul>
<p>Strict runs should also enforce real IEBIN loading and anti-optimization work-touch:</p>
<ul>
<li><code>IE_REQUIRE_MODEL=1</code></li>
<li><code>IE_VERIFY_TOUCH=1</code></li>
<li><code>IE_BYTES_PER_TOKEN=&lt;nonzero&gt;</code></li>
<li><code>IE_STRIDE_BYTES=256</code> (default baseline)</li>
</ul>
<h3><a class="anchor" id="autotoc_md79"></a>
Schema2 JSON compatibility notes (model.ie.json)</h3>
<p>The schema2 parser is strict about per-tensor metadata. When ingesting IEBIN metadata derived from HF shards, ensure the following are true for each tensor entry:</p>
<ul>
<li><code>dtype</code> is lowercase (<code>bf16</code>, <code>f16</code>, <code>f32</code>, <code>u8</code>, ...)</li>
<li>file metadata uses the schema2 keys:<ul>
<li><code>file</code> (the backing shard filename)</li>
<li><code>file_data_offset</code> (byte offset inside that shard)</li>
</ul>
</li>
</ul>
<p>If your <code>model.ie.json</code> uses <code>shard</code> / <code>shard_data_offset</code> instead, you can add aliases:</p>
<div class="fragment"><div class="line">python3 - &lt;&lt;&#39;PY&#39;</div>
<div class="line">import json</div>
<div class="line">from pathlib import Path</div>
<div class="line"> </div>
<div class="line">p = Path(&quot;models/gpt-oss-20b/model.ie.json&quot;)</div>
<div class="line">j = json.loads(p.read_text(encoding=&quot;utf-8&quot;))</div>
<div class="line"> </div>
<div class="line">t = j.get(&quot;tensors&quot;)</div>
<div class="line">if not isinstance(t, list):</div>
<div class="line">    raise SystemExit(&quot;ERROR: tensors is not a list&quot;)</div>
<div class="line"> </div>
<div class="line">for e in t:</div>
<div class="line">    if not isinstance(e, dict):</div>
<div class="line">        continue</div>
<div class="line">    d = e.get(&quot;dtype&quot;)</div>
<div class="line">    if isinstance(d, str):</div>
<div class="line">        e[&quot;dtype&quot;] = d.lower()</div>
<div class="line">    if &quot;file&quot; not in e and &quot;shard&quot; in e and isinstance(e[&quot;shard&quot;], str):</div>
<div class="line">        e[&quot;file&quot;] = e[&quot;shard&quot;]</div>
<div class="line">    if &quot;file_data_offset&quot; not in e and &quot;shard_data_offset&quot; in e and isinstance(e[&quot;shard_data_offset&quot;], int):</div>
<div class="line">        e[&quot;file_data_offset&quot;] = e[&quot;shard_data_offset&quot;]</div>
<div class="line"> </div>
<div class="line">p.write_text(json.dumps(j, ensure_ascii=False, separators=(&quot;,&quot;, &quot;:&quot;), sort_keys=True) + &quot;\n&quot;, encoding=&quot;utf-8&quot;)</div>
<div class="line">print(&quot;OK: tensors =&quot;, len(t))</div>
<div class="line">PY</div>
</div><!-- fragment --><p>You may see warnings like <code>unknown dtype=u8 ... (continuing)</code> for quantized auxiliary tensors (blocks/scales); these are expected as long as the loader is configured to ignore/skip unknown storage dtypes where safe.</p>
<h3><a class="anchor" id="autotoc_md80"></a>
Example strict run (CPU)</h3>
<div class="fragment"><div class="line">env -i \</div>
<div class="line">  IE_REQUIRE_MODEL=1 \</div>
<div class="line">  IE_VERIFY_TOUCH=1 \</div>
<div class="line">  IE_BYTES_PER_TOKEN=67108864 \</div>
<div class="line">  IE_STRIDE_BYTES=256 \</div>
<div class="line">  IE_DEDUP=1 \</div>
<div class="line">  IE_DEDUP_STRICT=1 \</div>
<div class="line">  IE_DEDUP_POLICY=lossless \</div>
<div class="line">  IE_DEDUP_CACHE_MB=512 \</div>
<div class="line">  IE_DEDUP_DEBUG=1 \</div>
<div class="line">  PRECISION=int4w \</div>
<div class="line">  ./build/inference-engine \</div>
<div class="line">    --device cpu \</div>
<div class="line">    --model-dir &quot;$(pwd)/models/gpt-oss-20b&quot; \</div>
<div class="line">    --prompts-file &quot;$(pwd)/benchmarks/prompts_10.txt&quot; \</div>
<div class="line">    --max-new 8 --rounds 1</div>
</div><!-- fragment --> </div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8 </li>
  </ul>
</div>
</body>
</html>
